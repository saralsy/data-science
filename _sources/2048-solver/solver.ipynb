{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (0.22.1)\n",
      "Requirement already satisfied: numpy in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game 2048 Environment and DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Game2048Env:\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((4, 4), dtype=int)\n",
    "        self._add_tile()\n",
    "        self._add_tile()\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "        return self.board.copy()\n",
    "\n",
    "    def _add_tile(self):\n",
    "        empty = list(zip(*np.where(self.board == 0)))\n",
    "        if not empty:\n",
    "            return\n",
    "        i, j = random.choice(empty)\n",
    "        self.board[i][j] = 2 if random.random() < 0.9 else 4\n",
    "\n",
    "    def _move_line(self, line):\n",
    "        new_line = [i for i in line if i != 0]\n",
    "        score_increase = 0\n",
    "        merged_line = []\n",
    "        skip = False\n",
    "        for i in range(len(new_line)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(new_line) and new_line[i] == new_line[i + 1]:\n",
    "                merged_value = new_line[i] * 2\n",
    "                merged_line.append(merged_value)\n",
    "                score_increase += merged_value\n",
    "                skip = True\n",
    "            else:\n",
    "                merged_line.append(new_line[i])\n",
    "        merged_line += [0] * (self.size - len(merged_line))\n",
    "        return np.array(merged_line), score_increase\n",
    "\n",
    "    def _move_board(self, direction):\n",
    "        board = self.board.copy()\n",
    "        total_reward = 0\n",
    "        rotated_board = np.rot90(board, k=direction)\n",
    "        new_board = np.zeros_like(rotated_board)\n",
    "        for i in range(self.size):\n",
    "            line = rotated_board[i, :]\n",
    "            new_line, reward = self._move_line(line)\n",
    "            new_board[i, :] = new_line\n",
    "            total_reward += reward\n",
    "        final_board = np.rot90(new_board, k=-direction)\n",
    "        return final_board, total_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.board.copy(), 0, True, {}\n",
    "        new_board, reward = self._move_board(action)\n",
    "        if not np.array_equal(new_board, self.board):\n",
    "            self.board = new_board\n",
    "            self._add_tile()\n",
    "            self.score += reward\n",
    "            if self.is_done():\n",
    "                self.done = True\n",
    "        else:\n",
    "            reward = -10\n",
    "        return self.board.copy(), reward, self.done, {}\n",
    "    \n",
    "    def _move(self, board, action):\n",
    "        rotated_board = np.rot90(board, k=action)\n",
    "        new_board = np.zeros_like(rotated_board)\n",
    "        for i in range(self.size):\n",
    "            line = rotated_board[i, :]\n",
    "            new_line, reward = self._move_line(line)\n",
    "            new_board[i, :] = new_line\n",
    "        final_board = np.rot90(new_board, k=-action)\n",
    "        return final_board\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        valid_actions = []\n",
    "        for action in range(4):\n",
    "            temp_board = self.board.copy()\n",
    "             # Apply the move logic manually\n",
    "            rotated_board = np.rot90(temp_board, k=action)\n",
    "            new_board = np.zeros_like(rotated_board)\n",
    "            for i in range(self.size):\n",
    "                line = rotated_board[i, :]\n",
    "                new_line, _ = self._move_line(line)\n",
    "                new_board[i, :] = new_line\n",
    "            final_board = np.rot90(new_board, k=-action)\n",
    "            \n",
    "            if not np.array_equal(final_board, temp_board):\n",
    "                valid_actions.append(action)\n",
    "        \n",
    "        return valid_actions\n",
    "\n",
    "    def is_done(self):\n",
    "        return len(self.get_valid_actions()) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Optimized Environment ---\n",
    "class Game2048Env:\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((4, 4), dtype=int)\n",
    "        self._add_tile()\n",
    "        self._add_tile()\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "        return self.board.copy()\n",
    "\n",
    "    def _add_tile(self):\n",
    "        empty = list(zip(*np.where(self.board == 0)))\n",
    "        if not empty:\n",
    "            return\n",
    "        i, j = random.choice(empty)\n",
    "        self.board[i][j] = 2 if random.random() < 0.9 else 4\n",
    "\n",
    "    def _move_line(self, line):\n",
    "        new_line = [i for i in line if i != 0]\n",
    "        score_increase = 0\n",
    "        merged_line = []\n",
    "        skip = False\n",
    "        for i in range(len(new_line)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(new_line) and new_line[i] == new_line[i + 1]:\n",
    "                merged_value = new_line[i] * 2\n",
    "                merged_line.append(merged_value)\n",
    "                score_increase += merged_value\n",
    "                skip = True\n",
    "            else:\n",
    "                merged_line.append(new_line[i])\n",
    "        merged_line += [0] * (self.size - len(merged_line))\n",
    "        return np.array(merged_line), score_increase\n",
    "\n",
    "    def _move_board(self, direction):\n",
    "        board = self.board.copy()\n",
    "        total_reward = 0\n",
    "        rotated_board = np.rot90(board, k=direction)\n",
    "        new_board = np.zeros_like(rotated_board)\n",
    "        for i in range(self.size):\n",
    "            line = rotated_board[i, :]\n",
    "            new_line, reward = self._move_line(line)\n",
    "            new_board[i, :] = new_line\n",
    "            total_reward += reward\n",
    "        final_board = np.rot90(new_board, k=-direction)\n",
    "        return final_board, total_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.board.copy(), 0, True, {}\n",
    "        new_board, reward = self._move_board(action)\n",
    "        if not np.array_equal(new_board, self.board):\n",
    "            self.board = new_board\n",
    "            self._add_tile()\n",
    "            self.score += reward\n",
    "            if self.is_done():\n",
    "                self.done = True\n",
    "        else:\n",
    "            reward = -10\n",
    "        return self.board.copy(), reward, self.done, {}\n",
    "    \n",
    "    def _move(self, board, action):\n",
    "        rotated_board = np.rot90(board, k=action)\n",
    "        new_board = np.zeros_like(rotated_board)\n",
    "        for i in range(self.size):\n",
    "            line = rotated_board[i, :]\n",
    "            new_line, reward = self._move_line(line)\n",
    "            new_board[i, :] = new_line\n",
    "        final_board = np.rot90(new_board, k=-action)\n",
    "        return final_board\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        valid_actions = []\n",
    "        for action in range(4):\n",
    "            temp_board = self.board.copy()\n",
    "             # Apply the move logic manually\n",
    "            rotated_board = np.rot90(temp_board, k=action)\n",
    "            new_board = np.zeros_like(rotated_board)\n",
    "            for i in range(self.size):\n",
    "                line = rotated_board[i, :]\n",
    "                new_line, _ = self._move_line(line)\n",
    "                new_board[i, :] = new_line\n",
    "            final_board = np.rot90(new_board, k=-action)\n",
    "            \n",
    "            if not np.array_equal(final_board, temp_board):\n",
    "                valid_actions.append(action)\n",
    "        \n",
    "        return valid_actions\n",
    "\n",
    "    def is_done(self):\n",
    "        return len(self.get_valid_actions()) == 0\n",
    "\n",
    "# --- One-Hot Board Encoding ---\n",
    "def encode_board(board):\n",
    "    channels = 16 \n",
    "    out = np.zeros((channels, 4, 4), dtype=np.float32)\n",
    "    for r in range(4):\n",
    "        for c in range(4):\n",
    "            value = board[r][c]\n",
    "            if value > 0:\n",
    "                index = int(np.log2(value)) - 1\n",
    "                if 0 <= index < channels:\n",
    "                    out[index, r, c] = 1.0\n",
    "    return out\n",
    "\n",
    "# --- CNN-Based DQN ---\n",
    "class CNN_DQN(nn.Module):\n",
    "    def __init__(self, input_channels=16, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 128, kernel_size=2, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=2, padding='same'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# --- OPTIMIZATION 1: Prioritized Experience Replay (PER) Buffer ---\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        max_prio = np.max(self.priorities) if self.buffer else 1.0\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "            \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        return samples, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio + 1e-5 # Add epsilon to avoid zero priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# --- Agent with Double DQN ---\n",
    "class Agent:\n",
    "    def __init__(self, learning_rate=2.5e-5, gamma=0.99, max_memory=20000,\n",
    "                 batch_size=128, epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.9995, per_alpha=0.6, per_beta_start=0.4, per_beta_frames=200000):\n",
    "        self.q_net = CNN_DQN()\n",
    "        self.target_net = CNN_DQN()\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.memory = PrioritizedReplayBuffer(max_memory, per_alpha)\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Beta for PER importance sampling, annealed from start to 1 over training frames\n",
    "        self.per_beta = per_beta_start\n",
    "        self.per_beta_start = per_beta_start\n",
    "        self.per_beta_frames = per_beta_frames\n",
    "        self.frame_idx = 0\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10000, gamma=0.9)\n",
    "        self.training_steps = 0\n",
    "\n",
    "        # Training monitoring\n",
    "        self.training_stats = {\n",
    "            'losses': [],\n",
    "            'td_errors': [],\n",
    "            'q_values_mean': [],\n",
    "            'targets_mean': [],\n",
    "            'rewards_mean': [],\n",
    "            'gradient_norms': []\n",
    "        }\n",
    "\n",
    "    def get_beta(self):\n",
    "        return min(1.0, self.per_beta_start + self.frame_idx * (1.0 - self.per_beta_start) / self.per_beta_frames)\n",
    "\n",
    "    def act(self, state_board, valid_actions):\n",
    "        if not valid_actions:\n",
    "             return 0\n",
    "        self.frame_idx += 1\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(encode_board(state_board)).unsqueeze(0)\n",
    "            q_values = self.q_net(state_tensor).squeeze().numpy()\n",
    "        masked_q = np.full(q_values.shape, -np.inf)\n",
    "        for a in valid_actions:\n",
    "            masked_q[a] = q_values[a]\n",
    "        return int(np.argmax(masked_q))\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        beta = self.get_beta()\n",
    "        batch, indices, weights = self.memory.sample(self.batch_size, beta)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states_enc = torch.tensor(np.array([encode_board(s) for s in states]), dtype=torch.float32)\n",
    "        next_states_enc = torch.tensor(np.array([encode_board(s) for s in next_states]), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "        # Calculate Q-values for the current state\n",
    "        q_values = self.q_net(states_enc).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Calculate Q-values for the next state\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.q_net(next_states_enc)\n",
    "            # Get the best action for the next state\n",
    "            best_next_actions = torch.argmax(next_q_values_online, dim=1)\n",
    "            # Get the Q-values for the best action for the next state\n",
    "            next_q_values_target = self.target_net(next_states_enc)\n",
    "            next_q_target = next_q_values_target.gather(1, best_next_actions.unsqueeze(1)).squeeze(1)\n",
    "            targets = rewards + self.gamma * next_q_target * (1 - dones)\n",
    "\n",
    "        # Calculate the TD errors (target - q_values)\n",
    "        td_errors = torch.abs(targets - q_values)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.mse_loss(q_values, targets, reduction='none')\n",
    "        loss = (loss * weights).mean()\n",
    "\n",
    "        # Update the Q-network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        gradient_norms = torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update the learning rate\n",
    "        self.scheduler.step()\n",
    "        self.training_steps += 1\n",
    "\n",
    "        # Store the training stats\n",
    "        self.store_training_stats(loss, td_errors, q_values, targets, rewards, gradient_norms)\n",
    "\n",
    "        # Update the PER priorities\n",
    "        priorities = td_errors.detach().clamp(max=5.0).cpu().numpy()\n",
    "        self.memory.update_priorities(indices, priorities)\n",
    "\n",
    "    \n",
    "    # store the training stats\n",
    "    def store_training_stats(self, loss, td_errors, q_values, targets, rewards, gradient_norms):\n",
    "        self.training_stats['losses'].append(loss.item())\n",
    "        self.training_stats['td_errors'].append(td_errors.mean().item())\n",
    "        self.training_stats['q_values_mean'].append(q_values.mean().item())  # Take mean first\n",
    "        self.training_stats['targets_mean'].append(targets.mean().item())    # Take mean first\n",
    "        self.training_stats['rewards_mean'].append(rewards.mean().item())    # Take mean first\n",
    "        self.training_stats['gradient_norms'].append(gradient_norms.item())\n",
    "    \n",
    "    def get_training_summary(self):\n",
    "        \"\"\"Get summary statistics of recent training\"\"\"\n",
    "        if not self.training_stats['losses']:\n",
    "            return None\n",
    "            \n",
    "        recent_losses = self.training_stats['losses'][-100:]  # Last 100 training steps\n",
    "        recent_td_errors = self.training_stats['td_errors'][-100:]\n",
    "        recent_q_values = self.training_stats['q_values_mean'][-100:]\n",
    "        recent_targets = self.training_stats['targets_mean'][-100:]\n",
    "        recent_rewards = self.training_stats['rewards_mean'][-100:]\n",
    "        recent_gradient_norms = self.training_stats['gradient_norms'][-100:]\n",
    "        \n",
    "        return {\n",
    "            'avg_loss': np.mean(recent_losses),\n",
    "            'avg_td_error': np.mean(recent_td_errors),\n",
    "            'loss_trend': np.mean(recent_losses[-20:]) - np.mean(recent_losses[:20]),  # Positive = increasing\n",
    "            'td_error_trend': np.mean(recent_td_errors[-20:]) - np.mean(recent_td_errors[:20]),\n",
    "            'total_training_steps': len(self.training_stats['losses'])\n",
    "        }\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# --- Reward Shaping ---\n",
    "def get_shaped_reward(reward_from_env, board, prev_board, done):\n",
    "    if reward_from_env == -10:  # Invalid move\n",
    "        return -10.0\n",
    "    \n",
    "    # Base reward from merges\n",
    "    log_reward = 0\n",
    "    if reward_from_env > 0:\n",
    "        log_reward = np.log2(reward_from_env)\n",
    "    \n",
    "    # Bonus for reaching higher tiles\n",
    "    max_tile = np.max(board)\n",
    "    tile_bonus = np.log2(max_tile) * 0.5\n",
    "    \n",
    "    # Penalty for game over\n",
    "    game_over_penalty = -50.0 if done else 0.0\n",
    "    \n",
    "    # Bonus for keeping high tiles in corners\n",
    "    corner_bonus = 0\n",
    "    if max_tile >= 128:\n",
    "        corners = [board[0,0], board[0,3], board[3,0], board[3,3]]\n",
    "        if max_tile in corners:\n",
    "            corner_bonus = 2.0\n",
    "    \n",
    "    # Bonus for monotonicity (tiles decreasing from corner)\n",
    "    monotonicity_bonus = calculate_monotonicity(board) * 0.1\n",
    "    \n",
    "    # Penalty for too many small tiles\n",
    "    small_tile_penalty = -np.sum(board == 2) * 0.01\n",
    "    \n",
    "    return float(log_reward + tile_bonus + corner_bonus + monotonicity_bonus + small_tile_penalty + game_over_penalty)\n",
    "\n",
    "def calculate_monotonicity(board):\n",
    "    \"\"\"Calculate how well tiles decrease from corner\"\"\"\n",
    "    score = 0\n",
    "    # Check horizontal monotonicity\n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            if board[i,j] >= board[i,j+1] and board[i,j] != 0:\n",
    "                score += 1\n",
    "    # Check vertical monotonicity  \n",
    "    for i in range(3):\n",
    "        for j in range(4):\n",
    "            if board[i,j] >= board[i+1,j] and board[i,j] != 0:\n",
    "                score += 1\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training to reach 64 tile...\n",
      "*** New best tile: 16 at episode 1! ***\n",
      "*** New best tile: 128 at episode 2! ***\n",
      "Successfully reached 64!\n",
      "Successfully reached 64!\n",
      "==================================================\n",
      "Training to reach 128 tile...\n",
      "*** New best tile: 128 at episode 1! ***\n",
      "Successfully reached 128!\n",
      "Successfully reached 128!\n",
      "==================================================\n",
      "Training to reach 256 tile...\n",
      "*** New best tile: 256 at episode 1! ***\n",
      "Successfully reached 256!\n",
      "Successfully reached 256!\n",
      "==================================================\n",
      "Training to reach 512 tile...\n",
      "*** New best tile: 32 at episode 1! ***\n",
      "*** New best tile: 256 at episode 2! ***\n",
      "Ep 100 | Steps: 153 | Max Tile: 128 | Reward: 878 | Epsilon: 0.9897\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent, best_tile\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m agent, final_best_tile \u001b[38;5;241m=\u001b[39m \u001b[43mcurriculum_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 98\u001b[0m, in \u001b[0;36mcurriculum_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m target_tiles:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining to reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tile...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m     current_best \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_to_reach_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     best_tile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(best_tile, current_best)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_tile \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m target:\n",
      "Cell \u001b[0;32mIn[35], line 20\u001b[0m, in \u001b[0;36mtraining_to_reach_target\u001b[0;34m(agent, env, target_tile, max_episodes)\u001b[0m\n\u001b[1;32m     17\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m next_state, reward_from_env, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Use shaped reward\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 239\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, state_board, valid_actions)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    238\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encode_board(state_board))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    240\u001b[0m masked_q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(q_values\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m valid_actions:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[32], line 140\u001b[0m, in \u001b[0;36mCNN_DQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 140\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def training_to_reach_target(agent, env, target_tile, max_episodes=2000):\n",
    "    # Training loop with better monitoring\n",
    "    target_update_freq = 5  # More frequent updates\n",
    "    best_tile = 0\n",
    "    consecutive_no_improvement = 0\n",
    "\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        episode_training_steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                done = True\n",
    "                continue\n",
    "                \n",
    "            action = agent.act(state, valid_actions)\n",
    "            next_state, reward_from_env, done, _ = env.step(action)\n",
    "            \n",
    "            # Use shaped reward\n",
    "            shaped_reward = get_shaped_reward(reward_from_env, next_state, state, done)\n",
    "            agent.memory.store(state, action, shaped_reward, next_state, done)\n",
    "            \n",
    "            total_reward += shaped_reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Train more frequently\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                agent.train_step()\n",
    "                episode_training_steps += 1\n",
    "            \n",
    "            steps += 1\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        if episode % target_update_freq == 0:\n",
    "            agent.update_target()\n",
    "    \n",
    "        \n",
    "        max_tile = np.max(env.board)\n",
    "        if max_tile > best_tile:\n",
    "            best_tile = max_tile\n",
    "            consecutive_no_improvement = 0\n",
    "            print(f\"*** New best tile: {best_tile} at episode {episode}! ***\")\n",
    "            torch.save(agent.q_net.state_dict(), f\"cnn_dqn_best_{best_tile}.pth\")\n",
    "            if best_tile >= target_tile:\n",
    "                print(f\"Successfully reached {target_tile}!\")\n",
    "                return best_tile\n",
    "        else:\n",
    "            consecutive_no_improvement += 1\n",
    "        \n",
    "        # Early stopping if no improvement\n",
    "        if consecutive_no_improvement > 500:\n",
    "            print(f\"No improvement for 500 episodes, stopping at tile {best_tile}\")\n",
    "            break\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Ep {episode} | Steps: {steps} | Max Tile: {max_tile} | Reward: {int(total_reward)} | Epsilon: {agent.epsilon:.4f}\")\n",
    "    return best_tile\n",
    "\n",
    "# --- Optimized Training Loop ---\n",
    "env = Game2048Env()\n",
    "agent = Agent(\n",
    "    learning_rate=1e-4,  # Higher learning rate\n",
    "    gamma=0.99,\n",
    "    max_memory=50000,  # Larger buffer\n",
    "    batch_size=256,  # Larger batches\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_min=0.01,\n",
    "    epsilon_decay=0.9999,  # Slower decay\n",
    "    per_alpha=0.7,  # Higher priority\n",
    "    per_beta_start=0.5\n",
    ")\n",
    "\n",
    "def curriculum_training():\n",
    "    target_tiles = [64, 128, 256, 512, 1024, 2048]\n",
    "    best_tile = 0\n",
    "    \n",
    "    # Create agent and environment\n",
    "    env = Game2048Env()\n",
    "    agent = Agent(\n",
    "        learning_rate=1e-4,\n",
    "        gamma=0.99,\n",
    "        max_memory=50000,\n",
    "        batch_size=256,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_min=0.01,\n",
    "        epsilon_decay=0.9999,\n",
    "        per_alpha=0.7,\n",
    "        per_beta_start=0.5\n",
    "    )\n",
    "    \n",
    "    for target in target_tiles:\n",
    "        print(f\"Training to reach {target} tile...\")\n",
    "        current_best = training_to_reach_target(agent, env, target)\n",
    "        best_tile = max(best_tile, current_best)\n",
    "        \n",
    "        if best_tile >= target:\n",
    "            print(f\"Successfully reached {target}!\")\n",
    "        else:\n",
    "            print(f\"Could only reach {best_tile}, stopping curriculum\")\n",
    "            break\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Final best tile: {best_tile}\")\n",
    "    return agent, best_tile\n",
    "\n",
    "# Run the training\n",
    "agent, final_best_tile = curriculum_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2048 PyGame Visualizer for DQN Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in /Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages (2.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score: 326\n",
      "Max tile: 128\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "SIZE = 4\n",
    "TILE_SIZE = 100\n",
    "MARGIN = 10\n",
    "WIDTH = SIZE * TILE_SIZE + (SIZE + 1) * MARGIN\n",
    "HEIGHT = WIDTH\n",
    "FONT = pygame.font.SysFont(\"arial\", 32)\n",
    "\n",
    "# Colors\n",
    "BG_COLOR = (187, 173, 160)\n",
    "TILE_COLORS = {\n",
    "    0: (205, 193, 180),\n",
    "    2: (238, 228, 218),\n",
    "    4: (237, 224, 200),\n",
    "    8: (242, 177, 121),\n",
    "    16: (245, 149, 99),\n",
    "    32: (246, 124, 95),\n",
    "    64: (246, 94, 59),\n",
    "    128: (237, 207, 114),\n",
    "    256: (237, 204, 97),\n",
    "    512: (237, 200, 80),\n",
    "    1024: (237, 197, 63),\n",
    "    2048: (237, 194, 46),\n",
    "}\n",
    "TEXT_COLOR_DARK = (119, 110, 101)\n",
    "TEXT_COLOR_LIGHT = (249, 246, 242)\n",
    "\n",
    "# Create screen\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"2048 Visualizer\")\n",
    "\n",
    "def draw_board(board):\n",
    "    screen.fill(BG_COLOR)\n",
    "    for row in range(SIZE):\n",
    "        for col in range(SIZE):\n",
    "            value = board[row][col]\n",
    "            color = TILE_COLORS.get(value, (60, 58, 50))\n",
    "            rect_x = MARGIN + col * (TILE_SIZE + MARGIN)\n",
    "            rect_y = MARGIN + row * (TILE_SIZE + MARGIN)\n",
    "            pygame.draw.rect(screen, color, (rect_x, rect_y, TILE_SIZE, TILE_SIZE))\n",
    "            if value != 0:\n",
    "                text_color = TEXT_COLOR_DARK if value <= 4 else TEXT_COLOR_LIGHT\n",
    "                text = FONT.render(str(value), True, text_color)\n",
    "                text_rect = text.get_rect(center=(rect_x + TILE_SIZE / 2, rect_y + TILE_SIZE / 2))\n",
    "                screen.blit(text, text_rect)\n",
    "    pygame.display.update()\n",
    "\n",
    "# --- Agent Playing Loop ---\n",
    "env = Game2048Env()\n",
    "state = env.reset()\n",
    "done = False\n",
    "running = True\n",
    "\n",
    "while running:\n",
    "    # Handle quit event\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    if not done:\n",
    "        valid_actions = env.get_valid_actions()\n",
    "\n",
    "        if not valid_actions:\n",
    "            done = True\n",
    "            continue\n",
    "\n",
    "        # Get Q-values from model\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(encode_board(state)).unsqueeze(0)  # shape: (1, 16)\n",
    "            q_values = agent.q_net(state_tensor).squeeze().numpy()\n",
    "\n",
    "        # Mask invalid actions\n",
    "        masked_q = np.full_like(q_values, -np.inf)\n",
    "        for a in valid_actions:\n",
    "            masked_q[a] = q_values[a]\n",
    "\n",
    "        action = int(np.argmax(masked_q))\n",
    "\n",
    "        # Step and draw\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        draw_board(env.board)\n",
    "        pygame.time.wait(200)\n",
    "    else:\n",
    "        # Show game over\n",
    "        font = pygame.font.SysFont('arial', 40, bold=True)\n",
    "        text = font.render(f\"Game Over!\", True, (255, 0, 0))\n",
    "        text_rect = text.get_rect(center=(WIDTH / 2, HEIGHT / 2))\n",
    "        screen.blit(text, text_rect)\n",
    "        pygame.display.update()\n",
    "        pygame.time.wait(2000)\n",
    "        running = False\n",
    "\n",
    "pygame.quit()\n",
    "\n",
    "# Log final results\n",
    "print(\"Final score:\", np.sum(env.board))\n",
    "print(\"Max tile:\", np.max(env.board))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectimax Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Game2048Env:\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the game board and add two initial tiles\"\"\"\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "        self._add_random_tile()\n",
    "        self._add_random_tile()\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "        return self.board.copy()\n",
    "\n",
    "    def _add_random_tile(self):\n",
    "        \"\"\"Add a random tile (2 or 4) to an empty position\"\"\"\n",
    "        empty_positions = list(zip(*np.where(self.board == 0)))\n",
    "        if not empty_positions:\n",
    "            return\n",
    "        \n",
    "        row, col = random.choice(empty_positions)\n",
    "        # 90% chance of 2, 10% chance of 4\n",
    "        self.board[row, col] = 2 if random.random() < 0.9 else 4\n",
    "\n",
    "    def _merge_line(self, line):\n",
    "        \"\"\"\n",
    "        Merge a single line (row or column) to the left\n",
    "        Returns: (merged_line, score_gained)\n",
    "        \"\"\"\n",
    "        # Remove zeros\n",
    "        non_zero = line[line != 0]\n",
    "        merged = []\n",
    "        score = 0\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(non_zero):\n",
    "            if i + 1 < len(non_zero) and non_zero[i] == non_zero[i + 1]:\n",
    "                # Merge identical adjacent tiles\n",
    "                merged_value = non_zero[i] * 2\n",
    "                merged.append(merged_value)\n",
    "                score += merged_value\n",
    "                i += 2  # Skip the next tile since it was merged\n",
    "            else:\n",
    "                merged.append(non_zero[i])\n",
    "                i += 1\n",
    "        \n",
    "        # Pad with zeros to maintain original length\n",
    "        merged.extend([0] * (len(line) - len(merged)))\n",
    "        return np.array(merged), score\n",
    "\n",
    "    def _move_left(self, board):\n",
    "        \"\"\"Move all tiles left\"\"\"\n",
    "        new_board = np.zeros_like(board)\n",
    "        total_score = 0\n",
    "        \n",
    "        for row in range(self.size):\n",
    "            new_board[row, :], score = self._merge_line(board[row, :])\n",
    "            total_score += score\n",
    "            \n",
    "        return new_board, total_score\n",
    "\n",
    "    def _move_right(self, board):\n",
    "        \"\"\"Move all tiles right\"\"\"\n",
    "        new_board = np.zeros_like(board)\n",
    "        total_score = 0\n",
    "        \n",
    "        for row in range(self.size):\n",
    "            # Reverse, merge left, then reverse back\n",
    "            reversed_row = board[row, ::-1]\n",
    "            merged_row, score = self._merge_line(reversed_row)\n",
    "            new_board[row, :] = merged_row[::-1]\n",
    "            total_score += score\n",
    "            \n",
    "        return new_board, total_score\n",
    "\n",
    "    def _move_up(self, board):\n",
    "        \"\"\"Move all tiles up\"\"\"\n",
    "        new_board = np.zeros_like(board)\n",
    "        total_score = 0\n",
    "        \n",
    "        for col in range(self.size):\n",
    "            new_board[:, col], score = self._merge_line(board[:, col])\n",
    "            total_score += score\n",
    "            \n",
    "        return new_board, total_score\n",
    "\n",
    "    def _move_down(self, board):\n",
    "        \"\"\"Move all tiles down\"\"\"\n",
    "        new_board = np.zeros_like(board)\n",
    "        total_score = 0\n",
    "        \n",
    "        for col in range(self.size):\n",
    "            # Reverse, merge up, then reverse back\n",
    "            reversed_col = board[::-1, col]\n",
    "            merged_col, score = self._merge_line(reversed_col)\n",
    "            new_board[:, col] = merged_col[::-1]\n",
    "            total_score += score\n",
    "            \n",
    "        return new_board, total_score\n",
    "\n",
    "    def _execute_move(self, board, action):\n",
    "        \"\"\"\n",
    "        Execute a move on the board\n",
    "        Actions: 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\n",
    "        Returns: (new_board, score_gained)\n",
    "        \"\"\"\n",
    "        if action == 0:    # UP\n",
    "            return self._move_up(board)\n",
    "        elif action == 1:  # DOWN\n",
    "            return self._move_down(board)\n",
    "        elif action == 2:  # LEFT\n",
    "            return self._move_left(board)\n",
    "        elif action == 3:  # RIGHT\n",
    "            return self._move_right(board)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}. Must be 0-3.\")\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one game step\n",
    "        Returns: (new_state, reward, done, info)\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return self.board.copy(), 0, True, {}\n",
    "\n",
    "        # Try to execute the move\n",
    "        new_board, reward = self._execute_move(self.board, action)\n",
    "        \n",
    "        # Check if the move actually changed the board\n",
    "        if np.array_equal(new_board, self.board):\n",
    "            # Invalid move - board didn't change\n",
    "            return self.board.copy(), -10, self.done, {}\n",
    "        \n",
    "        # Valid move - update board and add new tile\n",
    "        self.board = new_board\n",
    "        self._add_random_tile()\n",
    "        self.score += reward\n",
    "        \n",
    "        # Check if game is over\n",
    "        if self.is_game_over():\n",
    "            self.done = True\n",
    "            \n",
    "        return self.board.copy(), reward, self.done, {}\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        \"\"\"Return list of valid actions (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT)\"\"\"\n",
    "        valid_actions = []\n",
    "        \n",
    "        for action in range(4):\n",
    "            new_board, _ = self._execute_move(self.board, action)\n",
    "            if not np.array_equal(new_board, self.board):\n",
    "                valid_actions.append(action)\n",
    "                \n",
    "        return valid_actions\n",
    "\n",
    "    def is_game_over(self):\n",
    "        \"\"\"Check if the game is over (no valid moves)\"\"\"\n",
    "        return len(self.get_valid_actions()) == 0\n",
    "\n",
    "    def get_max_tile(self):\n",
    "        \"\"\"Get the highest tile value on the board\"\"\"\n",
    "        return np.max(self.board)\n",
    "\n",
    "    def print_board(self):\n",
    "        \"\"\"Print the current board state\"\"\"\n",
    "        print(\"Current board:\")\n",
    "        print(self.board)\n",
    "        print(f\"Score: {self.score}\")\n",
    "        print(f\"Max tile: {self.get_max_tile()}\")\n",
    "        print(f\"Valid actions: {self.get_valid_actions()}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original board:\n",
      "[[   2    2    4    8]\n",
      " [  16   32   64  128]\n",
      " [ 256  512 1024 2048]\n",
      " [  16    8    2    2]]\n",
      "\n",
      "UP move:\n",
      "[[   2    2    4    8]\n",
      " [  16   32   64  128]\n",
      " [ 256  512 1024 2048]\n",
      " [  16    8    2    2]]\n",
      "Score gained: 0\n",
      "Board changed: False\n",
      "\n",
      "DOWN move:\n",
      "[[   2    2    4    8]\n",
      " [  16   32   64  128]\n",
      " [ 256  512 1024 2048]\n",
      " [  16    8    2    2]]\n",
      "Score gained: 0\n",
      "Board changed: False\n",
      "\n",
      "LEFT move:\n",
      "[[   4    4    8    0]\n",
      " [  16   32   64  128]\n",
      " [ 256  512 1024 2048]\n",
      " [  16    8    4    0]]\n",
      "Score gained: 8\n",
      "Board changed: True\n",
      "\n",
      "RIGHT move:\n",
      "[[   0    4    4    8]\n",
      " [  16   32   64  128]\n",
      " [ 256  512 1024 2048]\n",
      " [   0   16    8    4]]\n",
      "Score gained: 8\n",
      "Board changed: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed implementation\n",
    "env = Game2048Env()\n",
    "env.board = np.array([\n",
    "    [2, 2, 4, 8],\n",
    "    [16, 32, 64, 128], \n",
    "    [256, 512, 1024, 2048],\n",
    "    [16, 8, 2, 2]\n",
    "])\n",
    "\n",
    "print(\"Original board:\")\n",
    "print(env.board)\n",
    "print()\n",
    "\n",
    "# Test all directions\n",
    "for action, name in [(0, \"UP\"), (1, \"DOWN\"), (2, \"LEFT\"), (3, \"RIGHT\")]:\n",
    "    new_board, score = env._execute_move(env.board, action)\n",
    "    print(f\"{name} move:\")\n",
    "    print(new_board)\n",
    "    print(f\"Score gained: {score}\")\n",
    "    print(f\"Board changed: {not np.array_equal(new_board, env.board)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectimaxAgent:\n",
    "    def __init__(self, depth=4):\n",
    "        self.depth = depth\n",
    "        # Create a temporary environment for move simulation\n",
    "        self.temp_env = Game2048Env()\n",
    "\n",
    "    def get_action(self, board):\n",
    "        \"\"\"Get the best action using expectimax algorithm\"\"\"\n",
    "        valid_actions = self.get_valid_actions(board)\n",
    "        if not valid_actions:\n",
    "            return 0  # No valid actions, return default\n",
    "        \n",
    "        _, best_move = self.expectimax(board, self.depth, True)\n",
    "        # Ensure the best move is valid, otherwise pick first valid action\n",
    "        return best_move if best_move in valid_actions else valid_actions[0]\n",
    "\n",
    "    def get_valid_actions(self, board):\n",
    "        \"\"\"Get valid actions for a given board state\"\"\"\n",
    "        valid_actions = []\n",
    "        for action in range(4):\n",
    "            new_board, _ = self.temp_env._execute_move(board, action)\n",
    "            if not np.array_equal(new_board, board):\n",
    "                valid_actions.append(action)\n",
    "        return valid_actions\n",
    "\n",
    "    def expectimax(self, board, depth, is_max):\n",
    "        \"\"\"\n",
    "        Expectimax algorithm implementation\n",
    "        is_max: True for player turn (maximizing), False for random tile placement (expectation)\n",
    "        \"\"\"\n",
    "        if depth == 0 or self.is_terminal(board):\n",
    "            return self.evaluate(board), None\n",
    "\n",
    "        if is_max:\n",
    "            # Player's turn - maximize\n",
    "            best_value = float('-inf')\n",
    "            best_move = None\n",
    "            \n",
    "            for action in range(4):  # 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\n",
    "                new_board, _ = self.temp_env._execute_move(board, action)\n",
    "                if not np.array_equal(new_board, board):  # Valid move\n",
    "                    value, _ = self.expectimax(new_board, depth - 1, False)\n",
    "                    if value > best_value:\n",
    "                        best_value = value\n",
    "                        best_move = action\n",
    "                        \n",
    "            return best_value if best_move is not None else float('-inf'), best_move\n",
    "        else:\n",
    "            # Random tile placement - expectation\n",
    "            empty_positions = list(zip(*np.where(board == 0)))\n",
    "            if not empty_positions:\n",
    "                return self.evaluate(board), None\n",
    "\n",
    "            expected_value = 0\n",
    "            for (row, col) in empty_positions:\n",
    "                for tile_value, probability in [(2, 0.9), (4, 0.1)]:\n",
    "                    new_board = board.copy()\n",
    "                    new_board[row, col] = tile_value\n",
    "                    value, _ = self.expectimax(new_board, depth - 1, True)\n",
    "                    expected_value += probability * value / len(empty_positions)\n",
    "                    \n",
    "            return expected_value, None\n",
    "\n",
    "    def is_terminal(self, board):\n",
    "        \"\"\"Check if the board state is terminal (game over)\"\"\"\n",
    "        # Game is terminal if no empty spaces and no valid moves\n",
    "        if np.any(board == 0):\n",
    "            return False  # Still has empty spaces\n",
    "        return len(self.get_valid_actions(board)) == 0\n",
    "\n",
    "    def evaluate(self, board):\n",
    "        \"\"\"\n",
    "        Evaluation function for board states\n",
    "        Consider multiple factors for better play\n",
    "        \"\"\"\n",
    "        if self.is_terminal(board):\n",
    "            return -1000  # Game over is bad\n",
    "            \n",
    "        # Basic evaluation components\n",
    "        max_tile = np.max(board)\n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        \n",
    "        # Monotonicity bonus (higher tiles should be in corners/edges)\n",
    "        monotonicity_score = self.calculate_monotonicity(board)\n",
    "        \n",
    "        # Smoothness bonus (adjacent tiles should have similar values)\n",
    "        smoothness_score = self.calculate_smoothness(board)\n",
    "        \n",
    "        # Weighted combination\n",
    "        score = (\n",
    "            max_tile * 1.0 +           # Favor higher tiles\n",
    "            empty_tiles * 2.7 +        # Favor more empty spaces\n",
    "            monotonicity_score * 1.0 +  # Favor monotonic arrangements\n",
    "            smoothness_score * 0.1      # Favor smooth transitions\n",
    "        )\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def calculate_monotonicity(self, board):\n",
    "        \"\"\"Calculate monotonicity score - prefer tiles arranged in order\"\"\"\n",
    "        totals = [0, 0, 0, 0]  # up, down, left, right\n",
    "        \n",
    "        for x in range(4):\n",
    "            current = 0\n",
    "            next_pos = 1\n",
    "            while next_pos < 4:\n",
    "                while next_pos < 4 and board[x][next_pos] == 0:\n",
    "                    next_pos += 1\n",
    "                if next_pos >= 4:\n",
    "                    next_pos -= 1\n",
    "                \n",
    "                if board[x][current] != 0 and board[x][next_pos] != 0:\n",
    "                    current_value = np.log2(board[x][current])\n",
    "                    next_value = np.log2(board[x][next_pos])\n",
    "                    \n",
    "                    if current_value > next_value:\n",
    "                        totals[0] += next_value - current_value\n",
    "                    elif next_value > current_value:\n",
    "                        totals[1] += current_value - next_value\n",
    "                        \n",
    "                current = next_pos\n",
    "                next_pos += 1\n",
    "        \n",
    "        for y in range(4):\n",
    "            current = 0\n",
    "            next_pos = 1\n",
    "            while next_pos < 4:\n",
    "                while next_pos < 4 and board[next_pos][y] == 0:\n",
    "                    next_pos += 1\n",
    "                if next_pos >= 4:\n",
    "                    next_pos -= 1\n",
    "                    \n",
    "                if board[current][y] != 0 and board[next_pos][y] != 0:\n",
    "                    current_value = np.log2(board[current][y])\n",
    "                    next_value = np.log2(board[next_pos][y])\n",
    "                    \n",
    "                    if current_value > next_value:\n",
    "                        totals[2] += next_value - current_value\n",
    "                    elif next_value > current_value:\n",
    "                        totals[3] += current_value - next_value\n",
    "                        \n",
    "                current = next_pos\n",
    "                next_pos += 1\n",
    "                \n",
    "        return max(totals[0], totals[1]) + max(totals[2], totals[3])\n",
    "\n",
    "    def calculate_smoothness(self, board):\n",
    "        \"\"\"Calculate smoothness score - prefer similar adjacent values\"\"\"\n",
    "        smoothness = 0\n",
    "        \n",
    "        for x in range(4):\n",
    "            for y in range(4):\n",
    "                if board[x][y] != 0:\n",
    "                    value = np.log2(board[x][y])\n",
    "                    # Check right neighbor\n",
    "                    if y + 1 < 4 and board[x][y + 1] != 0:\n",
    "                        target_value = np.log2(board[x][y + 1])\n",
    "                        smoothness -= abs(value - target_value)\n",
    "                    # Check down neighbor  \n",
    "                    if x + 1 < 4 and board[x + 1][y] != 0:\n",
    "                        target_value = np.log2(board[x + 1][y])\n",
    "                        smoothness -= abs(value - target_value)\n",
    "                        \n",
    "        return smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board:\n",
      "[[   2    2    4    8]\n",
      " [  16   32   64  128]\n",
      " [ 256  512 1024 2048]\n",
      " [  16    8    2    2]]\n",
      "Valid actions: [2, 3]\n",
      "Agent's choice: 3\n",
      "Agent chooses: RIGHT\n"
     ]
    }
   ],
   "source": [
    "# Test the updated ExpectimaxAgent\n",
    "env = Game2048Env()\n",
    "agent = ExpectimaxAgent(depth=3)\n",
    "\n",
    "env.board = np.array([\n",
    "    [2, 2, 4, 8],\n",
    "    [16, 32, 64, 128], \n",
    "    [256, 512, 1024, 2048],\n",
    "    [16, 8, 2, 2]\n",
    "])\n",
    "\n",
    "print(\"Board:\")\n",
    "print(env.board)\n",
    "print(f\"Valid actions: {env.get_valid_actions()}\")\n",
    "print(f\"Agent's choice: {agent.get_action(env.board)}\")\n",
    "\n",
    "# Actions: 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\n",
    "action_names = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "action = agent.get_action(env.board)\n",
    "print(f\"Agent chooses: {action_names[action]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyGame Visualizer for Expectimax Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score: 1608\n",
      "Max tile: 1024\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "SIZE = 4\n",
    "TILE_SIZE = 100\n",
    "MARGIN = 10\n",
    "WIDTH = SIZE * TILE_SIZE + (SIZE + 1) * MARGIN\n",
    "HEIGHT = WIDTH\n",
    "FONT = pygame.font.SysFont(\"arial\", 32)\n",
    "\n",
    "# Colors\n",
    "BG_COLOR = (187, 173, 160)\n",
    "TILE_COLORS = {\n",
    "    0: (205, 193, 180),\n",
    "    2: (238, 228, 218),\n",
    "    4: (237, 224, 200),\n",
    "    8: (242, 177, 121),\n",
    "    16: (245, 149, 99),\n",
    "    32: (246, 124, 95),\n",
    "    64: (246, 94, 59),\n",
    "    128: (237, 207, 114),\n",
    "    256: (237, 204, 97),\n",
    "    512: (237, 200, 80),\n",
    "    1024: (237, 197, 63),\n",
    "    2048: (237, 194, 46),\n",
    "}\n",
    "TEXT_COLOR_DARK = (119, 110, 101)\n",
    "TEXT_COLOR_LIGHT = (249, 246, 242)\n",
    "\n",
    "# Setup display\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"2048 Expectimax\")\n",
    "\n",
    "def draw_board(board):\n",
    "    screen.fill(BG_COLOR)\n",
    "    for row in range(SIZE):\n",
    "        for col in range(SIZE):\n",
    "            value = board[row][col]\n",
    "            color = TILE_COLORS.get(value, (60, 58, 50))\n",
    "            rect_x = MARGIN + col * (TILE_SIZE + MARGIN)\n",
    "            rect_y = MARGIN + row * (TILE_SIZE + MARGIN)\n",
    "            pygame.draw.rect(screen, color, (rect_x, rect_y, TILE_SIZE, TILE_SIZE))\n",
    "            if value != 0:\n",
    "                text_color = TEXT_COLOR_DARK if value <= 4 else TEXT_COLOR_LIGHT\n",
    "                text = FONT.render(str(value), True, text_color)\n",
    "                text_rect = text.get_rect(center=(rect_x + TILE_SIZE / 2, rect_y + TILE_SIZE / 2))\n",
    "                screen.blit(text, text_rect)\n",
    "    pygame.display.update()\n",
    "\n",
    "# --- Expectimax playing loop ---\n",
    "\n",
    "env = Game2048Env()\n",
    "agent = ExpectimaxAgent(depth=3)\n",
    "state = env.reset()\n",
    "done = False\n",
    "running = True\n",
    "\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    if not done:\n",
    "        draw_board(env.board)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            done = True\n",
    "            continue\n",
    "\n",
    "        action = agent.get_action(env.board)\n",
    "        if action not in valid_actions:\n",
    "            print(\"Expectimax chose invalid action, picking random.\")\n",
    "            action = random.choice(valid_actions)\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    else:\n",
    "        font = pygame.font.SysFont('arial', 40, bold=True)\n",
    "        text = font.render(f\"Game Over!\", True, (255, 0, 0))\n",
    "        text_rect = text.get_rect(center=(WIDTH / 2, HEIGHT / 2))\n",
    "        screen.blit(text, text_rect)\n",
    "        pygame.display.update()\n",
    "        pygame.time.wait(2000)\n",
    "        running = False\n",
    "\n",
    "pygame.quit()\n",
    "print(\"Final score:\", np.sum(env.board))\n",
    "print(\"Max tile:\", np.max(env.board))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Super Expectimax Agent ===\n",
      "Move 100: Max tile = 128\n",
      "Move 200: Max tile = 256\n",
      "Move 300: Max tile = 512\n",
      "Move 400: Max tile = 512\n",
      "Move 500: Max tile = 1024\n",
      "Move 600: Max tile = 1024\n",
      "Move 700: Max tile = 1024\n",
      "Move 800: Max tile = 1024\n",
      "Move 900: Max tile = 1024\n",
      "Final results: Max tile = 1024, Score = 15612, Moves = 921\n",
      "Memoization hit rate: 50.16% (1704027/3397286)\n"
     ]
    }
   ],
   "source": [
    "class SuperExpectimaxAgent:\n",
    "    def __init__(self, depth=5):\n",
    "        self.depth = depth\n",
    "        self.temp_env = Game2048Env()\n",
    "        \n",
    "        # Memoization for speed\n",
    "        self.memo = {}\n",
    "        self.memo_hits = 0\n",
    "        self.memo_misses = 0\n",
    "\n",
    "    def get_action(self, board):\n",
    "        \"\"\"Get best action using advanced expectimax\"\"\"\n",
    "        # Clear memo periodically to prevent memory issues\n",
    "        if len(self.memo) > 50000:\n",
    "            self.memo.clear()\n",
    "            \n",
    "        valid_actions = self.get_valid_actions(board)\n",
    "        if not valid_actions:\n",
    "            return 0\n",
    "        \n",
    "        best_value = float('-inf')\n",
    "        best_action = valid_actions[0]\n",
    "        \n",
    "        for action in valid_actions:\n",
    "            new_board, _ = self.temp_env._execute_move(board, action)\n",
    "            value, _ = self.expectimax(new_board, self.depth - 1, False)\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "                \n",
    "        return best_action\n",
    "\n",
    "    def get_valid_actions(self, board):\n",
    "        \"\"\"Get valid actions\"\"\"\n",
    "        valid_actions = []\n",
    "        for action in range(4):\n",
    "            new_board, _ = self.temp_env._execute_move(board, action)\n",
    "            if not np.array_equal(new_board, board):\n",
    "                valid_actions.append(action)\n",
    "        return valid_actions\n",
    "\n",
    "    def expectimax(self, board, depth, is_max):\n",
    "        \"\"\"Expectimax with memoization\"\"\"\n",
    "        # Create hash for memoization\n",
    "        board_hash = hash(board.tobytes())\n",
    "        memo_key = (board_hash, depth, is_max)\n",
    "        \n",
    "        if memo_key in self.memo:\n",
    "            self.memo_hits += 1\n",
    "            return self.memo[memo_key]\n",
    "        \n",
    "        self.memo_misses += 1\n",
    "        \n",
    "        if depth == 0 or self.is_terminal(board):\n",
    "            result = self.evaluate(board), None\n",
    "            self.memo[memo_key] = result\n",
    "            return result\n",
    "\n",
    "        if is_max:\n",
    "            best_value = float('-inf')\n",
    "            best_move = None\n",
    "            \n",
    "            for action in self.get_valid_actions(board):\n",
    "                new_board, _ = self.temp_env._execute_move(board, action)\n",
    "                value, _ = self.expectimax(new_board, depth - 1, False)\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_move = action\n",
    "                    \n",
    "            result = best_value if best_move is not None else float('-inf'), best_move\n",
    "        else:\n",
    "            empty_positions = list(zip(*np.where(board == 0)))\n",
    "            if not empty_positions:\n",
    "                result = self.evaluate(board), None\n",
    "            else:\n",
    "                expected_value = 0\n",
    "                for (row, col) in empty_positions:\n",
    "                    for tile_value, probability in [(2, 0.9), (4, 0.1)]:\n",
    "                        new_board = board.copy()\n",
    "                        new_board[row, col] = tile_value\n",
    "                        value, _ = self.expectimax(new_board, depth - 1, True)\n",
    "                        expected_value += probability * value / len(empty_positions)\n",
    "                result = expected_value, None\n",
    "                \n",
    "        self.memo[memo_key] = result\n",
    "        return result\n",
    "\n",
    "    def is_terminal(self, board):\n",
    "        \"\"\"Check if game is over\"\"\"\n",
    "        if np.any(board == 0):\n",
    "            return False\n",
    "        return len(self.get_valid_actions(board)) == 0\n",
    "\n",
    "    def evaluate(self, board):\n",
    "        \"\"\"\n",
    "        MUCH BETTER evaluation function for reaching 2048\n",
    "        \"\"\"\n",
    "        if self.is_terminal(board):\n",
    "            return -100000  # Severe penalty for game over\n",
    "            \n",
    "        # Core metrics\n",
    "        max_tile = np.max(board)\n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        \n",
    "        # 1. CORNER STRATEGY - This is crucial!\n",
    "        corner_score = self.calculate_corner_score(board)\n",
    "        \n",
    "        # 2. SNAKE PATTERN - Keep tiles in monotonic order\n",
    "        snake_score = self.calculate_snake_score(board)\n",
    "        \n",
    "        # 3. EDGE WEIGHT - Prefer high tiles on edges\n",
    "        edge_score = self.calculate_edge_score(board)\n",
    "        \n",
    "        # 4. CLUSTERING - Penalize scattered high tiles\n",
    "        clustering_penalty = self.calculate_clustering_penalty(board)\n",
    "        \n",
    "        # 5. MERGE POTENTIAL - Reward potential merges\n",
    "        merge_potential = self.calculate_merge_potential(board)\n",
    "        \n",
    "        # 6. SMOOTHNESS - Penalize large gaps between adjacent tiles\n",
    "        smoothness_penalty = self.calculate_smoothness_penalty(board)\n",
    "        \n",
    "        # OPTIMIZED WEIGHTS for reaching 2048\n",
    "        score = (\n",
    "            max_tile * 1.0 +                    # Base tile value\n",
    "            empty_tiles * 100.0 +               # Empty spaces are CRUCIAL\n",
    "            corner_score * 1000.0 +             # Corner strategy is KEY\n",
    "            snake_score * 200.0 +               # Snake pattern helps a lot\n",
    "            edge_score * 50.0 +                 # Edge preference\n",
    "            clustering_penalty * -500.0 +       # Avoid scattered tiles\n",
    "            merge_potential * 80.0 +            # Reward merge opportunities\n",
    "            smoothness_penalty * -30.0          # Smooth transitions\n",
    "        )\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def calculate_corner_score(self, board):\n",
    "        \"\"\"Enhanced corner scoring\"\"\"\n",
    "        max_val = np.max(board)\n",
    "        \n",
    "        # Find position of max tile\n",
    "        max_pos = np.unravel_index(np.argmax(board), board.shape)\n",
    "        \n",
    "        # HUGE bonus if max tile is in corner\n",
    "        if max_pos in [(0,0), (0,3), (3,0), (3,3)]:\n",
    "            bonus = max_val * 10\n",
    "            \n",
    "            # Extra bonus for specific corner preferences\n",
    "            if max_pos == (0,0):  # Top-left preferred\n",
    "                bonus *= 1.5\n",
    "            elif max_pos == (3,0):  # Bottom-left second choice\n",
    "                bonus *= 1.3\n",
    "                \n",
    "            return bonus\n",
    "        \n",
    "        # Medium bonus if on edge\n",
    "        row, col = max_pos\n",
    "        if row == 0 or row == 3 or col == 0 or col == 3:\n",
    "            return max_val * 3\n",
    "            \n",
    "        # Penalty if in middle\n",
    "        return -max_val * 2\n",
    "\n",
    "    def calculate_snake_score(self, board):\n",
    "        \"\"\"Calculate snake pattern score\"\"\"\n",
    "        # Define snake patterns from each corner\n",
    "        patterns = {\n",
    "            'top_left': [\n",
    "                (0,0), (0,1), (0,2), (0,3),\n",
    "                (1,3), (1,2), (1,1), (1,0),\n",
    "                (2,0), (2,1), (2,2), (2,3),\n",
    "                (3,3), (3,2), (3,1), (3,0)\n",
    "            ],\n",
    "            'top_right': [\n",
    "                (0,3), (0,2), (0,1), (0,0),\n",
    "                (1,0), (1,1), (1,2), (1,3),\n",
    "                (2,3), (2,2), (2,1), (2,0),\n",
    "                (3,0), (3,1), (3,2), (3,3)\n",
    "            ],\n",
    "            'bottom_left': [\n",
    "                (3,0), (3,1), (3,2), (3,3),\n",
    "                (2,3), (2,2), (2,1), (2,0),\n",
    "                (1,0), (1,1), (1,2), (1,3),\n",
    "                (0,3), (0,2), (0,1), (0,0)\n",
    "            ],\n",
    "            'bottom_right': [\n",
    "                (3,3), (3,2), (3,1), (3,0),\n",
    "                (2,0), (2,1), (2,2), (2,3),\n",
    "                (1,3), (1,2), (1,1), (1,0),\n",
    "                (0,0), (0,1), (0,2), (0,3)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        best_score = 0\n",
    "        for pattern in patterns.values():\n",
    "            score = 0\n",
    "            for i in range(len(pattern) - 1):\n",
    "                r1, c1 = pattern[i]\n",
    "                r2, c2 = pattern[i + 1]\n",
    "                \n",
    "                if board[r1, c1] != 0 and board[r2, c2] != 0:\n",
    "                    # Reward decreasing pattern\n",
    "                    if board[r1, c1] >= board[r2, c2]:\n",
    "                        score += min(board[r1, c1], board[r2, c2])\n",
    "                    else:\n",
    "                        score -= abs(board[r1, c1] - board[r2, c2])\n",
    "                        \n",
    "            best_score = max(best_score, score)\n",
    "            \n",
    "        return best_score\n",
    "\n",
    "    def calculate_edge_score(self, board):\n",
    "        \"\"\"Reward high values on edges\"\"\"\n",
    "        edge_score = 0\n",
    "        \n",
    "        # Top and bottom edges\n",
    "        for col in range(4):\n",
    "            edge_score += board[0, col] * 2  # Top edge\n",
    "            edge_score += board[3, col] * 2  # Bottom edge\n",
    "            \n",
    "        # Left and right edges  \n",
    "        for row in range(4):\n",
    "            edge_score += board[row, 0] * 2  # Left edge\n",
    "            edge_score += board[row, 3] * 2  # Right edge\n",
    "            \n",
    "        return edge_score\n",
    "\n",
    "    def calculate_clustering_penalty(self, board):\n",
    "        \"\"\"Penalize high tiles that are isolated\"\"\"\n",
    "        penalty = 0\n",
    "        \n",
    "        for row in range(4):\n",
    "            for col in range(4):\n",
    "                if board[row, col] >= 64:  # High tiles\n",
    "                    # Check if surrounded by much smaller tiles\n",
    "                    neighbors = []\n",
    "                    for dr, dc in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "                        nr, nc = row + dr, col + dc\n",
    "                        if 0 <= nr < 4 and 0 <= nc < 4:\n",
    "                            neighbors.append(board[nr, nc])\n",
    "                    \n",
    "                    if neighbors:\n",
    "                        avg_neighbor = sum(neighbors) / len(neighbors)\n",
    "                        if avg_neighbor < board[row, col] / 4:  # Much smaller neighbors\n",
    "                            penalty += board[row, col]\n",
    "                            \n",
    "        return penalty\n",
    "\n",
    "    def calculate_merge_potential(self, board):\n",
    "        \"\"\"Calculate potential for merges\"\"\"\n",
    "        potential = 0\n",
    "        \n",
    "        # Check horizontal merges\n",
    "        for row in range(4):\n",
    "            for col in range(3):\n",
    "                if board[row, col] != 0 and board[row, col] == board[row, col + 1]:\n",
    "                    potential += board[row, col] * 2\n",
    "                    \n",
    "        # Check vertical merges\n",
    "        for row in range(3):\n",
    "            for col in range(4):\n",
    "                if board[row, col] != 0 and board[row, col] == board[row + 1, col]:\n",
    "                    potential += board[row, col] * 2\n",
    "                    \n",
    "        return potential\n",
    "\n",
    "    def calculate_smoothness_penalty(self, board):\n",
    "        \"\"\"Penalty for rough transitions\"\"\"\n",
    "        penalty = 0\n",
    "        \n",
    "        for row in range(4):\n",
    "            for col in range(3):\n",
    "                if board[row, col] != 0 and board[row, col + 1] != 0:\n",
    "                    diff = abs(np.log2(board[row, col]) - np.log2(board[row, col + 1]))\n",
    "                    penalty += diff ** 2  # Quadratic penalty\n",
    "                    \n",
    "        for row in range(3):\n",
    "            for col in range(4):\n",
    "                if board[row, col] != 0 and board[row + 1, col] != 0:\n",
    "                    diff = abs(np.log2(board[row, col]) - np.log2(board[row + 1, col]))\n",
    "                    penalty += diff ** 2  # Quadratic penalty\n",
    "                    \n",
    "        return penalty\n",
    "\n",
    "# Test the super agent\n",
    "print(\"=== Testing Super Expectimax Agent ===\")\n",
    "super_agent = SuperExpectimaxAgent(depth=5)\n",
    "\n",
    "# Quick test function\n",
    "def quick_test_agent(agent, max_moves=2000):\n",
    "    \"\"\"Quick test without visualization\"\"\"\n",
    "    env = Game2048Env()\n",
    "    state = env.reset()\n",
    "    moves = 0\n",
    "    \n",
    "    while not env.done and moves < max_moves:\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            break\n",
    "            \n",
    "        action = agent.get_action(env.board)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        moves += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if moves % 100 == 0:\n",
    "            print(f\"Move {moves}: Max tile = {env.get_max_tile()}\")\n",
    "            \n",
    "        # Stop if we reach 2048!\n",
    "        if env.get_max_tile() >= 2048:\n",
    "            print(f\" SUCCESS! Reached 2048 in {moves} moves! \")\n",
    "            break\n",
    "    \n",
    "    return env.get_max_tile(), env.score, moves\n",
    "\n",
    "# Run the test\n",
    "max_tile, score, moves = quick_test_agent(super_agent)\n",
    "print(f\"Final results: Max tile = {max_tile}, Score = {score}, Moves = {moves}\")\n",
    "\n",
    "if hasattr(super_agent, 'memo_hits'):\n",
    "    total_calls = super_agent.memo_hits + super_agent.memo_misses\n",
    "    hit_rate = super_agent.memo_hits / total_calls if total_calls > 0 else 0\n",
    "    print(f\"Memoization hit rate: {hit_rate:.2%} ({super_agent.memo_hits}/{total_calls})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Ultimate Expectimax Agent ===\n",
      "\n",
      "=== Run 1/3 ===\n",
      "Move 200: Max tile = 256\n",
      "Move 400: Max tile = 512\n",
      "Move 600: Max tile = 1024\n",
      "Move 800: Max tile = 1024\n",
      " SUCCESS! Reached 2048 in 980 moves! \n",
      "Result: Max tile = 2048, Score = 20436\n",
      "\n",
      "=== Run 2/3 ===\n",
      "Move 200: Max tile = 256\n",
      "Move 400: Max tile = 512\n",
      "Move 600: Max tile = 512\n",
      "Move 800: Max tile = 1024\n",
      "Result: Max tile = 1024, Score = 15148\n",
      "\n",
      "=== Run 3/3 ===\n",
      "Move 200: Max tile = 256\n",
      "Move 400: Max tile = 512\n",
      "Move 600: Max tile = 512\n",
      "Result: Max tile = 512, Score = 11016\n",
      "\n",
      "=== SUMMARY OF 3 RUNS ===\n",
      "Success rate (2048): 33.3%\n",
      "Average max tile: 1195\n",
      "Best max tile: 2048\n",
      "Average score: 15533\n",
      "Best score: 20436\n"
     ]
    }
   ],
   "source": [
    "class UltimateExpectimaxAgent:\n",
    "    def __init__(self, depth=6):\n",
    "        self.depth = depth\n",
    "        self.temp_env = Game2048Env()\n",
    "        \n",
    "        # Advanced memoization with better cache management\n",
    "        self.memo = {}\n",
    "        self.memo_hits = 0\n",
    "        self.memo_misses = 0\n",
    "        \n",
    "        # Adaptive depth based on game state\n",
    "        self.adaptive_depth = True\n",
    "        \n",
    "        # Endgame detection threshold\n",
    "        self.endgame_threshold = 8  # When fewer than 8 empty spaces\n",
    "        \n",
    "        # Opening book for early game\n",
    "        self.opening_moves = {\n",
    "            # Start by going to corners\n",
    "            hash(np.array([[2, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 2]]).tobytes()): 2,  # LEFT\n",
    "            hash(np.array([[2, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [2, 0, 0, 0]]).tobytes()): 1,  # DOWN\n",
    "        }\n",
    "\n",
    "    def get_action(self, board):\n",
    "        \"\"\"Enhanced action selection with adaptive strategies\"\"\"\n",
    "        # Clear memo periodically\n",
    "        if len(self.memo) > 100000:\n",
    "            self.memo.clear()\n",
    "            \n",
    "        # Check opening book first\n",
    "        board_hash = hash(board.tobytes())\n",
    "        if board_hash in self.opening_moves:\n",
    "            return self.opening_moves[board_hash]\n",
    "            \n",
    "        valid_actions = self.get_valid_actions(board)\n",
    "        if not valid_actions:\n",
    "            return 0\n",
    "        \n",
    "        # Adaptive depth based on game state\n",
    "        current_depth = self.get_adaptive_depth(board)\n",
    "        \n",
    "        best_value = float('-inf')\n",
    "        best_action = valid_actions[0]\n",
    "        \n",
    "        # Try actions in smart order (corner-preserving moves first)\n",
    "        ordered_actions = self.order_actions(board, valid_actions)\n",
    "        \n",
    "        for action in ordered_actions:\n",
    "            new_board, _ = self.temp_env._execute_move(board, action)\n",
    "            value, _ = self.expectimax(new_board, current_depth - 1, False)\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "                \n",
    "        return best_action\n",
    "\n",
    "    def get_adaptive_depth(self, board):\n",
    "        \"\"\"Adjust search depth based on game state\"\"\"\n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        max_tile = np.max(board)\n",
    "        \n",
    "        # Deeper search in critical situations\n",
    "        if empty_tiles <= 4:  # Endgame\n",
    "            return min(8, self.depth + 2)\n",
    "        elif empty_tiles <= 8:  # Mid-late game\n",
    "            return self.depth + 1\n",
    "        elif max_tile >= 512:  # High value tiles present\n",
    "            return self.depth\n",
    "        else:  # Early game\n",
    "            return max(4, self.depth - 1)\n",
    "\n",
    "    def order_actions(self, board, valid_actions):\n",
    "        \"\"\"Order actions by priority (corner-preserving first)\"\"\"\n",
    "        max_tile = np.max(board)\n",
    "        max_pos = np.unravel_index(np.argmax(board), board.shape)\n",
    "        \n",
    "        # Priority based on max tile position\n",
    "        if max_pos == (0, 0):  # Top-left corner\n",
    "            priority = [2, 1, 0, 3]  # LEFT, DOWN, UP, RIGHT\n",
    "        elif max_pos == (0, 3):  # Top-right corner\n",
    "            priority = [3, 1, 0, 2]  # RIGHT, DOWN, UP, LEFT\n",
    "        elif max_pos == (3, 0):  # Bottom-left corner\n",
    "            priority = [2, 0, 1, 3]  # LEFT, UP, DOWN, RIGHT\n",
    "        elif max_pos == (3, 3):  # Bottom-right corner\n",
    "            priority = [3, 0, 1, 2]  # RIGHT, UP, DOWN, LEFT\n",
    "        else:  # Max tile not in corner - try to move it there\n",
    "            priority = [2, 1, 0, 3]  # Default to LEFT priority\n",
    "            \n",
    "        # Sort valid actions by priority\n",
    "        return sorted(valid_actions, key=lambda x: priority.index(x) if x in priority else 999)\n",
    "\n",
    "    def expectimax(self, board, depth, is_max):\n",
    "        \"\"\"Enhanced expectimax with alpha-beta style pruning\"\"\"\n",
    "        board_hash = hash(board.tobytes())\n",
    "        memo_key = (board_hash, depth, is_max)\n",
    "        \n",
    "        if memo_key in self.memo:\n",
    "            self.memo_hits += 1\n",
    "            return self.memo[memo_key]\n",
    "        \n",
    "        self.memo_misses += 1\n",
    "        \n",
    "        if depth == 0 or self.is_terminal(board):\n",
    "            result = self.evaluate(board), None\n",
    "            self.memo[memo_key] = result\n",
    "            return result\n",
    "\n",
    "        if is_max:\n",
    "            best_value = float('-inf')\n",
    "            best_move = None\n",
    "            \n",
    "            actions = self.order_actions(board, self.get_valid_actions(board))\n",
    "            \n",
    "            for action in actions:\n",
    "                new_board, _ = self.temp_env._execute_move(board, action)\n",
    "                value, _ = self.expectimax(new_board, depth - 1, False)\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_move = action\n",
    "                    \n",
    "            result = best_value if best_move is not None else float('-inf'), best_move\n",
    "        else:\n",
    "            empty_positions = list(zip(*np.where(board == 0)))\n",
    "            if not empty_positions:\n",
    "                result = self.evaluate(board), None\n",
    "            else:\n",
    "                expected_value = 0\n",
    "                # Sample fewer positions if too many empty tiles (performance optimization)\n",
    "                if len(empty_positions) > 6:\n",
    "                    empty_positions = random.sample(empty_positions, 6)\n",
    "                    \n",
    "                for (row, col) in empty_positions:\n",
    "                    for tile_value, probability in [(2, 0.9), (4, 0.1)]:\n",
    "                        new_board = board.copy()\n",
    "                        new_board[row, col] = tile_value\n",
    "                        value, _ = self.expectimax(new_board, depth - 1, True)\n",
    "                        expected_value += probability * value / len(empty_positions)\n",
    "                result = expected_value, None\n",
    "                \n",
    "        self.memo[memo_key] = result\n",
    "        return result\n",
    "\n",
    "    def get_valid_actions(self, board):\n",
    "        \"\"\"Get valid actions\"\"\"\n",
    "        valid_actions = []\n",
    "        for action in range(4):\n",
    "            new_board, _ = self.temp_env._execute_move(board, action)\n",
    "            if not np.array_equal(new_board, board):\n",
    "                valid_actions.append(action)\n",
    "        return valid_actions\n",
    "\n",
    "    def is_terminal(self, board):\n",
    "        \"\"\"Enhanced terminal detection\"\"\"\n",
    "        if np.any(board == 0):\n",
    "            return False\n",
    "        return len(self.get_valid_actions(board)) == 0\n",
    "\n",
    "    def evaluate(self, board):\n",
    "        \"\"\"\n",
    "        ULTIMATE evaluation function - heavily tuned for 2048\n",
    "        \"\"\"\n",
    "        if self.is_terminal(board):\n",
    "            return -1000000  # Massive penalty for game over\n",
    "            \n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        max_tile = np.max(board)\n",
    "        \n",
    "        # CRITICAL: If we're close to 2048, prioritize that above all else\n",
    "        if max_tile >= 1024:\n",
    "            return self.evaluate_endgame(board)\n",
    "        \n",
    "        # Core components\n",
    "        corner_score = self.calculate_corner_score(board)\n",
    "        snake_score = self.calculate_snake_score(board)\n",
    "        edge_score = self.calculate_edge_score(board)\n",
    "        clustering_penalty = self.calculate_clustering_penalty(board)\n",
    "        merge_potential = self.calculate_merge_potential(board)\n",
    "        smoothness_penalty = self.calculate_smoothness_penalty(board)\n",
    "        \n",
    "        # TUNED WEIGHTS - heavily favor corner strategy and empty spaces\n",
    "        score = (\n",
    "            max_tile * 1.0 +                    \n",
    "            empty_tiles * 200.0 +               # INCREASED: Empty spaces crucial\n",
    "            corner_score * 2000.0 +             # INCREASED: Corner strategy essential\n",
    "            snake_score * 300.0 +               # INCREASED: Snake pattern\n",
    "            edge_score * 75.0 +                 \n",
    "            clustering_penalty * -800.0 +       # INCREASED: Avoid scattered tiles\n",
    "            merge_potential * 150.0 +           # INCREASED: Merge opportunities\n",
    "            smoothness_penalty * -50.0          \n",
    "        )\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def evaluate_endgame(self, board):\n",
    "        \"\"\"Special evaluation for when max tile >= 1024\"\"\"\n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        max_tile = np.max(board)\n",
    "        \n",
    "        # Find the 1024 tile position\n",
    "        max_pos = np.unravel_index(np.argmax(board), board.shape)\n",
    "        \n",
    "        # MASSIVE bonus if 1024 is in corner\n",
    "        if max_pos in [(0,0), (0,3), (3,0), (3,3)]:\n",
    "            corner_bonus = max_tile * 50  # 50x multiplier!\n",
    "        else:\n",
    "            corner_bonus = -max_tile * 10  # Heavy penalty if not in corner\n",
    "            \n",
    "        # Check for 1024 merge potential (path to 2048!)\n",
    "        merge_1024_bonus = 0\n",
    "        for dr, dc in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "            nr, nc = max_pos[0] + dr, max_pos[1] + dc\n",
    "            if 0 <= nr < 4 and 0 <= nc < 4:\n",
    "                if board[nr, nc] == 512:  # Can merge to make 2048!\n",
    "                    merge_1024_bonus = 100000  # HUGE bonus\n",
    "                    break\n",
    "        \n",
    "        # Conservative play - preserve empty spaces at all costs\n",
    "        empty_bonus = empty_tiles * 1000  # 5x normal weight\n",
    "        \n",
    "        # Penalize any risky moves heavily\n",
    "        risk_penalty = 0\n",
    "        if empty_tiles <= 3:\n",
    "            risk_penalty = -50000  # Very risky state\n",
    "        elif empty_tiles <= 5:\n",
    "            risk_penalty = -10000  # Somewhat risky\n",
    "            \n",
    "        return corner_bonus + merge_1024_bonus + empty_bonus + risk_penalty\n",
    "\n",
    "    # [Keep all the existing heuristic methods but with small improvements]\n",
    "    def calculate_corner_score(self, board):\n",
    "        \"\"\"Enhanced corner scoring with stronger preferences\"\"\"\n",
    "        max_val = np.max(board)\n",
    "        max_pos = np.unravel_index(np.argmax(board), board.shape)\n",
    "        \n",
    "        if max_pos in [(0,0), (0,3), (3,0), (3,3)]:\n",
    "            bonus = max_val * 15  # Increased from 10\n",
    "            if max_pos == (0,0):  # Top-left preferred\n",
    "                bonus *= 2.0  # Increased from 1.5\n",
    "            elif max_pos == (3,0):  # Bottom-left second choice\n",
    "                bonus *= 1.8  # Increased from 1.3\n",
    "            return bonus\n",
    "        \n",
    "        row, col = max_pos\n",
    "        if row == 0 or row == 3 or col == 0 or col == 3:\n",
    "            return max_val * 5  # Increased from 3\n",
    "            \n",
    "        return -max_val * 5  # Increased penalty from 2\n",
    "\n",
    "    def calculate_snake_score(self, board):\n",
    "        \"\"\"Keep existing implementation\"\"\"\n",
    "        patterns = {\n",
    "            'top_left': [\n",
    "                (0,0), (0,1), (0,2), (0,3),\n",
    "                (1,3), (1,2), (1,1), (1,0),\n",
    "                (2,0), (2,1), (2,2), (2,3),\n",
    "                (3,3), (3,2), (3,1), (3,0)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        best_score = 0\n",
    "        for pattern in patterns.values():\n",
    "            score = 0\n",
    "            for i in range(len(pattern) - 1):\n",
    "                r1, c1 = pattern[i]\n",
    "                r2, c2 = pattern[i + 1]\n",
    "                \n",
    "                if board[r1, c1] != 0 and board[r2, c2] != 0:\n",
    "                    if board[r1, c1] >= board[r2, c2]:\n",
    "                        score += min(board[r1, c1], board[r2, c2])\n",
    "                    else:\n",
    "                        score -= abs(board[r1, c1] - board[r2, c2])\n",
    "                        \n",
    "            best_score = max(best_score, score)\n",
    "            \n",
    "        return best_score\n",
    "\n",
    "    def calculate_edge_score(self, board):\n",
    "        \"\"\"Keep existing implementation\"\"\"\n",
    "        edge_score = 0\n",
    "        for col in range(4):\n",
    "            edge_score += board[0, col] * 2\n",
    "            edge_score += board[3, col] * 2\n",
    "        for row in range(4):\n",
    "            edge_score += board[row, 0] * 2\n",
    "            edge_score += board[row, 3] * 2\n",
    "        return edge_score\n",
    "\n",
    "    def calculate_clustering_penalty(self, board):\n",
    "        \"\"\"Keep existing implementation\"\"\"\n",
    "        penalty = 0\n",
    "        for row in range(4):\n",
    "            for col in range(4):\n",
    "                if board[row, col] >= 64:\n",
    "                    neighbors = []\n",
    "                    for dr, dc in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "                        nr, nc = row + dr, col + dc\n",
    "                        if 0 <= nr < 4 and 0 <= nc < 4:\n",
    "                            neighbors.append(board[nr, nc])\n",
    "                    \n",
    "                    if neighbors:\n",
    "                        avg_neighbor = sum(neighbors) / len(neighbors)\n",
    "                        if avg_neighbor < board[row, col] / 4:\n",
    "                            penalty += board[row, col]\n",
    "        return penalty\n",
    "\n",
    "    def calculate_merge_potential(self, board):\n",
    "        \"\"\"Enhanced to prioritize high-value merges\"\"\"\n",
    "        potential = 0\n",
    "        \n",
    "        for row in range(4):\n",
    "            for col in range(3):\n",
    "                if board[row, col] != 0 and board[row, col] == board[row, col + 1]:\n",
    "                    merge_value = board[row, col] * 2\n",
    "                    # Extra bonus for high-value merges\n",
    "                    if merge_value >= 512:\n",
    "                        potential += merge_value * 3\n",
    "                    else:\n",
    "                        potential += merge_value\n",
    "                        \n",
    "        for row in range(3):\n",
    "            for col in range(4):\n",
    "                if board[row, col] != 0 and board[row, col] == board[row + 1, col]:\n",
    "                    merge_value = board[row, col] * 2\n",
    "                    if merge_value >= 512:\n",
    "                        potential += merge_value * 3\n",
    "                    else:\n",
    "                        potential += merge_value\n",
    "                        \n",
    "        return potential\n",
    "\n",
    "    def calculate_smoothness_penalty(self, board):\n",
    "        \"\"\"Keep existing implementation\"\"\"\n",
    "        penalty = 0\n",
    "        for row in range(4):\n",
    "            for col in range(3):\n",
    "                if board[row, col] != 0 and board[row, col + 1] != 0:\n",
    "                    diff = abs(np.log2(board[row, col]) - np.log2(board[row, col + 1]))\n",
    "                    penalty += diff ** 2\n",
    "                    \n",
    "        for row in range(3):\n",
    "            for col in range(4):\n",
    "                if board[row, col] != 0 and board[row + 1, col] != 0:\n",
    "                    diff = abs(np.log2(board[row, col]) - np.log2(board[row + 1, col]))\n",
    "                    penalty += diff ** 2\n",
    "        return penalty\n",
    "\n",
    "# Test multiple runs to find best strategy\n",
    "def test_multiple_runs(agent_class, num_runs=5):\n",
    "    \"\"\"Run multiple games and report statistics\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n=== Run {run + 1}/{num_runs} ===\")\n",
    "        agent = agent_class(depth=5)\n",
    "        \n",
    "        env = Game2048Env()\n",
    "        state = env.reset()\n",
    "        moves = 0\n",
    "        \n",
    "        while not env.done and moves < 2000:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            action = agent.get_action(env.board)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            moves += 1\n",
    "            \n",
    "            if moves % 200 == 0:\n",
    "                print(f\"Move {moves}: Max tile = {env.get_max_tile()}\")\n",
    "                \n",
    "            if env.get_max_tile() >= 2048:\n",
    "                print(f\" SUCCESS! Reached 2048 in {moves} moves! \")\n",
    "                break\n",
    "        \n",
    "        result = {\n",
    "            'max_tile': env.get_max_tile(),\n",
    "            'score': env.score,\n",
    "            'moves': moves,\n",
    "            'reached_2048': env.get_max_tile() >= 2048\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Result: Max tile = {result['max_tile']}, Score = {result['score']}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== SUMMARY OF {num_runs} RUNS ===\")\n",
    "    max_tiles = [r['max_tile'] for r in results]\n",
    "    scores = [r['score'] for r in results]\n",
    "    success_rate = sum(1 for r in results if r['reached_2048']) / num_runs\n",
    "    \n",
    "    print(f\"Success rate (2048): {success_rate:.1%}\")\n",
    "    print(f\"Average max tile: {np.mean(max_tiles):.0f}\")\n",
    "    print(f\"Best max tile: {max(max_tiles)}\")\n",
    "    print(f\"Average score: {np.mean(scores):.0f}\")\n",
    "    print(f\"Best score: {max(scores)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the ultimate test\n",
    "print(\"=== Testing Ultimate Expectimax Agent ===\")\n",
    "results = test_multiple_runs(UltimateExpectimaxAgent, num_runs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting visual gameplay...\n",
      "Final result: Max tile = 256, Score = 5956, Moves = 439\n"
     ]
    }
   ],
   "source": [
    "# Pygame visualization for the fixed agent\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "def visualize_agent_gameplay(agent, delay=0.5):\n",
    "    \"\"\"Visualize agent playing with pygame\"\"\"\n",
    "    pygame.init()\n",
    "    \n",
    "    # Constants\n",
    "    SIZE = 4\n",
    "    TILE_SIZE = 100\n",
    "    MARGIN = 10\n",
    "    WIDTH = SIZE * TILE_SIZE + (SIZE + 1) * MARGIN\n",
    "    HEIGHT = WIDTH + 100  # Extra space for text\n",
    "    FONT = pygame.font.SysFont(\"arial\", 32)\n",
    "    INFO_FONT = pygame.font.SysFont(\"arial\", 24)\n",
    "\n",
    "    # Colors\n",
    "    BG_COLOR = (187, 173, 160)\n",
    "    TILE_COLORS = {\n",
    "        0: (205, 193, 180), 2: (238, 228, 218), 4: (237, 224, 200),\n",
    "        8: (242, 177, 121), 16: (245, 149, 99), 32: (246, 124, 95),\n",
    "        64: (246, 94, 59), 128: (237, 207, 114), 256: (237, 204, 97),\n",
    "        512: (237, 200, 80), 1024: (237, 197, 63), 2048: (237, 194, 46),\n",
    "        4096: (237, 194, 46)\n",
    "    }\n",
    "    TEXT_COLOR_DARK = (119, 110, 101)\n",
    "    TEXT_COLOR_LIGHT = (249, 246, 242)\n",
    "\n",
    "    screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    pygame.display.set_caption(\"2048 AI Agent\")\n",
    "\n",
    "    def draw_game_state(env, move_count):\n",
    "        screen.fill(BG_COLOR)\n",
    "        \n",
    "        # Draw board\n",
    "        for row in range(SIZE):\n",
    "            for col in range(SIZE):\n",
    "                value = env.board[row][col]\n",
    "                color = TILE_COLORS.get(value, (60, 58, 50))\n",
    "                rect_x = MARGIN + col * (TILE_SIZE + MARGIN)\n",
    "                rect_y = MARGIN + row * (TILE_SIZE + MARGIN)\n",
    "                pygame.draw.rect(screen, color, (rect_x, rect_y, TILE_SIZE, TILE_SIZE))\n",
    "                \n",
    "                if value != 0:\n",
    "                    text_color = TEXT_COLOR_DARK if value <= 4 else TEXT_COLOR_LIGHT\n",
    "                    text = FONT.render(str(value), True, text_color)\n",
    "                    text_rect = text.get_rect(center=(rect_x + TILE_SIZE / 2, rect_y + TILE_SIZE / 2))\n",
    "                    screen.blit(text, text_rect)\n",
    "        \n",
    "        # Draw info\n",
    "        info_y = WIDTH + 10\n",
    "        info_text = INFO_FONT.render(f\"Move: {move_count} | Score: {env.score} | Max: {env.get_max_tile()}\", True, (0, 0, 0))\n",
    "        screen.blit(info_text, (10, info_y))\n",
    "        \n",
    "        pygame.display.update()\n",
    "\n",
    "    # Run game\n",
    "    env = Game2048Env()\n",
    "    state = env.reset()\n",
    "    move_count = 0\n",
    "    running = True\n",
    "\n",
    "    while running and not env.done:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            break\n",
    "\n",
    "        action = agent.get_action(env.board)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        move_count += 1\n",
    "        \n",
    "        draw_game_state(env, move_count)\n",
    "        time.sleep(delay)\n",
    "\n",
    "        if env.get_max_tile() >= 2048:\n",
    "            print(\" REACHED 2048! \")\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "    # Show final result\n",
    "    final_font = pygame.font.SysFont('arial', 40, bold=True)\n",
    "    if env.get_max_tile() >= 2048:\n",
    "        text = final_font.render(\"SUCCESS! Reached 2048!\", True, (0, 255, 0))\n",
    "    else:\n",
    "        text = final_font.render(f\"Game Over. Max: {env.get_max_tile()}\", True, (255, 0, 0))\n",
    "    \n",
    "    text_rect = text.get_rect(center=(WIDTH / 2, HEIGHT / 2))\n",
    "    screen.blit(text, text_rect)\n",
    "    pygame.display.update()\n",
    "    time.sleep(3)\n",
    "\n",
    "    pygame.quit()\n",
    "    return env.get_max_tile(), env.score, move_count\n",
    "\n",
    "# Run visualization\n",
    "print(\"Starting visual gameplay...\")\n",
    "max_tile, score, moves = visualize_agent_gameplay(super_agent, delay=0.1)\n",
    "print(f\"Final result: Max tile = {max_tile}, Score = {score}, Moves = {moves}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting visual gameplay...\n",
      "Final result: Max tile = 256, Score = 5956, Moves = 439\n"
     ]
    }
   ],
   "source": [
    "# Pygame visualization for the fixed agent\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "def visualize_agent_gameplay(agent, delay=0.5):\n",
    "    \"\"\"Visualize agent playing with pygame\"\"\"\n",
    "    pygame.init()\n",
    "    \n",
    "    # Constants\n",
    "    SIZE = 4\n",
    "    TILE_SIZE = 100\n",
    "    MARGIN = 10\n",
    "    WIDTH = SIZE * TILE_SIZE + (SIZE + 1) * MARGIN\n",
    "    HEIGHT = WIDTH + 100  # Extra space for text\n",
    "    FONT = pygame.font.SysFont(\"arial\", 32)\n",
    "    INFO_FONT = pygame.font.SysFont(\"arial\", 24)\n",
    "\n",
    "    # Colors\n",
    "    BG_COLOR = (187, 173, 160)\n",
    "    TILE_COLORS = {\n",
    "        0: (205, 193, 180), 2: (238, 228, 218), 4: (237, 224, 200),\n",
    "        8: (242, 177, 121), 16: (245, 149, 99), 32: (246, 124, 95),\n",
    "        64: (246, 94, 59), 128: (237, 207, 114), 256: (237, 204, 97),\n",
    "        512: (237, 200, 80), 1024: (237, 197, 63), 2048: (237, 194, 46),\n",
    "        4096: (237, 194, 46)\n",
    "    }\n",
    "    TEXT_COLOR_DARK = (119, 110, 101)\n",
    "    TEXT_COLOR_LIGHT = (249, 246, 242)\n",
    "\n",
    "    screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    pygame.display.set_caption(\"2048 AI Agent\")\n",
    "\n",
    "    def draw_game_state(env, move_count):\n",
    "        screen.fill(BG_COLOR)\n",
    "        \n",
    "        # Draw board\n",
    "        for row in range(SIZE):\n",
    "            for col in range(SIZE):\n",
    "                value = env.board[row][col]\n",
    "                color = TILE_COLORS.get(value, (60, 58, 50))\n",
    "                rect_x = MARGIN + col * (TILE_SIZE + MARGIN)\n",
    "                rect_y = MARGIN + row * (TILE_SIZE + MARGIN)\n",
    "                pygame.draw.rect(screen, color, (rect_x, rect_y, TILE_SIZE, TILE_SIZE))\n",
    "                \n",
    "                if value != 0:\n",
    "                    text_color = TEXT_COLOR_DARK if value <= 4 else TEXT_COLOR_LIGHT\n",
    "                    text = FONT.render(str(value), True, text_color)\n",
    "                    text_rect = text.get_rect(center=(rect_x + TILE_SIZE / 2, rect_y + TILE_SIZE / 2))\n",
    "                    screen.blit(text, text_rect)\n",
    "        \n",
    "        # Draw info\n",
    "        info_y = WIDTH + 10\n",
    "        info_text = INFO_FONT.render(f\"Move: {move_count} | Score: {env.score} | Max: {env.get_max_tile()}\", True, (0, 0, 0))\n",
    "        screen.blit(info_text, (10, info_y))\n",
    "        \n",
    "        pygame.display.update()\n",
    "\n",
    "    # Run game\n",
    "    env = Game2048Env()\n",
    "    state = env.reset()\n",
    "    move_count = 0\n",
    "    running = True\n",
    "\n",
    "    while running and not env.done:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            break\n",
    "\n",
    "        action = agent.get_action(env.board)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        move_count += 1\n",
    "        \n",
    "        draw_game_state(env, move_count)\n",
    "        time.sleep(delay)\n",
    "\n",
    "        if env.get_max_tile() >= 2048:\n",
    "            print(\" REACHED 2048! \")\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "    # Show final result\n",
    "    final_font = pygame.font.SysFont('arial', 40, bold=True)\n",
    "    if env.get_max_tile() >= 2048:\n",
    "        text = final_font.render(\"SUCCESS! Reached 2048!\", True, (0, 255, 0))\n",
    "    else:\n",
    "        text = final_font.render(f\"Game Over. Max: {env.get_max_tile()}\", True, (255, 0, 0))\n",
    "    \n",
    "    text_rect = text.get_rect(center=(WIDTH / 2, HEIGHT / 2))\n",
    "    screen.blit(text, text_rect)\n",
    "    pygame.display.update()\n",
    "    time.sleep(3)\n",
    "\n",
    "    pygame.quit()\n",
    "    return env.get_max_tile(), env.score, move_count\n",
    "\n",
    "# Run visualization\n",
    "print(\"Starting visual gameplay...\")\n",
    "max_tile, score, moves = visualize_agent_gameplay(super_agent, delay=0.1)\n",
    "print(f\"Final result: Max tile = {max_tile}, Score = {score}, Moves = {moves}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing MCTS Agent ===\n",
      "\n",
      "=== Run 1/5 ===\n",
      "Result: Max tile = 32, Score = 212, Moves = 43\n",
      "\n",
      "=== Run 2/5 ===\n",
      "Move 300: Max=512, Empty=1, Score=4464\n",
      "Result: Max tile = 512, Score = 5564, Moves = 385\n",
      "\n",
      "=== Run 3/5 ===\n",
      "Result: Max tile = 32, Score = 504, Moves = 76\n",
      "\n",
      "=== Run 4/5 ===\n",
      "Move 300: Max=512, Empty=2, Score=4452\n",
      "Result: Max tile = 512, Score = 4680, Moves = 319\n",
      "\n",
      "=== Run 5/5 ===\n",
      "Result: Max tile = 256, Score = 2372, Moves = 202\n",
      "\n",
      "=== DETAILED ANALYSIS OF 5 RUNS ===\n",
      "Success rate (2048): 0.0%\n",
      "Average max tile: 269 (min: 32, max: 512)\n",
      "Average score: 2666 (min: 212, max: 5564)\n",
      "Average moves: 205 (min: 43, max: 385)\n",
      "\n",
      "Tile distribution:\n",
      "  512: 2 times (40.0%)\n",
      "  256: 1 times (20.0%)\n",
      "  32: 2 times (40.0%)\n",
      "\n",
      "==================================================\n",
      "=== Testing Hybrid Agent ===\n",
      "\n",
      "=== Run 1/5 ===\n",
      "Move 300: Max=512, Empty=0, Score=4448\n",
      "Result: Max tile = 512, Score = 5084, Moves = 354\n",
      "\n",
      "=== Run 2/5 ===\n",
      "Move 300: Max=512, Empty=4, Score=4496\n",
      "Result: Max tile = 512, Score = 5284, Moves = 369\n",
      "\n",
      "=== Run 3/5 ===\n",
      "Move 300: Max=512, Empty=1, Score=4408\n",
      "Result: Max tile = 512, Score = 4544, Moves = 313\n",
      "\n",
      "=== Run 4/5 ===\n",
      "Move 300: Max=512, Empty=4, Score=4540\n",
      "Result: Max tile = 512, Score = 5304, Moves = 364\n",
      "\n",
      "=== Run 5/5 ===\n",
      "Move 300: Max=256, Empty=2, Score=4012\n",
      "Result: Max tile = 512, Score = 5332, Moves = 367\n",
      "\n",
      "=== DETAILED ANALYSIS OF 5 RUNS ===\n",
      "Success rate (2048): 0.0%\n",
      "Average max tile: 512 (min: 512, max: 512)\n",
      "Average score: 5110 (min: 4544, max: 5332)\n",
      "Average moves: 353 (min: 313, max: 369)\n",
      "\n",
      "Tile distribution:\n",
      "  512: 5 times (100.0%)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, board, parent=None, action=None, is_chance=False):\n",
    "        self.board = board.copy()\n",
    "        self.parent = parent\n",
    "        self.action = action  # Action that led to this node\n",
    "        self.is_chance = is_chance  # True if this is a chance node (random tile placement)\n",
    "        \n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value_sum = 0.0\n",
    "        self.untried_actions = None\n",
    "        \n",
    "        if not is_chance:\n",
    "            # Player node - actions are moves\n",
    "            temp_env = Game2048Env()\n",
    "            temp_env.board = board.copy()\n",
    "            self.untried_actions = temp_env.get_valid_actions()\n",
    "        else:\n",
    "            # Chance node - actions are tile placements\n",
    "            empty_positions = list(zip(*np.where(board == 0)))\n",
    "            self.untried_actions = [(pos, val) for pos in empty_positions for val in [2, 4]]\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.untried_actions) == 0\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        if self.is_chance:\n",
    "            return False\n",
    "        temp_env = Game2048Env()\n",
    "        temp_env.board = self.board.copy()\n",
    "        return temp_env.is_game_over()\n",
    "    \n",
    "    def get_average_value(self):\n",
    "        if self.visits == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visits\n",
    "\n",
    "class MCTS2048Agent:\n",
    "    def __init__(self, iterations=1000, exploration_weight=1.4):\n",
    "        self.iterations = iterations\n",
    "        self.exploration_weight = exploration_weight\n",
    "        self.temp_env = Game2048Env()\n",
    "        \n",
    "    def get_action(self, board):\n",
    "        \"\"\"Get the best action using MCTS\"\"\"\n",
    "        if np.count_nonzero(board == 0) <= 4:\n",
    "            # Endgame - use more iterations\n",
    "            iterations = min(2000, self.iterations * 2)\n",
    "        else:\n",
    "            iterations = self.iterations\n",
    "            \n",
    "        root = MCTSNode(board, is_chance=False)\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            # Selection and Expansion\n",
    "            leaf = self._select_and_expand(root)\n",
    "            \n",
    "            # Simulation\n",
    "            reward = self._simulate(leaf)\n",
    "            \n",
    "            # Backpropagation\n",
    "            self._backpropagate(leaf, reward)\n",
    "        \n",
    "        # Choose best action\n",
    "        if not root.children:\n",
    "            # Fallback to random valid action\n",
    "            temp_env = Game2048Env()\n",
    "            temp_env.board = board.copy()\n",
    "            valid_actions = temp_env.get_valid_actions()\n",
    "            return random.choice(valid_actions) if valid_actions else 0\n",
    "        \n",
    "        best_child = max(root.children, key=lambda c: c.visits)\n",
    "        return best_child.action\n",
    "    \n",
    "    def _select_and_expand(self, node):\n",
    "        \"\"\"Select and expand using UCB1\"\"\"\n",
    "        while not node.is_terminal():\n",
    "            if not node.is_fully_expanded():\n",
    "                return self._expand(node)\n",
    "            else:\n",
    "                node = self._select_child(node)\n",
    "        return node\n",
    "    \n",
    "    def _expand(self, node):\n",
    "        \"\"\"Expand the node by adding a new child\"\"\"\n",
    "        if not node.untried_actions:\n",
    "            return node\n",
    "            \n",
    "        if node.is_chance:\n",
    "            # Chance node - add tile placement\n",
    "            (row, col), value = node.untried_actions.pop(0)\n",
    "            new_board = node.board.copy()\n",
    "            new_board[row, col] = value\n",
    "            child = MCTSNode(new_board, parent=node, action=((row, col), value), is_chance=False)\n",
    "        else:\n",
    "            # Player node - add move\n",
    "            action = node.untried_actions.pop(0)\n",
    "            self.temp_env.board = node.board.copy()\n",
    "            new_board, _ = self.temp_env._execute_move(node.board, action)\n",
    "            child = MCTSNode(new_board, parent=node, action=action, is_chance=True)\n",
    "        \n",
    "        node.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def _select_child(self, node):\n",
    "        \"\"\"Select child using UCB1 formula\"\"\"\n",
    "        if node.is_chance:\n",
    "            # For chance nodes, select proportionally to probability\n",
    "            # Favor 2-tiles (90%) over 4-tiles (10%)\n",
    "            weights = []\n",
    "            for child in node.children:\n",
    "                if child.action[1] == 2:  # 2-tile\n",
    "                    weights.append(0.9)\n",
    "                else:  # 4-tile\n",
    "                    weights.append(0.1)\n",
    "            \n",
    "            # Weighted random selection\n",
    "            return random.choices(node.children, weights=weights)[0]\n",
    "        else:\n",
    "            # For player nodes, use UCB1\n",
    "            return max(node.children, key=lambda c: self._ucb1_value(c, node.visits))\n",
    "    \n",
    "    def _ucb1_value(self, child, parent_visits):\n",
    "        \"\"\"Calculate UCB1 value for child selection\"\"\"\n",
    "        if child.visits == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        exploitation = child.get_average_value()\n",
    "        exploration = self.exploration_weight * math.sqrt(math.log(parent_visits) / child.visits)\n",
    "        return exploitation + exploration\n",
    "    \n",
    "    def _simulate(self, node):\n",
    "        \"\"\"Simulate a random rollout from the node\"\"\"\n",
    "        current_board = node.board.copy()\n",
    "        self.temp_env.board = current_board.copy()\n",
    "        self.temp_env.score = 0\n",
    "        self.temp_env.done = False\n",
    "        \n",
    "        moves = 0\n",
    "        max_moves = 100  # Limit simulation length\n",
    "        \n",
    "        while not self.temp_env.done and moves < max_moves:\n",
    "            valid_actions = self.temp_env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            # Use intelligent random policy for simulation\n",
    "            action = self._intelligent_random_action(self.temp_env.board, valid_actions)\n",
    "            _, reward, done, _ = self.temp_env.step(action)\n",
    "            moves += 1\n",
    "        \n",
    "        # Return evaluation of final state\n",
    "        return self._evaluate_board(self.temp_env.board)\n",
    "    \n",
    "    def _intelligent_random_action(self, board, valid_actions):\n",
    "        \"\"\"Smart random policy that prefers corner-preserving moves\"\"\"\n",
    "        max_tile = np.max(board)\n",
    "        max_pos = np.unravel_index(np.argmax(board), board.shape)\n",
    "        \n",
    "        # Prioritize moves that keep max tile in corner\n",
    "        if max_pos in [(0,0), (0,3), (3,0), (3,3)]:\n",
    "            if max_pos == (0,0) and 2 in valid_actions:  # Prefer LEFT\n",
    "                return 2 if random.random() < 0.7 else random.choice(valid_actions)\n",
    "            elif max_pos == (0,3) and 3 in valid_actions:  # Prefer RIGHT\n",
    "                return 3 if random.random() < 0.7 else random.choice(valid_actions)\n",
    "            elif max_pos == (3,0) and 2 in valid_actions:  # Prefer LEFT\n",
    "                return 2 if random.random() < 0.7 else random.choice(valid_actions)\n",
    "            elif max_pos == (3,3) and 3 in valid_actions:  # Prefer RIGHT\n",
    "                return 3 if random.random() < 0.7 else random.choice(valid_actions)\n",
    "        \n",
    "        return random.choice(valid_actions)\n",
    "    \n",
    "    def _backpropagate(self, node, reward):\n",
    "        \"\"\"Backpropagate the reward up the tree\"\"\"\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            node.value_sum += reward\n",
    "            node = node.parent\n",
    "    \n",
    "    def _evaluate_board(self, board):\n",
    "        \"\"\"Enhanced evaluation function for MCTS\"\"\"\n",
    "        if len(self.temp_env.get_valid_actions()) == 0:\n",
    "            return -100000  # Game over\n",
    "        \n",
    "        max_tile = np.max(board)\n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        \n",
    "        # Massive bonus for reaching 2048!\n",
    "        if max_tile >= 2048:\n",
    "            return 1000000 + empty_tiles * 10000\n",
    "        \n",
    "        # Strong bonuses for high tiles\n",
    "        tile_bonus = max_tile * 2\n",
    "        \n",
    "        # Corner bonus\n",
    "        max_pos = np.unravel_index(np.argmax(board), board.shape)\n",
    "        corner_bonus = 0\n",
    "        if max_pos in [(0,0), (0,3), (3,0), (3,3)]:\n",
    "            corner_bonus = max_tile * 10\n",
    "            if max_pos == (0,0):  # Prefer top-left\n",
    "                corner_bonus *= 1.5\n",
    "        \n",
    "        # Empty space bonus\n",
    "        empty_bonus = empty_tiles * 100\n",
    "        \n",
    "        # Monotonicity bonus\n",
    "        monotonicity = self._calculate_monotonicity(board)\n",
    "        \n",
    "        # Merge potential\n",
    "        merge_potential = self._calculate_merge_potential(board)\n",
    "        \n",
    "        return tile_bonus + corner_bonus + empty_bonus + monotonicity * 50 + merge_potential * 20\n",
    "    \n",
    "    def _calculate_monotonicity(self, board):\n",
    "        \"\"\"Calculate monotonicity score\"\"\"\n",
    "        # Check for decreasing sequences from top-left\n",
    "        score = 0\n",
    "        \n",
    "        # Horizontal monotonicity\n",
    "        for row in range(4):\n",
    "            for col in range(3):\n",
    "                if board[row, col] != 0 and board[row, col + 1] != 0:\n",
    "                    if board[row, col] >= board[row, col + 1]:\n",
    "                        score += 1\n",
    "        \n",
    "        # Vertical monotonicity\n",
    "        for col in range(4):\n",
    "            for row in range(3):\n",
    "                if board[row, col] != 0 and board[row + 1, col] != 0:\n",
    "                    if board[row, col] >= board[row + 1, col]:\n",
    "                        score += 1\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _calculate_merge_potential(self, board):\n",
    "        \"\"\"Calculate merge potential\"\"\"\n",
    "        potential = 0\n",
    "        \n",
    "        # Horizontal merges\n",
    "        for row in range(4):\n",
    "            for col in range(3):\n",
    "                if board[row, col] != 0 and board[row, col] == board[row, col + 1]:\n",
    "                    potential += board[row, col]\n",
    "        \n",
    "        # Vertical merges\n",
    "        for row in range(3):\n",
    "            for col in range(4):\n",
    "                if board[row, col] != 0 and board[row, col] == board[row + 1, col]:\n",
    "                    potential += board[row, col]\n",
    "        \n",
    "        return potential\n",
    "\n",
    "# Hybrid agent that combines MCTS with Expectimax for different situations\n",
    "class HybridAgent:\n",
    "    def __init__(self):\n",
    "        self.mcts_agent = MCTS2048Agent(iterations=1500)\n",
    "        self.expectimax_agent = UltimateExpectimaxAgent(depth=5)\n",
    "        \n",
    "    def get_action(self, board):\n",
    "        \"\"\"Choose between MCTS and Expectimax based on game state\"\"\"\n",
    "        max_tile = np.max(board)\n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        \n",
    "        # Use MCTS for critical endgame situations\n",
    "        if max_tile >= 512 and empty_tiles <= 6:\n",
    "            return self.mcts_agent.get_action(board)\n",
    "        \n",
    "        # Use MCTS when we have 1024 tile (close to winning)\n",
    "        if max_tile >= 1024:\n",
    "            return self.mcts_agent.get_action(board)\n",
    "        \n",
    "        # Use Expectimax for early/mid game\n",
    "        return self.expectimax_agent.get_action(board)\n",
    "\n",
    "# Enhanced test function with detailed analysis\n",
    "def detailed_test_runs(agent_class, num_runs=5):\n",
    "    \"\"\"Run detailed tests with more analysis\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n=== Run {run + 1}/{num_runs} ===\")\n",
    "        \n",
    "        if agent_class == HybridAgent:\n",
    "            agent = agent_class()\n",
    "        else:\n",
    "            agent = agent_class(iterations=1500) if hasattr(agent_class(), 'iterations') else agent_class()\n",
    "        \n",
    "        env = Game2048Env()\n",
    "        state = env.reset()\n",
    "        moves = 0\n",
    "        move_history = []\n",
    "        \n",
    "        while not env.done and moves < 3000:  # Increased move limit\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            action = agent.get_action(env.board)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            move_history.append({\n",
    "                'move': moves,\n",
    "                'action': action,\n",
    "                'max_tile': env.get_max_tile(),\n",
    "                'empty_tiles': np.count_nonzero(env.board == 0),\n",
    "                'score': env.score\n",
    "            })\n",
    "            \n",
    "            moves += 1\n",
    "            \n",
    "            if moves % 300 == 0:\n",
    "                print(f\"Move {moves}: Max={env.get_max_tile()}, Empty={np.count_nonzero(env.board == 0)}, Score={env.score}\")\n",
    "                \n",
    "            if env.get_max_tile() >= 2048:\n",
    "                print(f\" SUCCESS! Reached 2048 in {moves} moves! \")\n",
    "                break\n",
    "        \n",
    "        result = {\n",
    "            'max_tile': env.get_max_tile(),\n",
    "            'score': env.score,\n",
    "            'moves': moves,\n",
    "            'reached_2048': env.get_max_tile() >= 2048,\n",
    "            'move_history': move_history\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Result: Max tile = {result['max_tile']}, Score = {result['score']}, Moves = {moves}\")\n",
    "    \n",
    "    # Detailed analysis\n",
    "    print(f\"\\n=== DETAILED ANALYSIS OF {num_runs} RUNS ===\")\n",
    "    max_tiles = [r['max_tile'] for r in results]\n",
    "    scores = [r['score'] for r in results]\n",
    "    moves_list = [r['moves'] for r in results]\n",
    "    success_rate = sum(1 for r in results if r['reached_2048']) / num_runs\n",
    "    \n",
    "    print(f\"Success rate (2048): {success_rate:.1%}\")\n",
    "    print(f\"Average max tile: {np.mean(max_tiles):.0f} (min: {min(max_tiles)}, max: {max(max_tiles)})\")\n",
    "    print(f\"Average score: {np.mean(scores):.0f} (min: {min(scores)}, max: {max(scores)})\")\n",
    "    print(f\"Average moves: {np.mean(moves_list):.0f} (min: {min(moves_list)}, max: {max(moves_list)})\")\n",
    "    \n",
    "    # Count how often each max tile was reached\n",
    "    tile_counts = {}\n",
    "    for tile in max_tiles:\n",
    "        tile_counts[tile] = tile_counts.get(tile, 0) + 1\n",
    "    \n",
    "    print(f\"\\nTile distribution:\")\n",
    "    for tile in sorted(tile_counts.keys(), reverse=True):\n",
    "        print(f\"  {tile}: {tile_counts[tile]} times ({tile_counts[tile]/num_runs:.1%})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different agents\n",
    "print(\"=== Testing MCTS Agent ===\")\n",
    "mcts_results = detailed_test_runs(MCTS2048Agent, num_runs=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== Testing Hybrid Agent ===\")\n",
    "hybrid_results = detailed_test_runs(HybridAgent, num_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Enhanced MCTS Agent ===\n",
      "\n",
      "=== Run 1/3 ===\n",
      "Result: Max tile = 128, Score = 1172\n",
      "\n",
      "=== Run 2/3 ===\n",
      "Result: Max tile = 64, Score = 500\n",
      "\n",
      "=== Run 3/3 ===\n",
      "Result: Max tile = 64, Score = 752\n",
      "\n",
      "=== ENHANCED MCTS SUMMARY ===\n",
      "Success rate (2048): 0.0%\n",
      "Average max tile: 85\n",
      "Best max tile: 128\n"
     ]
    }
   ],
   "source": [
    "class EnhancedMCTS2048Agent:\n",
    "    def __init__(self, iterations=3000, exploration_weight=1.4):\n",
    "        self.iterations = iterations\n",
    "        self.exploration_weight = exploration_weight\n",
    "        self.temp_env = Game2048Env()\n",
    "        \n",
    "        # Import sophisticated evaluation from UltimateExpectimaxAgent\n",
    "        self.expectimax_evaluator = UltimateExpectimaxAgent(depth=1)\n",
    "        \n",
    "    def get_action(self, board):\n",
    "        \"\"\"Enhanced MCTS with better search strategy\"\"\"\n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        max_tile = np.max(board)\n",
    "        \n",
    "        # Adaptive iterations based on game state\n",
    "        if max_tile >= 1024:\n",
    "            iterations = self.iterations * 2  # Critical endgame\n",
    "        elif empty_tiles <= 6:\n",
    "            iterations = int(self.iterations * 1.5)  # Late game\n",
    "        else:\n",
    "            iterations = self.iterations\n",
    "            \n",
    "        root = MCTSNode(board, is_chance=False)\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            leaf = self._select_and_expand(root)\n",
    "            reward = self._enhanced_simulate(leaf)\n",
    "            self._backpropagate(leaf, reward)\n",
    "        \n",
    "        if not root.children:\n",
    "            # Fallback to Expectimax action\n",
    "            return self.expectimax_evaluator.get_action(board)\n",
    "        \n",
    "        # Choose child with best average value (not just most visits)\n",
    "        best_child = max(root.children, key=lambda c: c.get_average_value() if c.visits > 10 else float('-inf'))\n",
    "        return best_child.action\n",
    "    \n",
    "    def _enhanced_simulate(self, node):\n",
    "        \"\"\"Much better simulation using strategic rollout policy\"\"\"\n",
    "        current_board = node.board.copy()\n",
    "        self.temp_env.board = current_board.copy()\n",
    "        self.temp_env.score = 0\n",
    "        self.temp_env.done = False\n",
    "        \n",
    "        moves = 0\n",
    "        max_moves = 50  # Shorter but higher quality rollouts\n",
    "        \n",
    "        while not self.temp_env.done and moves < max_moves:\n",
    "            valid_actions = self.temp_env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            # Use strategic policy instead of random\n",
    "            action = self._strategic_rollout_policy(self.temp_env.board, valid_actions)\n",
    "            _, reward, done, _ = self.temp_env.step(action)\n",
    "            moves += 1\n",
    "        \n",
    "        # Use sophisticated evaluation function\n",
    "        return self.expectimax_evaluator.evaluate(self.temp_env.board)\n",
    "    \n",
    "    def _strategic_rollout_policy(self, board, valid_actions):\n",
    "        \"\"\"Strategic rollout that uses 2048 domain knowledge\"\"\"\n",
    "        max_tile = np.max(board)\n",
    "        max_pos = np.unravel_index(np.argmax(board), board.shape)\n",
    "        empty_tiles = np.count_nonzero(board == 0)\n",
    "        \n",
    "        # Priority order based on game state\n",
    "        action_priorities = {}\n",
    "        \n",
    "        # Corner preservation strategy\n",
    "        if max_pos in [(0,0), (0,3), (3,0), (3,3)]:\n",
    "            if max_pos == (0,0):  # Top-left corner\n",
    "                action_priorities = {2: 0.4, 1: 0.3, 0: 0.2, 3: 0.1}  # Prefer LEFT, DOWN\n",
    "            elif max_pos == (0,3):  # Top-right corner  \n",
    "                action_priorities = {3: 0.4, 1: 0.3, 0: 0.2, 2: 0.1}  # Prefer RIGHT, DOWN\n",
    "            elif max_pos == (3,0):  # Bottom-left corner\n",
    "                action_priorities = {2: 0.4, 0: 0.3, 1: 0.2, 3: 0.1}  # Prefer LEFT, UP\n",
    "            elif max_pos == (3,3):  # Bottom-right corner\n",
    "                action_priorities = {3: 0.4, 0: 0.3, 1: 0.2, 2: 0.1}  # Prefer RIGHT, UP\n",
    "        else:\n",
    "            # Max tile not in corner - prioritize moving it there\n",
    "            action_priorities = {2: 0.4, 1: 0.3, 0: 0.2, 3: 0.1}  # Default: prefer LEFT/DOWN\n",
    "        \n",
    "        # Filter by valid actions and create weighted choice\n",
    "        valid_priorities = [(action, action_priorities.get(action, 0.1)) for action in valid_actions]\n",
    "        \n",
    "        if not valid_priorities:\n",
    "            return random.choice(valid_actions)\n",
    "            \n",
    "        actions, weights = zip(*valid_priorities)\n",
    "        \n",
    "        # Add some randomness but bias toward good moves\n",
    "        if random.random() < 0.8:  # 80% strategic, 20% random\n",
    "            return random.choices(actions, weights=weights)[0]\n",
    "        else:\n",
    "            return random.choice(valid_actions)\n",
    "\n",
    "    # Keep existing MCTS tree operations but with minor improvements\n",
    "    def _select_and_expand(self, node):\n",
    "        \"\"\"Enhanced selection with better exploration\"\"\"\n",
    "        path = [node]\n",
    "        \n",
    "        while not node.is_terminal():\n",
    "            if not node.is_fully_expanded():\n",
    "                return self._expand(node)\n",
    "            else:\n",
    "                node = self._enhanced_select_child(node)\n",
    "                path.append(node)\n",
    "        return node\n",
    "    \n",
    "    def _enhanced_select_child(self, node):\n",
    "        \"\"\"Better child selection with domain knowledge\"\"\"\n",
    "        if node.is_chance:\n",
    "            # For chance nodes, proportional to tile probability\n",
    "            weights = []\n",
    "            for child in node.children:\n",
    "                if child.action[1] == 2:  # 2-tile (90% probability)\n",
    "                    weights.append(0.9)\n",
    "                else:  # 4-tile (10% probability)\n",
    "                    weights.append(0.1)\n",
    "            return random.choices(node.children, weights=weights)[0]\n",
    "        else:\n",
    "            # Enhanced UCB1 with action preferences\n",
    "            def enhanced_ucb1(child):\n",
    "                if child.visits == 0:\n",
    "                    return float('inf')\n",
    "                \n",
    "                exploitation = child.get_average_value()\n",
    "                exploration = self.exploration_weight * math.sqrt(math.log(node.visits) / child.visits)\n",
    "                \n",
    "                # Add slight bias toward corner-preserving moves\n",
    "                action_bias = 0\n",
    "                if child.action in [2, 1]:  # LEFT, DOWN preferred\n",
    "                    action_bias = 100\n",
    "                elif child.action in [0]:  # UP\n",
    "                    action_bias = 50\n",
    "                # RIGHT gets no bonus (action 3)\n",
    "                \n",
    "                return exploitation + exploration + action_bias\n",
    "            \n",
    "            return max(node.children, key=enhanced_ucb1)\n",
    "    \n",
    "    def _expand(self, node):\n",
    "        \"\"\"Keep existing expand logic\"\"\"\n",
    "        if not node.untried_actions:\n",
    "            return node\n",
    "            \n",
    "        if node.is_chance:\n",
    "            (row, col), value = node.untried_actions.pop(0)\n",
    "            new_board = node.board.copy()\n",
    "            new_board[row, col] = value\n",
    "            child = MCTSNode(new_board, parent=node, action=((row, col), value), is_chance=False)\n",
    "        else:\n",
    "            action = node.untried_actions.pop(0)\n",
    "            new_board, _ = self.temp_env._execute_move(node.board, action)\n",
    "            child = MCTSNode(new_board, parent=node, action=action, is_chance=True)\n",
    "        \n",
    "        node.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def _backpropagate(self, node, reward):\n",
    "        \"\"\"Keep existing backpropagation\"\"\"\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            node.value_sum += reward\n",
    "            node = node.parent\n",
    "\n",
    "# Test the enhanced MCTS\n",
    "def test_enhanced_mcts():\n",
    "    print(\"=== Testing Enhanced MCTS Agent ===\")\n",
    "    agent = EnhancedMCTS2048Agent(iterations=2000)\n",
    "    \n",
    "    results = []\n",
    "    for run in range(3):\n",
    "        print(f\"\\n=== Run {run + 1}/3 ===\")\n",
    "        \n",
    "        env = Game2048Env()\n",
    "        state = env.reset()\n",
    "        moves = 0\n",
    "        \n",
    "        while not env.done and moves < 2000:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            action = agent.get_action(env.board)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            moves += 1\n",
    "            \n",
    "            if moves % 300 == 0:\n",
    "                print(f\"Move {moves}: Max={env.get_max_tile()}, Empty={np.count_nonzero(env.board == 0)}\")\n",
    "                \n",
    "            if env.get_max_tile() >= 2048:\n",
    "                print(f\" ENHANCED MCTS SUCCESS! Reached 2048 in {moves} moves! \")\n",
    "                break\n",
    "        \n",
    "        result = {\n",
    "            'max_tile': env.get_max_tile(),\n",
    "            'score': env.score, \n",
    "            'moves': moves,\n",
    "            'reached_2048': env.get_max_tile() >= 2048\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Result: Max tile = {result['max_tile']}, Score = {result['score']}\")\n",
    "    \n",
    "    # Summary\n",
    "    max_tiles = [r['max_tile'] for r in results]\n",
    "    success_rate = sum(1 for r in results if r['reached_2048']) / len(results)\n",
    "    \n",
    "    print(f\"\\n=== ENHANCED MCTS SUMMARY ===\")\n",
    "    print(f\"Success rate (2048): {success_rate:.1%}\")\n",
    "    print(f\"Average max tile: {np.mean(max_tiles):.0f}\")\n",
    "    print(f\"Best max tile: {max(max_tiles)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "enhanced_results = test_enhanced_mcts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic-SA5bcgBn-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
