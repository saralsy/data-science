
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'gan/keras-cycleGAN-tutorial';</script>
    <link rel="icon" href="../_static/icon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to Saralsyâ€™s Learning Projects
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../iris-data.html">Iris Species</a></li>
<li class="toctree-l1"><a class="reference internal" href="../boston-house.html">Boston House Prices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sonar-mines.html">Connectionist Bench (Sonar, Mines vs. Rocks)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../store-sales-time-series-forecasting/store-sales.html">Store Sales - Time Series Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../titanic-decision-tree/titanic-decision-tree.html">Titanic Survival Prediction using Decision Trees</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fgan/keras-cycleGAN-tutorial.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/gan/keras-cycleGAN-tutorial.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1><no title></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="simple visible nav section-nav flex-column">
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">ops</span>

<span class="n">autotune</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;KERAS_BACKEND&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;tensorflow&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">kagglehub</span>

<span class="c1"># Download latest version</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">kagglehub</span><span class="o">.</span><span class="n">dataset_download</span><span class="p">(</span><span class="s2">&quot;balraj98/horse2zebra-dataset&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Path to dataset files:&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/saraliu/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Warning: Looks like you&#39;re using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.12).
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Path to dataset files: /Users/saraliu/.cache/kagglehub/datasets/balraj98/horse2zebra-dataset/versions/1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the horse-zebra dataset from local directory</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">def</span> <span class="nf">load_local_dataset</span><span class="p">(</span><span class="n">data_dir</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">load_images_from_folder</span><span class="p">(</span><span class="n">folder_path</span><span class="p">):</span>
        <span class="n">file_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder_path</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span> 
                     <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">folder_path</span><span class="p">)]</span>
        <span class="n">images</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">file_paths</span><span class="p">:</span>
            <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">decode_jpeg</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">images</span>
    
    <span class="n">train_horse_images</span> <span class="o">=</span> <span class="n">load_images_from_folder</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;trainA&#39;</span><span class="p">))</span>
    <span class="n">train_zebra_images</span> <span class="o">=</span> <span class="n">load_images_from_folder</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;trainB&#39;</span><span class="p">))</span>
    <span class="n">test_horse_images</span> <span class="o">=</span> <span class="n">load_images_from_folder</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;testA&#39;</span><span class="p">))</span>
    <span class="n">test_zebra_images</span> <span class="o">=</span> <span class="n">load_images_from_folder</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;testB&#39;</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">train_horse_images</span><span class="p">,</span> <span class="n">train_zebra_images</span><span class="p">,</span> <span class="n">test_horse_images</span><span class="p">,</span> <span class="n">test_zebra_images</span>

<span class="c1"># Load the dataset</span>
<span class="n">train_horses_images</span><span class="p">,</span> <span class="n">train_zebras_images</span><span class="p">,</span> <span class="n">test_horses_images</span><span class="p">,</span> <span class="n">test_zebras_images</span> <span class="o">=</span> <span class="n">load_local_dataset</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">([</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">train_horses_images</span><span class="p">])</span>  <span class="c1"># Check shapes of loaded images</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3]), TensorShape([256, 256, 3])]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check if train_horses_images are normalized</span>
<span class="k">def</span> <span class="nf">check_normalization</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
        <span class="c1"># Convert the image tensor to a numpy array for easier manipulation</span>
        <span class="n">img_np</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="k">else</span> <span class="n">img</span>
        
        <span class="c1"># Print the min and max values</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="n">img_np</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="n">max_val</span> <span class="o">=</span> <span class="n">img_np</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Min: </span><span class="si">{</span><span class="n">min_val</span><span class="si">}</span><span class="s2">, Max: </span><span class="si">{</span><span class="n">max_val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Call the function to check normalization</span>
<span class="n">check_normalization</span><span class="p">(</span><span class="n">train_horses_images</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Min: 10, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 12, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 30, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 234
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 16, Max: 255
Min: 2, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 230
Min: 0, Max: 211
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 243
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 2, Max: 255
Min: 4, Max: 255
Min: 0, Max: 230
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 13, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 26, Max: 255
Min: 0, Max: 255
Min: 5, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 226
Min: 0, Max: 255
Min: 24, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 254
Min: 0, Max: 225
Min: 12, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 251
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 8, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 248
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 238
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 245
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 4, Max: 255
Min: 22, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 252
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 9, Max: 254
Min: 0, Max: 250
Min: 17, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 251
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 3, Max: 255
Min: 10, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 10, Max: 255
Min: 15, Max: 249
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 0, Max: 238
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 235
Min: 11, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 0, Max: 249
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 201
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 5, Max: 255
Min: 0, Max: 255
Min: 7, Max: 255
Min: 3, Max: 255
Min: 2, Max: 252
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 246
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 251
Min: 0, Max: 255
Min: 11, Max: 255
Min: 0, Max: 255
Min: 0, Max: 254
Min: 0, Max: 255
Min: 9, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 238
Min: 4, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 9, Max: 232
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 6, Max: 247
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 231
Min: 0, Max: 245
Min: 1, Max: 248
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 31, Max: 198
Min: 0, Max: 255
Min: 2, Max: 255
Min: 12, Max: 248
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 21, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 5, Max: 221
Min: 0, Max: 249
Min: 0, Max: 255
Min: 0, Max: 255
Min: 4, Max: 255
Min: 0, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 2, Max: 232
Min: 0, Max: 255
Min: 0, Max: 255
Min: 5, Max: 255
Min: 0, Max: 183
Min: 29, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 238
Min: 0, Max: 255
Min: 2, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 4, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 8, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 7, Max: 253
Min: 0, Max: 218
Min: 0, Max: 254
Min: 0, Max: 255
Min: 7, Max: 255
Min: 20, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 14, Max: 255
Min: 0, Max: 255
Min: 0, Max: 228
Min: 2, Max: 212
Min: 0, Max: 245
Min: 0, Max: 255
Min: 0, Max: 255
Min: 2, Max: 220
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 248
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 236
Min: 0, Max: 255
Min: 0, Max: 255
Min: 6, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 245
Min: 8, Max: 252
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 5, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 11, Max: 255
Min: 9, Max: 255
Min: 0, Max: 255
Min: 0, Max: 245
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 201
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 3, Max: 239
Min: 0, Max: 238
Min: 0, Max: 255
Min: 0, Max: 210
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 21, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 16, Max: 208
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 3, Max: 249
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 234
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 228
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 236
Min: 0, Max: 255
Min: 0, Max: 227
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 48, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 241
Min: 0, Max: 255
Min: 0, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 9, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 231
Min: 3, Max: 156
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 254
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 14, Max: 255
Min: 0, Max: 255
Min: 0, Max: 230
Min: 0, Max: 255
Min: 0, Max: 251
Min: 0, Max: 255
Min: 7, Max: 255
Min: 4, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 5, Max: 255
Min: 0, Max: 245
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 11, Max: 255
Min: 3, Max: 250
Min: 7, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 14, Max: 255
Min: 18, Max: 252
Min: 10, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 3, Max: 228
Min: 0, Max: 255
Min: 0, Max: 182
Min: 8, Max: 255
Min: 16, Max: 233
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 224
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 201
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 7, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 11, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 8, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 3, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 153
Min: 0, Max: 235
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 8, Max: 190
Min: 14, Max: 255
Min: 0, Max: 223
Min: 0, Max: 255
Min: 4, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 8, Max: 255
Min: 0, Max: 255
Min: 0, Max: 243
Min: 4, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 24, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 4, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 1, Max: 221
Min: 0, Max: 255
Min: 24, Max: 255
Min: 26, Max: 220
Min: 0, Max: 255
Min: 0, Max: 237
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 21, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 254
Min: 0, Max: 247
Min: 0, Max: 230
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 235
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 240
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 224
Min: 0, Max: 255
Min: 9, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 17, Max: 255
Min: 3, Max: 255
Min: 0, Max: 255
Min: 26, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 249
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 18, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 200
Min: 0, Max: 255
Min: 0, Max: 255
Min: 2, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 18, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 238
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 72, Max: 211
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 7, Max: 249
Min: 0, Max: 189
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 254
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 1, Max: 255
Min: 9, Max: 255
Min: 0, Max: 217
Min: 14, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 234
Min: 0, Max: 255
Min: 0, Max: 243
Min: 0, Max: 255
Min: 8, Max: 248
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 222
Min: 0, Max: 214
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 254
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 2, Max: 255
Min: 15, Max: 226
Min: 0, Max: 255
Min: 1, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 231
Min: 0, Max: 255
Min: 0, Max: 204
Min: 0, Max: 255
Min: 0, Max: 245
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 247
Min: 0, Max: 255
Min: 3, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 9, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 46, Max: 240
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 232
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 3, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 20, Max: 216
Min: 12, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 249
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 5, Max: 255
Min: 13, Max: 255
Min: 10, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 4, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 245
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 218
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 6, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 19, Max: 255
Min: 3, Max: 255
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 6, Max: 255
Min: 0, Max: 250
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 234
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 4, Max: 255
Min: 6, Max: 255
Min: 0, Max: 254
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 3, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 14, Max: 255
Min: 0, Max: 255
Min: 9, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 4, Max: 197
Min: 15, Max: 255
Min: 0, Max: 252
Min: 0, Max: 255
Min: 2, Max: 255
Min: 0, Max: 212
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 239
Min: 0, Max: 253
Min: 0, Max: 255
Min: 53, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 239
Min: 0, Max: 255
Min: 0, Max: 193
Min: 5, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 16, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 241
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 241
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 248
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 17, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 253
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 9, Max: 255
Min: 0, Max: 240
Min: 0, Max: 255
Min: 0, Max: 252
Min: 0, Max: 255
Min: 0, Max: 242
Min: 0, Max: 186
Min: 0, Max: 255
Min: 0, Max: 255
Min: 6, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 29, Max: 255
Min: 0, Max: 255
Min: 15, Max: 255
Min: 0, Max: 255
Min: 32, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 252
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 11, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 21, Max: 231
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 252
Min: 0, Max: 255
Min: 9, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 1, Max: 255
Min: 7, Max: 255
Min: 4, Max: 255
Min: 0, Max: 251
Min: 6, Max: 251
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 4, Max: 202
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 254
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 230
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 226
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 255
Min: 0, Max: 197
Min: 21, Max: 255
Min: 8, Max: 255
Min: 0, Max: 255
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the standard image size.</span>
<span class="n">orig_img_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">286</span><span class="p">,</span> <span class="mi">286</span><span class="p">)</span>
<span class="c1"># Size of the random crops to be used during training.</span>
<span class="n">input_img_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># Weights initializer for the layers.</span>
<span class="n">kernel_init</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
<span class="n">gamma_init</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>

<span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>


<span class="k">def</span> <span class="nf">normalize_img</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    
    <span class="c1"># Optional debugging - wrap in tf.debugging.Assert for validation</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_less_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="mf">255.0</span><span class="p">)</span>
    
    <span class="c1"># Map values to range [-1, 1]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="p">(</span><span class="n">img</span> <span class="o">/</span> <span class="mf">127.5</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span>
    
    <span class="c1"># Validate output range</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_less_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>


<span class="k">def</span> <span class="nf">preprocess_train_image</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="c1"># Random flip</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_flip_left_right</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="c1"># Resize to the original size first</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">[</span><span class="o">*</span><span class="n">orig_img_size</span><span class="p">])</span>
    <span class="c1"># Random crop to 256X256</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="o">*</span><span class="n">input_img_size</span><span class="p">])</span>
    <span class="c1"># Normalize the pixel values in the range [-1, 1]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">normalize_img</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>


<span class="k">def</span> <span class="nf">preprocess_test_image</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="c1"># Only resizing and normalization for the test images.</span>
    <span class="c1"># Ensure input is tensor</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="c1"># Resize</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">[</span><span class="n">input_img_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
    <span class="c1"># Normalize</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">normalize_img</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>

<span class="k">def</span> <span class="nf">prepare_dataset</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">image_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_train</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">preprocess_train_image</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">autotune</span><span class="p">)</span>
        <span class="o">.</span><span class="n">cache</span><span class="p">()</span>
        <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>
        <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">preprocess_test_image</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">autotune</span><span class="p">)</span>
        <span class="o">.</span><span class="n">cache</span><span class="p">()</span>
        <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>
        <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_horses_dataset</span> <span class="o">=</span> <span class="n">prepare_dataset</span><span class="p">(</span><span class="n">train_horses_images</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_zebras_dataset</span> <span class="o">=</span> <span class="n">prepare_dataset</span><span class="p">(</span><span class="n">train_zebras_images</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_horses_dataset</span> <span class="o">=</span> <span class="n">prepare_dataset</span><span class="p">(</span><span class="n">test_horses_images</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">test_zebras_dataset</span> <span class="o">=</span> <span class="n">prepare_dataset</span><span class="p">(</span><span class="n">test_zebras_images</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Call the function to check normalization</span>
<span class="n">check_normalization</span><span class="p">(</span><span class="n">train_horses_dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Min: -0.8937439322471619, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.7539961934089661, Max: 1.0
Min: -0.9586395025253296, Max: 1.0
Min: -0.7925728559494019, Max: 0.9314409494400024
Min: -1.0, Max: 0.9968537092208862
Min: -1.0, Max: 1.0
Min: -0.9166710376739502, Max: 1.0
Min: -0.8922146558761597, Max: 1.0
Min: -0.9991872310638428, Max: 0.9843801259994507
Min: -1.0, Max: 0.9979716539382935
Min: -1.0, Max: 0.9961224794387817
Min: -1.0, Max: 1.0
Min: -0.9746060371398926, Max: 0.9140743017196655
Min: -1.0, Max: 1.0
Min: -0.9551543593406677, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.987484872341156, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9760863780975342, Max: 0.9997849464416504
Min: -1.0, Max: 1.0
Min: -0.7440911531448364, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.946501612663269, Max: 0.9991140365600586
Min: -1.0, Max: 1.0
Min: -0.8961306214332581, Max: 1.0
Min: -0.997629702091217, Max: 1.0
Min: -0.9989526271820068, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9837653636932373, Max: 0.9079939126968384
Min: -1.0, Max: 0.961942195892334
Min: -0.8193657398223877, Max: 1.0
Min: -1.0, Max: 0.8759838342666626
Min: -1.0, Max: 0.9607367515563965
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9996825456619263
Min: -0.9998466968536377, Max: 1.0
Min: -0.9010282754898071, Max: 1.0
Min: -1.0, Max: 0.9353280067443848
Min: -1.0, Max: 1.0
Min: -0.9953874349594116, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9912900328636169, Max: 0.9997473955154419
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9974396228790283
Min: -0.9915024638175964, Max: 1.0
Min: -1.0, Max: 0.9999537467956543
Min: -0.965190589427948, Max: 1.0
Min: -0.9558972716331482, Max: 0.9758402109146118
Min: -0.9982432126998901, Max: 0.9998040199279785
Min: -0.9470045566558838, Max: 1.0
Min: -0.9998123049736023, Max: 0.36961138248443604
Min: -1.0, Max: 1.0
Min: -0.9294232130050659, Max: 1.0
Min: -0.988223671913147, Max: 0.6720885038375854
Min: -0.9969799518585205, Max: 1.0
Min: -0.8874529600143433, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9876647591590881, Max: 1.0
Min: -0.9873236417770386, Max: 0.7899199724197388
Min: -1.0, Max: 1.0
Min: -0.980204164981842, Max: 0.9898099899291992
Min: -1.0, Max: 1.0
Min: -0.9992932081222534, Max: 1.0
Min: -1.0, Max: 0.7514054775238037
Min: -0.9546859860420227, Max: 1.0
Min: -0.9433918595314026, Max: 1.0
Min: -1.0, Max: 0.9997756481170654
Min: -0.9711945652961731, Max: 0.8041949272155762
Min: -0.9926795959472656, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.8042380809783936, Max: 1.0
Min: -0.9204587340354919, Max: 1.0
Min: -1.0, Max: 0.9902139902114868
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9997196197509766, Max: 0.7745636701583862
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9711316823959351
Min: -0.9991405010223389, Max: 1.0
Min: -0.9852462410926819, Max: 1.0
Min: -0.8138717412948608, Max: 1.0
Min: -0.9219812154769897, Max: 0.9663512706756592
Min: -0.9979207515716553, Max: 0.9965442419052124
Min: -1.0, Max: 0.9994878768920898
Min: -1.0, Max: 1.0
Min: -0.999050498008728, Max: 1.0
Min: -0.897712230682373, Max: 0.7913998365402222
Min: -0.960039496421814, Max: 0.9892746210098267
Min: -0.9748364090919495, Max: 0.9746016263961792
Min: -0.9999445676803589, Max: 0.9880433082580566
Min: -1.0, Max: 1.0
Min: -0.7960882186889648, Max: 0.9667904376983643
Min: -0.9985321164131165, Max: 1.0
Min: -0.9998651742935181, Max: 0.9838292598724365
Min: -1.0, Max: 0.888433575630188
Min: -0.9540898203849792, Max: 1.0
Min: -0.9758120775222778, Max: 0.8933805227279663
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.998451828956604
Min: -1.0, Max: 0.999090313911438
Min: -1.0, Max: 1.0
Min: -0.9873301386833191, Max: 0.9993469715118408
Min: -1.0, Max: 1.0
Min: -0.9782218933105469, Max: 0.7622475624084473
Min: -0.912505030632019, Max: 1.0
Min: -1.0, Max: 0.9718121290206909
Min: -0.9964662194252014, Max: 0.9666942358016968
Min: -1.0, Max: 1.0
Min: -0.9994423389434814, Max: 0.9927620887756348
Min: -1.0, Max: 1.0
Min: -0.9504783153533936, Max: 0.9744737148284912
Min: -0.998521625995636, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.970527708530426, Max: 1.0
Min: -0.9998519420623779, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.91591876745224, Max: 1.0
Min: -0.9400767683982849, Max: 0.9986717700958252
Min: -0.9976068735122681, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9947360754013062, Max: 1.0
Min: -1.0, Max: 0.8067804574966431
Min: -1.0, Max: 1.0
Min: -0.9251103401184082, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9856012463569641, Max: 0.9753891229629517
Min: -0.9116724133491516, Max: 1.0
Min: -1.0, Max: 0.9998191595077515
Min: -0.8432151675224304, Max: 1.0
Min: -0.999808132648468, Max: 0.933341383934021
Min: -1.0, Max: 0.9991062879562378
Min: -0.9928268194198608, Max: 0.9997875690460205
Min: -1.0, Max: 0.9004784822463989
Min: -0.9639919400215149, Max: 0.9348738193511963
Min: -1.0, Max: 0.9965713024139404
Min: -0.9994872808456421, Max: 1.0
Min: -0.9710068702697754, Max: 0.9959626197814941
Min: -1.0, Max: 1.0
Min: -0.9909615516662598, Max: 0.9423865079879761
Min: -0.7082560062408447, Max: 0.9971472024917603
Min: -0.9999921321868896, Max: 1.0
Min: -1.0, Max: 0.8796482086181641
Min: -1.0, Max: 1.0
Min: -0.9797526001930237, Max: 1.0
Min: -1.0, Max: 0.9993702173233032
Min: -1.0, Max: 1.0
Min: -0.9942722320556641, Max: 1.0
Min: -0.9826328158378601, Max: 1.0
Min: -0.9917177557945251, Max: 0.6258808374404907
Min: -1.0, Max: 1.0
Min: -0.8034205436706543, Max: 0.5888439416885376
Min: -0.9603761434555054, Max: 1.0
Min: -0.9697793126106262, Max: 0.5792628526687622
Min: -1.0, Max: 1.0
Min: -0.987625241279602, Max: 0.7987498044967651
Min: -0.9981441497802734, Max: 1.0
Min: -0.9251943230628967, Max: 0.9142614603042603
Min: -0.7406770586967468, Max: 0.9683839082717896
Min: -1.0, Max: 0.9998679161071777
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9521472454071045
Min: -0.8921703696250916, Max: 1.0
Min: -0.9997027516365051, Max: 1.0
Min: -0.9916151165962219, Max: 1.0
Min: -0.9935145974159241, Max: 0.8868144750595093
Min: -1.0, Max: 0.9988349676132202
Min: -1.0, Max: 1.0
Min: -0.9855211973190308, Max: 0.9889489412307739
Min: -0.967379629611969, Max: 0.9566904306411743
Min: -0.7869930863380432, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9962246417999268, Max: 1.0
Min: -1.0, Max: 0.9921481609344482
Min: -0.9099173545837402, Max: 0.7254902124404907
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9997626543045044
Min: -1.0, Max: 1.0
Min: -0.999989926815033, Max: 1.0
Min: -0.9515050649642944, Max: 1.0
Min: -0.9987356662750244, Max: 1.0
Min: -1.0, Max: 0.9762678146362305
Min: -0.9905149936676025, Max: 0.9060769081115723
Min: -1.0, Max: 0.9995172023773193
Min: -1.0, Max: 0.9162211418151855
Min: -0.9366570711135864, Max: 1.0
Min: -1.0, Max: 0.9893261194229126
Min: -0.9997563362121582, Max: 1.0
Min: -1.0, Max: 0.9996875524520874
Min: -1.0, Max: 0.9959158897399902
Min: -1.0, Max: 1.0
Min: -0.9885237812995911, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9977988600730896, Max: 0.7291609048843384
Min: -0.9996254444122314, Max: 0.7815713882446289
Min: -0.9987475275993347, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9920690059661865
Min: -1.0, Max: 0.9993488788604736
Min: -0.9555417895317078, Max: 1.0
Min: -0.8789380192756653, Max: 0.9733991622924805
Min: -0.9892178773880005, Max: 1.0
Min: -0.882233738899231, Max: 0.9999364614486694
Min: -0.9389424324035645, Max: 0.9983549118041992
Min: -0.981544017791748, Max: 0.9786036014556885
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9752656817436218, Max: 0.8029814958572388
Min: -0.9817015528678894, Max: 1.0
Min: -0.998394787311554, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.843137264251709
Min: -0.9686599373817444, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.999962329864502
Min: -1.0, Max: 0.9984414577484131
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9961881637573242
Min: -0.997154951095581, Max: 0.9888933897018433
Min: -0.9999068379402161, Max: 1.0
Min: -1.0, Max: 0.9989994764328003
Min: -0.9951391816139221, Max: 0.999698281288147
Min: -1.0, Max: 0.4861462116241455
Min: -0.9993581771850586, Max: 1.0
Min: -1.0, Max: 0.9462906122207642
Min: -1.0, Max: 1.0
Min: -0.9918099045753479, Max: 0.9997438192367554
Min: -1.0, Max: 0.9997885227203369
Min: -0.9669446349143982, Max: 0.8254470825195312
Min: -0.8890174627304077, Max: 1.0
Min: -0.9848999381065369, Max: 0.6083540916442871
Min: -1.0, Max: 1.0
Min: -0.9948720335960388, Max: 1.0
Min: -0.8169432878494263, Max: 0.9913320541381836
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9931764602661133
Min: -0.9507561922073364, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9985886216163635, Max: 0.9633240699768066
Min: -0.9792454838752747, Max: 0.9658368825912476
Min: -1.0, Max: 0.8599379062652588
Min: -1.0, Max: 0.9947675466537476
Min: -0.979352593421936, Max: 0.8785572052001953
Min: -1.0, Max: 0.7579711675643921
Min: -0.762785792350769, Max: 1.0
Min: -0.9870651960372925, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.8507299423217773, Max: 0.9773455858230591
Min: -0.9878508448600769, Max: 1.0
Min: -0.9996365308761597, Max: 1.0
Min: -0.9827864766120911, Max: 0.9979395866394043
Min: -0.9097855091094971, Max: 0.9016928672790527
Min: -1.0, Max: 0.826999306678772
Min: -0.8932191729545593, Max: 1.0
Min: -0.9725901484489441, Max: 0.949993371963501
Min: -0.9967639446258545, Max: 0.7575925588607788
Min: -0.8644949197769165, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.6907291412353516, Max: 0.5017255544662476
Min: -0.8875579833984375, Max: 0.9534009695053101
Min: -0.880479633808136, Max: 0.969180703163147
Min: -1.0, Max: 0.9818702936172485
Min: -1.0, Max: 1.0
Min: -0.8954231142997742, Max: 0.9928004741668701
Min: -1.0, Max: 0.9992860555648804
Min: -0.9999935626983643, Max: 0.9995224475860596
Min: -1.0, Max: 1.0
Min: -0.9009992480278015, Max: 0.9502378702163696
Min: -0.9800018668174744, Max: 0.9840109348297119
Min: -0.9234199523925781, Max: 0.9986495971679688
Min: -0.9555203914642334, Max: 0.7882353067398071
Min: -0.9990339279174805, Max: 1.0
Min: -1.0, Max: 0.9996951818466187
Min: -1.0, Max: 1.0
Min: -0.9353367686271667, Max: 0.9994536638259888
Min: -0.9555186629295349, Max: 0.9799318313598633
Min: -0.7775015234947205, Max: 0.9716691970825195
Min: -1.0, Max: 1.0
Min: -0.979364812374115, Max: 0.9998903274536133
Min: -0.999575674533844, Max: 1.0
Min: -0.9921966195106506, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9979838728904724, Max: 0.9328281879425049
Min: -0.996055006980896, Max: 1.0
Min: -0.9152674674987793, Max: 0.9359546899795532
Min: -0.8459562063217163, Max: 1.0
Min: -0.996838390827179, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9994652271270752
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9804995059967041
Min: -1.0, Max: 0.1754910945892334
Min: -1.0, Max: 0.9822179079055786
Min: -1.0, Max: 1.0
Min: -0.9733077883720398, Max: 1.0
Min: -1.0, Max: 0.986784815788269
Min: -0.9799112677574158, Max: 0.9177088737487793
Min: -1.0, Max: 0.9741142988204956
Min: -1.0, Max: 0.9982153177261353
Min: -1.0, Max: 1.0
Min: -0.9759986996650696, Max: 0.9032237529754639
Min: -0.9753681421279907, Max: 0.5290431976318359
Min: -1.0, Max: 1.0
Min: -0.8727548122406006, Max: 0.7707955837249756
Min: -0.8970740437507629, Max: 0.9944322109222412
Min: -1.0, Max: 0.9367512464523315
Min: -0.9553412199020386, Max: 0.9960938692092896
Min: -0.7244399785995483, Max: 1.0
Min: -0.9999637603759766, Max: 0.9950087070465088
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9997807741165161
Min: -0.9171494245529175, Max: 0.9936913251876831
Min: -0.9904863238334656, Max: 0.6925899982452393
Min: -0.9889600872993469, Max: 0.9924478530883789
Min: -1.0, Max: 1.0
Min: -0.9805253148078918, Max: 0.9596699476242065
Min: -0.9574228525161743, Max: 0.8887882232666016
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9793540239334106, Max: 0.7855942249298096
Min: -1.0, Max: 0.961072564125061
Min: -0.9638933539390564, Max: 1.0
Min: -0.9886684417724609, Max: 1.0
Min: -0.9787342548370361, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9907964468002319
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9110305309295654
Min: -1.0, Max: 0.9871878623962402
Min: -0.9993463754653931, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9961353540420532, Max: 0.9974786043167114
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.8516485691070557, Max: 1.0
Min: -1.0, Max: 0.9059635400772095
Min: -0.9798653721809387, Max: 1.0
Min: -0.8927426338195801, Max: 0.9981987476348877
Min: -0.9963462948799133, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9198774099349976, Max: 0.9297432899475098
Min: -1.0, Max: 0.9998326301574707
Min: -0.7817694544792175, Max: 1.0
Min: -0.9995809197425842, Max: 0.9710178375244141
Min: -1.0, Max: 0.997870922088623
Min: -0.9993393421173096, Max: 0.932091474533081
Min: -1.0, Max: 1.0
Min: -0.9860207438468933, Max: 1.0
Min: -0.9778668284416199, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.7616873383522034, Max: 1.0
Min: -0.992281436920166, Max: 0.98833167552948
Min: -0.9975430965423584, Max: 0.9864183664321899
Min: -1.0, Max: 0.9996980428695679
Min: -0.9997297525405884, Max: 0.9987702369689941
Min: -0.7849773168563843, Max: 1.0
Min: -0.996462345123291, Max: 1.0
Min: -1.0, Max: 0.962110161781311
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9024269580841064, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9133570194244385, Max: 0.9988340139389038
Min: -0.9291449785232544, Max: 0.48547160625457764
Min: -0.9999527335166931, Max: 0.9989959001541138
Min: -0.8809155821800232, Max: 1.0
Min: -0.9890836477279663, Max: 1.0
Min: -0.9999949932098389, Max: 0.5411080121994019
Min: -1.0, Max: 0.8608802556991577
Min: -1.0, Max: 0.9874215126037598
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9882004261016846
Min: -1.0, Max: 1.0
Min: -0.9919402003288269, Max: 0.9299838542938232
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9999180436134338, Max: 1.0
Min: -0.9306700229644775, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9770110249519348, Max: 1.0
Min: -1.0, Max: 0.9890193939208984
Min: -1.0, Max: 0.9711688756942749
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.982028603553772
Min: -1.0, Max: 1.0
Min: -0.8151082992553711, Max: 0.9236564636230469
Min: -0.9730581641197205, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9579369425773621, Max: 1.0
Min: -1.0, Max: 0.7788089513778687
Min: -1.0, Max: 0.7875552177429199
Min: -0.9450883269309998, Max: 0.9938822984695435
Min: -0.9985468983650208, Max: 0.9570088386535645
Min: -1.0, Max: 0.9789674282073975
Min: -0.9971494078636169, Max: 0.8650102615356445
Min: -0.9954593181610107, Max: 0.9967470169067383
Min: -0.9442899227142334, Max: 0.9976440668106079
Min: -0.9962800741195679, Max: 1.0
Min: -1.0, Max: 0.9945542812347412
Min: -1.0, Max: 0.7703381776809692
Min: -1.0, Max: 0.9998964071273804
Min: -1.0, Max: 0.9414904117584229
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9911925196647644, Max: 0.9558204412460327
Min: -1.0, Max: 1.0
Min: -0.7994641661643982, Max: 1.0
Min: -1.0, Max: 0.9815927743911743
Min: -0.8414232134819031, Max: 1.0
Min: -0.9995238184928894, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9951321482658386, Max: 0.9658312797546387
Min: -0.9988160133361816, Max: 1.0
Min: -0.9908337593078613, Max: 1.0
Min: -0.9884458184242249, Max: 0.9214330911636353
Min: -0.9932717084884644, Max: 0.9990990161895752
Min: -0.9858074188232422, Max: 1.0
Min: -1.0, Max: 0.9989652633666992
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.8422131538391113
Min: -1.0, Max: 0.9983416795730591
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9803268909454346
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9520108699798584
Min: -0.98894202709198, Max: 0.9645832777023315
Min: -1.0, Max: 0.9997228384017944
Min: -1.0, Max: 0.998481273651123
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.917495846748352, Max: 1.0
Min: -0.9907971620559692, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9631785154342651
Min: -0.9974018335342407, Max: 0.9992328882217407
Min: -0.7691890597343445, Max: 0.9991773366928101
Min: -0.9943076372146606, Max: 0.9695791006088257
Min: -0.9964715242385864, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9666435122489929, Max: 0.9992078542709351
Min: -0.9668641090393066, Max: 0.8741247653961182
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.6702660322189331
Min: -0.9422420859336853, Max: 1.0
Min: -0.9163921475410461, Max: 0.9924641847610474
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9984217882156372
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.8422330617904663
Min: -0.8879302740097046, Max: 0.9838658571243286
Min: -1.0, Max: 0.9869928359985352
Min: -0.8894911408424377, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9995236992835999, Max: 1.0
Min: -0.9772787094116211, Max: 0.728484034538269
Min: -1.0, Max: 0.6508440971374512
Min: -1.0, Max: 1.0
Min: -0.8492625951766968, Max: 0.9676938056945801
Min: -0.8915001153945923, Max: 0.8622161149978638
Min: -0.9892718195915222, Max: 0.38823533058166504
Min: -1.0, Max: 0.9904448986053467
Min: -1.0, Max: 1.0
Min: -0.9998517632484436, Max: 1.0
Min: -0.9879692196846008, Max: 1.0
Min: -1.0, Max: 0.998313307762146
Min: -1.0, Max: 1.0
Min: -0.9183706045150757, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.999627411365509, Max: 0.9789841175079346
Min: -1.0, Max: 1.0
Min: -0.9831545948982239, Max: 0.9676711559295654
Min: -0.982426643371582, Max: 0.8767498731613159
Min: -0.9940705299377441, Max: 0.9999176263809204
Min: -0.9488764405250549, Max: 0.845535159111023
Min: -0.9967195391654968, Max: 0.9950644969940186
Min: -1.0, Max: 1.0
Min: -0.9914575219154358, Max: 0.808390736579895
Min: -0.8482792973518372, Max: 0.7148739099502563
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9995664954185486, Max: 1.0
Min: -0.9896528720855713, Max: 0.9931559562683105
Min: -0.9782662391662598, Max: 0.9555048942565918
Min: -0.9439970850944519, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9996852278709412, Max: 0.8352764844894409
Min: -0.8312617540359497, Max: 1.0
Min: -0.9999138712882996, Max: 1.0
Min: -0.9637279510498047, Max: 0.9980292320251465
Min: -0.9126095771789551, Max: 1.0
Min: -0.9934593439102173, Max: 0.9798755645751953
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.5963723659515381
Min: -1.0, Max: 0.9989298582077026
Min: -0.8588705062866211, Max: 0.9348866939544678
Min: -0.8324900269508362, Max: 0.9790304899215698
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.7144049406051636
Min: -1.0, Max: 1.0
Min: -0.9292811751365662, Max: 1.0
Min: -0.8849983811378479, Max: 1.0
Min: -0.9577232599258423, Max: 0.9976880550384521
Min: -1.0, Max: 0.9973722696304321
Min: -1.0, Max: 1.0
Min: -0.9958968162536621, Max: 0.808613657951355
Min: -1.0, Max: 0.9615514278411865
Min: -0.9700456261634827, Max: 0.9989763498306274
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.924872636795044
Min: -1.0, Max: 0.9512674808502197
Min: -0.9870812296867371, Max: 1.0
Min: -0.9906885623931885, Max: 1.0
Min: -1.0, Max: 0.9988319873809814
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9594722986221313
Min: -0.9848265051841736, Max: 1.0
Min: -1.0, Max: 0.9932118654251099
Min: -0.9870969653129578, Max: 1.0
Min: -0.9868634939193726, Max: 1.0
Min: -0.8295990824699402, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.8698195219039917, Max: 1.0
Min: -1.0, Max: 0.9406050443649292
Min: -0.9173295497894287, Max: 0.7003978490829468
Min: -1.0, Max: 1.0
Min: -0.995973527431488, Max: 0.9818000793457031
Min: -0.9760875701904297, Max: 0.973283052444458
Min: -0.9633718729019165, Max: 0.9841500520706177
Min: -1.0, Max: 1.0
Min: -0.9983371496200562, Max: 1.0
Min: -1.0, Max: 0.9121413230895996
Min: -1.0, Max: 0.8863844871520996
Min: -1.0, Max: 1.0
Min: -0.9956364631652832, Max: 0.9978762865066528
Min: -1.0, Max: 0.9510595798492432
Min: -0.968450665473938, Max: 0.8512197732925415
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9967241287231445
Min: -1.0, Max: 0.8415650129318237
Min: -0.931329607963562, Max: 0.9997210502624512
Min: -0.9931403994560242, Max: 0.9747190475463867
Min: -1.0, Max: 0.9848117828369141
Min: -1.0, Max: 1.0
Min: -0.9989518523216248, Max: 0.9984041452407837
Min: -0.9945020079612732, Max: 0.9162884950637817
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.8274509906768799
Min: -0.550319492816925, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.8626670837402344, Max: 1.0
Min: -0.9998548030853271, Max: 0.8354287147521973
Min: -1.0, Max: 0.9999973773956299
Min: -1.0, Max: 0.9570541381835938
Min: -1.0, Max: 0.9991556406021118
Min: -1.0, Max: 1.0
Min: -0.9887728095054626, Max: 0.8734657764434814
Min: -1.0, Max: 0.8614981174468994
Min: -1.0, Max: 0.9703296422958374
Min: -1.0, Max: 1.0
Min: -0.9876652359962463, Max: 0.9968438148498535
Min: -0.9997204542160034, Max: 0.5581518411636353
Min: -1.0, Max: 0.9991099834442139
Min: -0.7790607213973999, Max: 0.6927615404129028
Min: -1.0, Max: 1.0
Min: -0.9635813236236572, Max: 0.7720828056335449
Min: -0.9681612849235535, Max: 0.9999829530715942
Min: -0.9493104219436646, Max: 0.9787240028381348
Min: -0.9958245158195496, Max: 0.8227963447570801
Min: -1.0, Max: 1.0
Min: -0.9544003009796143, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.991073489189148
Min: -0.9973928332328796, Max: 1.0
Min: -0.9331228733062744, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9706485867500305, Max: 1.0
Min: -0.9999626278877258, Max: 0.9197862148284912
Min: -1.0, Max: 0.9195131063461304
Min: -1.0, Max: 1.0
Min: -0.9990032911300659, Max: 1.0
Min: -0.8409303426742554, Max: 1.0
Min: -0.999808132648468, Max: 1.0
Min: -1.0, Max: 0.9799802303314209
Min: -0.8054007291793823, Max: 1.0
Min: -0.7979767322540283, Max: 0.9937400817871094
Min: -0.9687158465385437, Max: 1.0
Min: -0.9963415265083313, Max: 1.0
Min: -0.9660584330558777, Max: 1.0
Min: -0.9999195337295532, Max: 0.9930033683776855
Min: -0.9703629016876221, Max: 1.0
Min: -0.9968345165252686, Max: 0.9757974147796631
Min: -0.9934734106063843, Max: 0.9002455472946167
Min: -0.9987642168998718, Max: 1.0
Min: -0.9731264710426331, Max: 1.0
Min: -0.9991844892501831, Max: 0.9854856729507446
Min: -1.0, Max: 0.9697123765945435
Min: -0.9608204960823059, Max: 1.0
Min: -0.9801173806190491, Max: 0.9998332262039185
Min: -0.9946409463882446, Max: 0.9987459182739258
Min: -0.9861178994178772, Max: 0.9923515319824219
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.999568521976471, Max: 0.9854110479354858
Min: -1.0, Max: 0.9943884611129761
Min: -1.0, Max: 1.0
Min: -0.9663986563682556, Max: 0.903884768486023
Min: -0.8677470684051514, Max: 0.8216136693954468
Min: -0.9880565404891968, Max: 0.9837774038314819
Min: -0.9948514103889465, Max: 0.8991541862487793
Min: -1.0, Max: 1.0
Min: -0.9163587093353271, Max: 0.7311522960662842
Min: -0.9899718165397644, Max: 1.0
Min: -0.9097424149513245, Max: 0.9200332164764404
Min: -0.9969660043716431, Max: 0.9538155794143677
Min: -1.0, Max: 1.0
Min: -0.943459689617157, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9457377791404724, Max: 0.954925537109375
Min: -1.0, Max: 1.0
Min: -0.998262882232666, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9841218590736389, Max: 0.8143374919891357
Min: -0.9935320019721985, Max: 0.9990369081497192
Min: -0.8559951186180115, Max: 0.9372549057006836
Min: -0.9844478964805603, Max: 0.983451247215271
Min: -1.0, Max: 1.0
Min: -0.9940371513366699, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.8667684197425842, Max: 0.9982120990753174
Min: -0.9778024554252625, Max: 0.9426681995391846
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.5321944952011108
Min: -1.0, Max: 1.0
Min: -0.993454098701477, Max: 0.9989304542541504
Min: -1.0, Max: 1.0
Min: -0.9973782896995544, Max: 1.0
Min: -0.9819697737693787, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.999588668346405, Max: 0.9914100170135498
Min: -1.0, Max: 0.9665424823760986
Min: -0.9958469867706299, Max: 0.7737071514129639
Min: -0.9932846426963806, Max: 1.0
Min: -0.8460366725921631, Max: 0.9922751188278198
Min: -0.9274265170097351, Max: 1.0
Min: -0.9896911978721619, Max: 0.9866318702697754
Min: -0.9951396584510803, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9967048168182373
Min: -0.9465298652648926, Max: 1.0
Min: -0.8692569732666016, Max: 1.0
Min: -1.0, Max: 0.96728515625
Min: -0.9850134253501892, Max: 1.0
Min: -1.0, Max: 0.9944804906845093
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.7451992034912109
Min: -0.9509897232055664, Max: 1.0
Min: -0.8288481831550598, Max: 0.6725746393203735
Min: -0.9794989228248596, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.8215008974075317
Min: -1.0, Max: 0.6234745979309082
Min: -0.957787275314331, Max: 1.0
Min: -0.8643553853034973, Max: 0.9991773366928101
Min: -1.0, Max: 1.0
Min: -0.9762108325958252, Max: 0.9890389442443848
Min: -0.9661014080047607, Max: 0.9941222667694092
Min: -1.0, Max: 0.9996843338012695
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9942185878753662
Min: -0.9664595127105713, Max: 1.0
Min: -1.0, Max: 0.9956941604614258
Min: -1.0, Max: 1.0
Min: -0.9443843364715576, Max: 0.910151481628418
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9826372861862183
Min: -1.0, Max: 1.0
Min: -0.9990739822387695, Max: 0.981383204460144
Min: -1.0, Max: 0.9980167150497437
Min: -0.9406060576438904, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.90849369764328, Max: 0.853716254234314
Min: -1.0, Max: 0.9944432973861694
Min: -0.919700026512146, Max: 0.6620854139328003
Min: -0.938266396522522, Max: 1.0
Min: -0.9984560012817383, Max: 1.0
Min: -0.9820675849914551, Max: 0.6974256038665771
Min: -0.9620867967605591, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9990813136100769, Max: 0.9976224899291992
Min: -1.0, Max: 0.9945166110992432
Min: -1.0, Max: 0.9806705713272095
Min: -0.9985575675964355, Max: 1.0
Min: -0.5570282936096191, Max: 1.0
Min: -1.0, Max: 0.9545040130615234
Min: -0.9608122110366821, Max: 0.4677087068557739
Min: -0.992513120174408, Max: 1.0
Min: -1.0, Max: 0.9983258247375488
Min: -1.0, Max: 0.869217038154602
Min: -1.0, Max: 0.9034439325332642
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9321028590202332, Max: 1.0
Min: -0.9911251068115234, Max: 0.7632385492324829
Min: -0.9731878042221069, Max: 0.9735287427902222
Min: -0.9978730082511902, Max: 0.9970279932022095
Min: -0.9131472706794739, Max: 1.0
Min: -0.9958357214927673, Max: 0.9768213033676147
Min: -0.9518886804580688, Max: 1.0
Min: -0.8572412729263306, Max: 0.9248768091201782
Min: -0.9892889261245728, Max: 1.0
Min: -0.7310374975204468, Max: 0.6471741199493408
Min: -0.9857995510101318, Max: 1.0
Min: -0.9828623533248901, Max: 0.9742976427078247
Min: -1.0, Max: 0.991855263710022
Min: -0.999737560749054, Max: 0.8607286214828491
Min: -1.0, Max: 1.0
Min: -0.9282287359237671, Max: 1.0
Min: -0.9600934982299805, Max: 0.8976109027862549
Min: -0.9206904172897339, Max: 1.0
Min: -0.9481408596038818, Max: 1.0
Min: -1.0, Max: 0.9229121208190918
Min: -0.952123761177063, Max: 0.9940099716186523
Min: -1.0, Max: 0.9942845106124878
Min: -1.0, Max: 1.0
Min: -0.9997567534446716, Max: 0.960004448890686
Min: -0.9779680967330933, Max: 1.0
Min: -0.7845617532730103, Max: 0.9985862970352173
Min: -0.8748936653137207, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9953931570053101
Min: -0.8237811326980591, Max: 0.9962011575698853
Min: -0.8413168787956238, Max: 1.0
Min: -0.9756960272789001, Max: 0.9426378011703491
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9790888428688049, Max: 1.0
Min: -1.0, Max: 0.9991735219955444
Min: -1.0, Max: 1.0
Min: -0.9867904186248779, Max: 1.0
Min: -0.9873400330543518, Max: 0.8381613492965698
Min: -0.8767036199569702, Max: 0.7268266677856445
Min: -0.999703049659729, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9954257011413574, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9603384137153625, Max: 1.0
Min: -0.9736565351486206, Max: 1.0
Min: -1.0, Max: 0.9944733381271362
Min: -1.0, Max: 0.981216549873352
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9998658299446106, Max: 0.9987456798553467
Min: -0.9619608521461487, Max: 0.6447272300720215
Min: -0.9616369009017944, Max: 1.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Min: -1.0, Max: 0.9923173189163208
Min: -0.9973931312561035, Max: 1.0
Min: -0.9967566728591919, Max: 0.8473668098449707
Min: -1.0, Max: 0.9693285226821899
Min: -1.0, Max: 0.9992609024047852
Min: -1.0, Max: 0.4151817560195923
Min: -0.921648383140564, Max: 0.7774288654327393
Min: -0.7771499156951904, Max: 0.8052307367324829
Min: -0.8324981927871704, Max: 1.0
Min: -0.6811962723731995, Max: 1.0
Min: -1.0, Max: 0.970758318901062
Min: -0.7772362232208252, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9884235262870789, Max: 0.9998446702957153
Min: -0.9228889346122742, Max: 0.995139479637146
Min: -0.9960281252861023, Max: 0.6412472724914551
Min: -1.0, Max: 1.0
Min: -0.9999673962593079, Max: 1.0
Min: -0.9031362533569336, Max: 0.9988119602203369
Min: -1.0, Max: 0.9813727140426636
Min: -1.0, Max: 1.0
Min: -0.9478951096534729, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9349386096000671, Max: 0.993709921836853
Min: -0.9996993541717529, Max: 1.0
Min: -0.9958864450454712, Max: 0.9967628717422485
Min: -1.0, Max: 1.0
Min: -0.9907575845718384, Max: 1.0
Min: -0.9988709092140198, Max: 0.9903244972229004
Min: -0.8326708674430847, Max: 1.0
Min: -0.9685755372047424, Max: 1.0
Min: -0.9982361197471619, Max: 0.6760908365249634
Min: -1.0, Max: 0.9980802536010742
Min: -0.9916478395462036, Max: 1.0
Min: -1.0, Max: 0.9845788478851318
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9978877305984497
Min: -1.0, Max: 0.7366776466369629
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9713506102561951, Max: 0.9267522096633911
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9932131767272949, Max: 0.9934005737304688
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9974991083145142
Min: -0.9115535020828247, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9804762005805969, Max: 1.0
Min: -0.8133825063705444, Max: 0.9995777606964111
Min: -1.0, Max: 0.9938453435897827
Min: -0.9959747195243835, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9990122318267822, Max: 1.0
Min: -0.9990434646606445, Max: 0.9720302820205688
Min: -0.9999195337295532, Max: 0.9930033683776855
Min: -1.0, Max: 0.9993765354156494
Min: -0.9764983654022217, Max: 1.0
Min: -1.0, Max: 0.9917963743209839
Min: -0.8722807765007019, Max: 0.9790209531784058
Min: -0.9761685132980347, Max: 0.9572358131408691
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9997535943984985, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9983646869659424
Min: -0.9975838661193848, Max: 1.0
Min: -0.9957113862037659, Max: 0.9998936653137207
Min: -0.9981313943862915, Max: 1.0
Min: -0.9044222831726074, Max: 1.0
Min: -1.0, Max: 0.8181780576705933
Min: -0.9988897442817688, Max: 1.0
Min: -0.9862320423126221, Max: 0.995405912399292
Min: -0.9993312954902649, Max: 1.0
Min: -0.9965760707855225, Max: 1.0
Min: -0.9030202031135559, Max: 1.0
Min: -0.9992650151252747, Max: 0.9873285293579102
Min: -0.9990018606185913, Max: 0.9893032312393188
Min: -1.0, Max: 1.0
Min: -0.998499870300293, Max: 0.995019793510437
Min: -1.0, Max: 1.0
Min: -0.9938212037086487, Max: 1.0
Min: -0.99775630235672, Max: 1.0
Min: -0.9449394345283508, Max: 0.9119219779968262
Min: -0.9947795867919922, Max: 0.9930317401885986
Min: -0.7329939603805542, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9991951584815979, Max: 0.9651381969451904
Min: -0.8521164655685425, Max: 1.0
Min: -1.0, Max: 0.9921324253082275
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.8216464519500732
Min: -0.9999859929084778, Max: 1.0
Min: -0.9733855128288269, Max: 0.9029650688171387
Min: -0.9843951463699341, Max: 0.9926363229751587
Min: -1.0, Max: 0.643302321434021
Min: -0.9995549321174622, Max: 0.9313316345214844
Min: -1.0, Max: 1.0
Min: -0.8207448720932007, Max: 0.9775127172470093
Min: -1.0, Max: 1.0
Min: -0.9989596605300903, Max: 1.0
Min: -0.9236719608306885, Max: 1.0
Min: -1.0, Max: 0.9997032880783081
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9937613010406494
Min: -1.0, Max: 1.0
Min: -0.9993472695350647, Max: 0.9291281700134277
Min: -1.0, Max: 1.0
Min: -0.8566858172416687, Max: 1.0
Min: -0.858906090259552, Max: 0.9216458797454834
Min: -0.9821215867996216, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9676907658576965, Max: 0.9155082702636719
Min: -1.0, Max: 0.9991426467895508
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.999483585357666
Min: -1.0, Max: 1.0
Min: -0.9992831349372864, Max: 1.0
Min: -0.9960250854492188, Max: 0.9968219995498657
Min: -1.0, Max: 0.9749115705490112
Min: -0.9441114068031311, Max: 0.9820023775100708
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9978545308113098, Max: 1.0
Min: -0.9997837543487549, Max: 0.9706132411956787
Min: -0.9987427592277527, Max: 0.9665998220443726
Min: -0.9799106121063232, Max: 0.9853067398071289
Min: -0.9774579405784607, Max: 1.0
Min: -0.9756019711494446, Max: 0.9917181730270386
Min: -1.0, Max: 0.9973399639129639
Min: -0.9748471975326538, Max: 1.0
Min: -0.9945590496063232, Max: 0.9372549057006836
Min: -1.0, Max: 0.9915996789932251
Min: -1.0, Max: 0.9872355461120605
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9964451789855957
Min: -0.9974942207336426, Max: 0.9687391519546509
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9995033740997314
Min: -0.9626350402832031, Max: 1.0
Min: -0.9989959001541138, Max: 0.9990456104278564
Min: -0.988432765007019, Max: 0.9999775886535645
Min: -0.9987928867340088, Max: 0.8690234422683716
Min: -1.0, Max: 0.99510657787323
Min: -0.9996351599693298, Max: 0.44364678859710693
Min: -1.0, Max: 0.9999027252197266
Min: -0.9963147640228271, Max: 1.0
Min: -0.9943599700927734, Max: 0.9988957643508911
Min: -1.0, Max: 0.9998661279678345
Min: -0.9883350133895874, Max: 0.9891698360443115
Min: -1.0, Max: 1.0
Min: -0.7926392555236816, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.7060974836349487
Min: -1.0, Max: 0.9998259544372559
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9675010442733765
Min: -1.0, Max: 1.0
Min: -0.9998903870582581, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9334927201271057, Max: 0.8863272666931152
Min: -1.0, Max: 0.9958779811859131
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9996452331542969, Max: 0.8343892097473145
Min: -1.0, Max: 1.0
Min: -0.9608199000358582, Max: 1.0
Min: -0.8656233549118042, Max: 1.0
Min: -0.9893516302108765, Max: 0.6804063320159912
Min: -0.9594279527664185, Max: 1.0
Min: -0.986872136592865, Max: 1.0
Min: -0.9999879002571106, Max: 1.0
Min: -1.0, Max: 0.9984148740768433
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9696317911148071
Min: -1.0, Max: 1.0
Min: -0.9995201230049133, Max: 0.9160162210464478
Min: -1.0, Max: 0.7907238006591797
Min: -0.9045192003250122, Max: 0.5236616134643555
Min: -0.9631986021995544, Max: 0.9828586578369141
Min: -0.9168242812156677, Max: 1.0
Min: -0.9998123049736023, Max: 0.9923464059829712
Min: -1.0, Max: 0.9280757904052734
Min: -1.0, Max: 0.9831624031066895
Min: -0.9973722100257874, Max: 1.0
Min: -0.9929258823394775, Max: 0.9917008876800537
Min: -0.8791065812110901, Max: 0.45897185802459717
Min: -0.8467110395431519, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.999646782875061, Max: 0.9970493316650391
Min: -0.9956305623054504, Max: 0.9996070861816406
Min: -0.9207451939582825, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.989299476146698, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.9970872402191162
Min: -1.0, Max: 1.0
Min: -0.9999239444732666, Max: 0.8273959159851074
Min: -0.9975903034210205, Max: 0.9997649192810059
Min: -0.8415016531944275, Max: 0.2041027545928955
Min: -0.9729660749435425, Max: 1.0
Min: -0.9827392101287842, Max: 0.8365943431854248
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.939532995223999
Min: -1.0, Max: 1.0
Min: -0.92071533203125, Max: 0.8803869485855103
Min: -0.9482118487358093, Max: 0.9965401887893677
Min: -0.9871848821640015, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.99904465675354, Max: 0.9689984321594238
Min: -0.9967212677001953, Max: 1.0
Min: -0.999651312828064, Max: 0.9999126195907593
Min: -1.0, Max: 1.0
Min: -0.9461040496826172, Max: 0.9849482774734497
Min: -0.9762977957725525, Max: 0.9938887357711792
Min: -0.8899223208427429, Max: 1.0
Min: -0.9780385494232178, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9762364625930786, Max: 0.9033129215240479
Min: -1.0, Max: 1.0
Min: -0.9999430775642395, Max: 1.0
Min: -1.0, Max: 0.9936027526855469
Min: -1.0, Max: 1.0
Min: -0.9634845852851868, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9998019933700562, Max: 0.9623802900314331
Min: -0.9948855042457581, Max: 1.0
Min: -0.9546855092048645, Max: 0.9959123134613037
Min: -0.988781750202179, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.6273912787437439, Max: 0.837815523147583
Min: -0.9568626284599304, Max: 0.9982033967971802
Min: -0.9966942667961121, Max: 0.9983782768249512
Min: -0.9327272772789001, Max: 0.6964291334152222
Min: -1.0, Max: 1.0
Min: -0.996544599533081, Max: 1.0
Min: -0.9995578527450562, Max: 0.982124924659729
Min: -1.0, Max: 1.0
Min: -0.902123212814331, Max: 1.0
Min: -0.9999232292175293, Max: 0.8804730176925659
Min: -0.4159809947013855, Max: 0.5445560216903687
Min: -0.9327449202537537, Max: 1.0
Min: -0.9995158314704895, Max: 1.0
Min: -1.0, Max: 0.9840675592422485
Min: -1.0, Max: 0.9741010665893555
Min: -1.0, Max: 0.9997208118438721
Min: -0.9948585033416748, Max: 1.0
Min: -1.0, Max: 1.0
Min: -0.9502653479576111, Max: 1.0
Min: -1.0, Max: 0.999709963798523
Min: -0.973253607749939, Max: 0.9948569536209106
Min: -0.9889271259307861, Max: 0.9976233243942261
Min: -0.9915258884429932, Max: 0.9137833118438721
Min: -0.9959295392036438, Max: 0.9455066919326782
Min: -0.9922413229942322, Max: 0.9970319271087646
Min: -0.9737911820411682, Max: 1.0
Min: -1.0, Max: 0.9998265504837036
Min: -1.0, Max: 0.5283746719360352
Min: -0.9824390411376953, Max: 0.9748358726501465
Min: -0.9974024295806885, Max: 0.9243708848953247
Min: -0.8563879728317261, Max: 0.9999665021896362
Min: -1.0, Max: 1.0
Min: -1.0, Max: 1.0
Min: -1.0, Max: 0.997697114944458
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-07-03 15:58:14.983971: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize the dataset</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">samples</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">train_horses_dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">train_zebras_dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">4</span><span class="p">))):</span>
    <span class="n">horse</span> <span class="o">=</span> <span class="p">(((</span><span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">127.5</span><span class="p">)</span> <span class="o">+</span> <span class="mf">127.5</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">zebra</span> <span class="o">=</span> <span class="p">(((</span><span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">127.5</span><span class="p">)</span> <span class="o">+</span> <span class="mf">127.5</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">horse</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">zebra</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-07-03 15:58:15.424502: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2025-07-03 15:58:15.426338: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
</pre></div>
</div>
<img alt="../_images/b80066626f2dcbb17e89c0ae9ca5afd3dfa9403c959ff7ec22ce9ab6ca923a0e.png" src="../_images/b80066626f2dcbb17e89c0ae9ca5afd3dfa9403c959ff7ec22ce9ab6ca923a0e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.8.0
2.18.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a Conv2D layer with a kernel initializer</span>
<span class="n">conv_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>  <span class="c1"># Using He initialization</span>
    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span>
<span class="p">)</span>

<span class="c1"># Example input shape (batch_size, height, width, channels)</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Batch size is None for flexibility</span>

<span class="c1"># Create a model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">conv_layer</span><span class="p">,</span>
<span class="p">])</span>

<span class="c1"># Print the model summary</span>
<span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ<span style="font-weight: bold"> Layer (type)                    </span>â”ƒ<span style="font-weight: bold"> Output Shape           </span>â”ƒ<span style="font-weight: bold">       Param # </span>â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ conv2d (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)                 â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">62</span>, <span style="color: #00af00; text-decoration-color: #00af00">62</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     â”‚           <span style="color: #00af00; text-decoration-color: #00af00">896</span> â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">896</span> (3.50 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">896</span> (3.50 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ReflectionPadding2D</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implements Reflection Padding as a layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        padding(tuple): Amount of padding for the</span>
<span class="sd">        spatial dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A padded tensor with the same type as the input tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">padding_width</span><span class="p">,</span> <span class="n">padding_height</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>
        <span class="n">padding_tensor</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
            <span class="p">[</span><span class="n">padding_height</span><span class="p">,</span> <span class="n">padding_height</span><span class="p">],</span>
            <span class="p">[</span><span class="n">padding_width</span><span class="p">,</span> <span class="n">padding_width</span><span class="p">],</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">padding_tensor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;REFLECT&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">residual_block</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_init</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
    <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_init</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">x</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">()(</span><span class="n">input_tensor</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GroupNormalization</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_initializer</span><span class="p">)(</span>
        <span class="n">x</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GroupNormalization</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_initializer</span><span class="p">)(</span>
        <span class="n">x</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">add</span><span class="p">([</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">downsample</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">filters</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_init</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_init</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GroupNormalization</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_initializer</span><span class="p">)(</span>
        <span class="n">x</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">activation</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">filters</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_init</span><span class="p">,</span>
    <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_init</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
        <span class="n">filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GroupNormalization</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_initializer</span><span class="p">)(</span>
        <span class="n">x</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">activation</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># build the generator</span>

<span class="k">def</span> <span class="nf">get_resnet_generator</span><span class="p">(</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">num_downsampling_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_residual_blocks</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
    <span class="n">num_upsample_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_init</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">img_input</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_img_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_img_input&quot;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">img_input</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_init</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span>
        <span class="n">x</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GroupNormalization</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="o">=</span><span class="n">gamma_initializer</span><span class="p">)(</span>
        <span class="n">x</span>
    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Downsampling</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_downsampling_blocks</span><span class="p">):</span>
        <span class="n">filters</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>

    <span class="c1"># Residual blocks</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_residual_blocks</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>

    <span class="c1"># Upsampling</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_upsample_blocks</span><span class="p">):</span>
        <span class="n">filters</span> <span class="o">//=</span> <span class="mi">2</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>

    <span class="c1"># Final block</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s2">&quot;tanh&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># build the discriminator</span>

<span class="k">def</span> <span class="nf">get_discriminator</span><span class="p">(</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_init</span><span class="p">,</span> <span class="n">num_downsampling</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="n">img_input</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_img_size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_img_input&quot;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">filters</span><span class="p">,</span>
        <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
        <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">img_input</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">num_filters</span> <span class="o">=</span> <span class="n">filters</span>
    <span class="k">for</span> <span class="n">num_downsample_block</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">num_filters</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">num_downsample_block</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">filters</span><span class="o">=</span><span class="n">num_filters</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">filters</span><span class="o">=</span><span class="n">num_filters</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="n">layers</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span>
    <span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">img_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># Get the generators</span>
<span class="n">gen_G</span> <span class="o">=</span> <span class="n">get_resnet_generator</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;generator_G&quot;</span><span class="p">)</span>
<span class="n">gen_F</span> <span class="o">=</span> <span class="n">get_resnet_generator</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;generator_F&quot;</span><span class="p">)</span>

<span class="c1"># Get the discriminators</span>
<span class="n">disc_X</span> <span class="o">=</span> <span class="n">get_discriminator</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;discriminator_X&quot;</span><span class="p">)</span>
<span class="n">disc_Y</span> <span class="o">=</span> <span class="n">get_discriminator</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;discriminator_Y&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># build the cycle gan</span>
<span class="k">class</span> <span class="nc">CycleGan</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generator_G</span><span class="p">,</span>
        <span class="n">generator_F</span><span class="p">,</span>
        <span class="n">discriminator_X</span><span class="p">,</span>
        <span class="n">discriminator_Y</span><span class="p">,</span>
        <span class="n">lambda_cycle</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
        <span class="n">lambda_identity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gen_G</span> <span class="o">=</span> <span class="n">generator_G</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gen_F</span> <span class="o">=</span> <span class="n">generator_F</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc_X</span> <span class="o">=</span> <span class="n">discriminator_X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc_Y</span> <span class="o">=</span> <span class="n">discriminator_Y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_cycle</span> <span class="o">=</span> <span class="n">lambda_cycle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_identity</span> <span class="o">=</span> <span class="n">lambda_identity</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">disc_X</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">disc_Y</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gen_G</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gen_F</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">gen_G_optimizer</span><span class="p">,</span>
        <span class="n">gen_F_optimizer</span><span class="p">,</span>
        <span class="n">disc_X_optimizer</span><span class="p">,</span>
        <span class="n">disc_Y_optimizer</span><span class="p">,</span>
        <span class="n">gen_loss_fn</span><span class="p">,</span>
        <span class="n">disc_loss_fn</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gen_G_optimizer</span> <span class="o">=</span> <span class="n">gen_G_optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gen_F_optimizer</span> <span class="o">=</span> <span class="n">gen_F_optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc_X_optimizer</span> <span class="o">=</span> <span class="n">disc_X_optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc_Y_optimizer</span> <span class="o">=</span> <span class="n">disc_Y_optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator_loss_fn</span> <span class="o">=</span> <span class="n">gen_loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator_loss_fn</span> <span class="o">=</span> <span class="n">disc_loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle_loss_fn</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MeanAbsoluteError</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">identity_loss_fn</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MeanAbsoluteError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_data</span><span class="p">):</span>
        <span class="c1"># x is Horse and y is zebra</span>
        <span class="n">real_x</span><span class="p">,</span> <span class="n">real_y</span> <span class="o">=</span> <span class="n">batch_data</span>

        <span class="c1"># For CycleGAN, we need to calculate different</span>
        <span class="c1"># kinds of losses for the generators and discriminators.</span>
        <span class="c1"># We will perform the following steps here:</span>
        <span class="c1">#</span>
        <span class="c1"># 1. Pass real images through the generators and get the generated images</span>
        <span class="c1"># 2. Pass the generated images back to the generators to check if we</span>
        <span class="c1">#    can predict the original image from the generated image.</span>
        <span class="c1"># 3. Do an identity mapping of the real images using the generators.</span>
        <span class="c1"># 4. Pass the generated images in 1) to the corresponding discriminators.</span>
        <span class="c1"># 5. Calculate the generators total loss (adversarial + cycle + identity)</span>
        <span class="c1"># 6. Calculate the discriminators loss</span>
        <span class="c1"># 7. Update the weights of the generators</span>
        <span class="c1"># 8. Update the weights of the discriminators</span>
        <span class="c1"># 9. Return the losses in a dictionary</span>

        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="c1"># Horse to fake zebra</span>
            <span class="n">fake_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_G</span><span class="p">(</span><span class="n">real_x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Zebra to fake horse -&gt; y2x</span>
            <span class="n">fake_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_F</span><span class="p">(</span><span class="n">real_y</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Cycle (Horse to fake zebra to fake horse): x -&gt; y -&gt; x</span>
            <span class="n">cycled_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_F</span><span class="p">(</span><span class="n">fake_y</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Cycle (Zebra to fake horse to fake zebra) y -&gt; x -&gt; y</span>
            <span class="n">cycled_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_G</span><span class="p">(</span><span class="n">fake_x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Identity mapping</span>
            <span class="n">same_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_F</span><span class="p">(</span><span class="n">real_x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">same_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_G</span><span class="p">(</span><span class="n">real_y</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Discriminator output</span>
            <span class="n">disc_real_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc_X</span><span class="p">(</span><span class="n">real_x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">disc_fake_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc_X</span><span class="p">(</span><span class="n">fake_x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">disc_real_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc_Y</span><span class="p">(</span><span class="n">real_y</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">disc_fake_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc_Y</span><span class="p">(</span><span class="n">fake_y</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Generator adversarial loss</span>
            <span class="n">gen_G_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_loss_fn</span><span class="p">(</span><span class="n">disc_fake_y</span><span class="p">)</span>
            <span class="n">gen_F_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_loss_fn</span><span class="p">(</span><span class="n">disc_fake_x</span><span class="p">)</span>

            <span class="c1"># Generator cycle loss</span>
            <span class="n">cycle_loss_G</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_loss_fn</span><span class="p">(</span><span class="n">real_y</span><span class="p">,</span> <span class="n">cycled_y</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_cycle</span>
            <span class="n">cycle_loss_F</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_loss_fn</span><span class="p">(</span><span class="n">real_x</span><span class="p">,</span> <span class="n">cycled_x</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_cycle</span>

            <span class="c1"># Generator identity loss</span>
            <span class="n">id_loss_G</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">identity_loss_fn</span><span class="p">(</span><span class="n">real_y</span><span class="p">,</span> <span class="n">same_y</span><span class="p">)</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_cycle</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_identity</span>
            <span class="p">)</span>
            <span class="n">id_loss_F</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">identity_loss_fn</span><span class="p">(</span><span class="n">real_x</span><span class="p">,</span> <span class="n">same_x</span><span class="p">)</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_cycle</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_identity</span>
            <span class="p">)</span>

            <span class="c1"># Total generator loss</span>
            <span class="n">total_loss_G</span> <span class="o">=</span> <span class="n">gen_G_loss</span> <span class="o">+</span> <span class="n">cycle_loss_G</span> <span class="o">+</span> <span class="n">id_loss_G</span>
            <span class="n">total_loss_F</span> <span class="o">=</span> <span class="n">gen_F_loss</span> <span class="o">+</span> <span class="n">cycle_loss_F</span> <span class="o">+</span> <span class="n">id_loss_F</span>

            <span class="c1"># Discriminator loss</span>
            <span class="n">disc_X_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator_loss_fn</span><span class="p">(</span><span class="n">disc_real_x</span><span class="p">,</span> <span class="n">disc_fake_x</span><span class="p">)</span>
            <span class="n">disc_Y_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator_loss_fn</span><span class="p">(</span><span class="n">disc_real_y</span><span class="p">,</span> <span class="n">disc_fake_y</span><span class="p">)</span>

        <span class="c1"># Get the gradients for the generators</span>
        <span class="n">grads_G</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">total_loss_G</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_G</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">grads_F</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">total_loss_F</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_F</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>

        <span class="c1"># Get the gradients for the discriminators</span>
        <span class="n">disc_X_grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">disc_X_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc_X</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">disc_Y_grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">disc_Y_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc_Y</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>

        <span class="c1"># Update the weights of the generators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gen_G_optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">grads_G</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_G</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gen_F_optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">grads_F</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen_F</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Update the weights of the discriminators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc_X_optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">disc_X_grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc_X</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc_Y_optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">disc_Y_grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc_Y</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;G_loss&quot;</span><span class="p">:</span> <span class="n">total_loss_G</span><span class="p">,</span>
            <span class="s2">&quot;F_loss&quot;</span><span class="p">:</span> <span class="n">total_loss_F</span><span class="p">,</span>
            <span class="s2">&quot;D_X_loss&quot;</span><span class="p">:</span> <span class="n">disc_X_loss</span><span class="p">,</span>
            <span class="s2">&quot;D_Y_loss&quot;</span><span class="p">:</span> <span class="n">disc_Y_loss</span><span class="p">,</span>
        <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create callback that saves generated images</span>

<span class="k">class</span> <span class="nc">GANMonitor</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">Callback</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A callback to generate and save images after each epoch&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_img</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_img</span> <span class="o">=</span> <span class="n">num_img</span>

    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_horses</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_img</span><span class="p">)):</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">gen_G</span><span class="p">(</span><span class="n">img</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">*</span> <span class="mf">127.5</span> <span class="o">+</span> <span class="mf">127.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="n">img</span> <span class="o">=</span> <span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">127.5</span> <span class="o">+</span> <span class="mf">127.5</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Input image&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Translated image&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

            <span class="n">prediction</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">array_to_img</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
            <span class="n">prediction</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                <span class="s2">&quot;generated_img_</span><span class="si">{i}</span><span class="s2">_</span><span class="si">{epoch}</span><span class="s2">.png&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># train the end-to-end model</span>

<span class="c1"># Loss function for evaluating adversarial loss</span>
<span class="n">adv_loss_fn</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MeanSquaredError</span><span class="p">()</span>

<span class="c1"># Define the loss function for the generators</span>


<span class="k">def</span> <span class="nf">generator_loss_fn</span><span class="p">(</span><span class="n">fake</span><span class="p">):</span>
    <span class="n">fake_loss</span> <span class="o">=</span> <span class="n">adv_loss_fn</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">fake</span><span class="p">),</span> <span class="n">fake</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fake_loss</span>


<span class="c1"># Define the loss function for the discriminators</span>
<span class="k">def</span> <span class="nf">discriminator_loss_fn</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">fake</span><span class="p">):</span>
    <span class="n">real_loss</span> <span class="o">=</span> <span class="n">adv_loss_fn</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">real</span><span class="p">),</span> <span class="n">real</span><span class="p">)</span>
    <span class="n">fake_loss</span> <span class="o">=</span> <span class="n">adv_loss_fn</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">fake</span><span class="p">),</span> <span class="n">fake</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">real_loss</span> <span class="o">+</span> <span class="n">fake_loss</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>


<span class="c1"># Create cycle gan model</span>
<span class="n">cycle_gan_model</span> <span class="o">=</span> <span class="n">CycleGan</span><span class="p">(</span>
    <span class="n">generator_G</span><span class="o">=</span><span class="n">gen_G</span><span class="p">,</span> <span class="n">generator_F</span><span class="o">=</span><span class="n">gen_F</span><span class="p">,</span> <span class="n">discriminator_X</span><span class="o">=</span><span class="n">disc_X</span><span class="p">,</span> <span class="n">discriminator_Y</span><span class="o">=</span><span class="n">disc_Y</span>
<span class="p">)</span>

<span class="c1"># Compile the model</span>
<span class="n">cycle_gan_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">gen_G_optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">gen_F_optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">disc_X_optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">disc_Y_optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">gen_loss_fn</span><span class="o">=</span><span class="n">generator_loss_fn</span><span class="p">,</span>
    <span class="n">disc_loss_fn</span><span class="o">=</span><span class="n">discriminator_loss_fn</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Callbacks</span>
<span class="n">plotter</span> <span class="o">=</span> <span class="n">GANMonitor</span><span class="p">()</span>
<span class="n">checkpoint_filepath</span> <span class="o">=</span> <span class="s2">&quot;./model_checkpoints/cyclegan_checkpoints.weights.h5&quot;</span>
<span class="n">model_checkpoint_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
    <span class="n">filepath</span><span class="o">=</span><span class="n">checkpoint_filepath</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Here we will train the model for just one epoch as each epoch takes around</span>
<span class="c1"># 7 minutes on a single P100 backed machine.</span>
<span class="n">cycle_gan_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">((</span><span class="n">train_horses_dataset</span><span class="p">,</span> <span class="n">train_zebras_dataset</span><span class="p">)),</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">plotter</span><span class="p">,</span> <span class="n">model_checkpoint_callback</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/90
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">17</span><span class="p">],</span> <span class="n">line</span> <span class="mi">44</span>
<span class="g g-Whitespace">     </span><span class="mi">38</span> <span class="n">model_checkpoint_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">39</span>     <span class="n">filepath</span><span class="o">=</span><span class="n">checkpoint_filepath</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">True</span>
<span class="g g-Whitespace">     </span><span class="mi">40</span> <span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span> <span class="c1"># Here we will train the model for just one epoch as each epoch takes around</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span> <span class="c1"># 7 minutes on a single P100 backed machine.</span>
<span class="ne">---&gt; </span><span class="mi">44</span> <span class="n">cycle_gan_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">45</span>     <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">((</span><span class="n">train_horses_dataset</span><span class="p">,</span> <span class="n">train_zebras_dataset</span><span class="p">)),</span>
<span class="g g-Whitespace">     </span><span class="mi">46</span>     <span class="n">epochs</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">47</span>     <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">plotter</span><span class="p">,</span> <span class="n">model_checkpoint_callback</span><span class="p">],</span>
<span class="g g-Whitespace">     </span><span class="mi">48</span> <span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117,</span> in <span class="ni">filter_traceback.&lt;locals&gt;.error_handler</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">115</span> <span class="n">filtered_tb</span> <span class="o">=</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">116</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">117</span>     <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">118</span> <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">119</span>     <span class="n">filtered_tb</span> <span class="o">=</span> <span class="n">_process_traceback_frames</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">__traceback__</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:371,</span> in <span class="ni">TensorFlowTrainer.fit</span><span class="nt">(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)</span>
<span class="g g-Whitespace">    </span><span class="mi">369</span> <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">epoch_iterator</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">370</span>     <span class="n">callbacks</span><span class="o">.</span><span class="n">on_train_batch_begin</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">371</span>     <span class="n">logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_function</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">372</span>     <span class="n">callbacks</span><span class="o">.</span><span class="n">on_train_batch_end</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">logs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">373</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_training</span><span class="p">:</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:219,</span> in <span class="ni">TensorFlowTrainer._make_function.&lt;locals&gt;.function</span><span class="nt">(iterator)</span>
<span class="g g-Whitespace">    </span><span class="mi">215</span> <span class="k">def</span> <span class="nf">function</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">216</span>     <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">217</span>         <span class="n">iterator</span><span class="p">,</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Iterator</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">DistributedIterator</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">218</span>     <span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">219</span>         <span class="n">opt_outputs</span> <span class="o">=</span> <span class="n">multi_step_on_iterator</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">220</span>         <span class="k">if</span> <span class="ow">not</span> <span class="n">opt_outputs</span><span class="o">.</span><span class="n">has_value</span><span class="p">():</span>
<span class="g g-Whitespace">    </span><span class="mi">221</span>             <span class="k">raise</span> <span class="ne">StopIteration</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150,</span> in <span class="ni">filter_traceback.&lt;locals&gt;.error_handler</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">148</span> <span class="n">filtered_tb</span> <span class="o">=</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">149</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">150</span>   <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">151</span> <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">152</span>   <span class="n">filtered_tb</span> <span class="o">=</span> <span class="n">_process_traceback_frames</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">__traceback__</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833,</span> in <span class="ni">Function.__call__</span><span class="nt">(self, *args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">830</span> <span class="n">compiler</span> <span class="o">=</span> <span class="s2">&quot;xla&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jit_compile</span> <span class="k">else</span> <span class="s2">&quot;nonXla&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">832</span> <span class="k">with</span> <span class="n">OptionalXlaContext</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jit_compile</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">833</span>   <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">835</span> <span class="n">new_tracing_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experimental_get_tracing_count</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">836</span> <span class="n">without_tracing</span> <span class="o">=</span> <span class="p">(</span><span class="n">tracing_count</span> <span class="o">==</span> <span class="n">new_tracing_count</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:889,</span> in <span class="ni">Function._call</span><span class="nt">(self, *args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">886</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">887</span>   <span class="c1"># This is the first call of __call__, so we have to initialize.</span>
<span class="g g-Whitespace">    </span><span class="mi">888</span>   <span class="n">initializers</span> <span class="o">=</span> <span class="p">[]</span>
<span class="ne">--&gt; </span><span class="mi">889</span>   <span class="bp">self</span><span class="o">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwds</span><span class="p">,</span> <span class="n">add_initializers_to</span><span class="o">=</span><span class="n">initializers</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">890</span> <span class="k">finally</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">891</span>   <span class="c1"># At this point we know that the initialization is complete (or less</span>
<span class="g g-Whitespace">    </span><span class="mi">892</span>   <span class="c1"># interestingly an exception was raised) so we no longer need a lock.</span>
<span class="g g-Whitespace">    </span><span class="mi">893</span>   <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:696,</span> in <span class="ni">Function._initialize</span><span class="nt">(self, args, kwds, add_initializers_to)</span>
<span class="g g-Whitespace">    </span><span class="mi">691</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_creation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_scoped_tracing_options</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">692</span>     <span class="n">variable_capturing_scope</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">693</span>     <span class="n">tracing_compilation</span><span class="o">.</span><span class="n">ScopeType</span><span class="o">.</span><span class="n">VARIABLE_CREATION</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">694</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">695</span> <span class="c1"># Force the definition of the function for these arguments</span>
<span class="ne">--&gt; </span><span class="mi">696</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concrete_variable_creation_fn</span> <span class="o">=</span> <span class="n">tracing_compilation</span><span class="o">.</span><span class="n">trace_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">697</span>     <span class="n">args</span><span class="p">,</span> <span class="n">kwds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_creation_config</span>
<span class="g g-Whitespace">    </span><span class="mi">698</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">700</span> <span class="k">def</span> <span class="nf">invalid_creator_scope</span><span class="p">(</span><span class="o">*</span><span class="n">unused_args</span><span class="p">,</span> <span class="o">**</span><span class="n">unused_kwds</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">701</span><span class="w">   </span><span class="sd">&quot;&quot;&quot;Disables variable creation.&quot;&quot;&quot;</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178,</span> in <span class="ni">trace_function</span><span class="nt">(args, kwargs, tracing_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">175</span>     <span class="n">args</span> <span class="o">=</span> <span class="n">tracing_options</span><span class="o">.</span><span class="n">input_signature</span>
<span class="g g-Whitespace">    </span><span class="mi">176</span>     <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
<span class="ne">--&gt; </span><span class="mi">178</span>   <span class="n">concrete_function</span> <span class="o">=</span> <span class="n">_maybe_define_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">179</span>       <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">tracing_options</span>
<span class="g g-Whitespace">    </span><span class="mi">180</span>   <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">182</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tracing_options</span><span class="o">.</span><span class="n">bind_graph_to_function</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">183</span>   <span class="n">concrete_function</span><span class="o">.</span><span class="n">_garbage_collector</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283,</span> in <span class="ni">_maybe_define_function</span><span class="nt">(args, kwargs, tracing_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">281</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">282</span>   <span class="n">target_func_type</span> <span class="o">=</span> <span class="n">lookup_func_type</span>
<span class="ne">--&gt; </span><span class="mi">283</span> <span class="n">concrete_function</span> <span class="o">=</span> <span class="n">_create_concrete_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">284</span>     <span class="n">target_func_type</span><span class="p">,</span> <span class="n">lookup_func_context</span><span class="p">,</span> <span class="n">func_graph</span><span class="p">,</span> <span class="n">tracing_options</span>
<span class="g g-Whitespace">    </span><span class="mi">285</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">287</span> <span class="k">if</span> <span class="n">tracing_options</span><span class="o">.</span><span class="n">function_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">288</span>   <span class="n">tracing_options</span><span class="o">.</span><span class="n">function_cache</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">289</span>       <span class="n">concrete_function</span><span class="p">,</span> <span class="n">current_func_context</span>
<span class="g g-Whitespace">    </span><span class="mi">290</span>   <span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310,</span> in <span class="ni">_create_concrete_function</span><span class="nt">(function_type, type_context, func_graph, tracing_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">303</span>   <span class="n">placeholder_bound_args</span> <span class="o">=</span> <span class="n">function_type</span><span class="o">.</span><span class="n">placeholder_arguments</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">304</span>       <span class="n">placeholder_context</span>
<span class="g g-Whitespace">    </span><span class="mi">305</span>   <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">307</span> <span class="n">disable_acd</span> <span class="o">=</span> <span class="n">tracing_options</span><span class="o">.</span><span class="n">attributes</span> <span class="ow">and</span> <span class="n">tracing_options</span><span class="o">.</span><span class="n">attributes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">308</span>     <span class="n">attributes_lib</span><span class="o">.</span><span class="n">DISABLE_ACD</span><span class="p">,</span> <span class="kc">False</span>
<span class="g g-Whitespace">    </span><span class="mi">309</span> <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">310</span> <span class="n">traced_func_graph</span> <span class="o">=</span> <span class="n">func_graph_module</span><span class="o">.</span><span class="n">func_graph_from_py_func</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">311</span>     <span class="n">tracing_options</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">312</span>     <span class="n">tracing_options</span><span class="o">.</span><span class="n">python_function</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">313</span>     <span class="n">placeholder_bound_args</span><span class="o">.</span><span class="n">args</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">314</span>     <span class="n">placeholder_bound_args</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">315</span>     <span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">316</span>     <span class="n">func_graph</span><span class="o">=</span><span class="n">func_graph</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">317</span>     <span class="n">add_control_dependencies</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_acd</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">318</span>     <span class="n">arg_names</span><span class="o">=</span><span class="n">function_type_utils</span><span class="o">.</span><span class="n">to_arg_names</span><span class="p">(</span><span class="n">function_type</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">319</span>     <span class="n">create_placeholders</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">320</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">322</span> <span class="n">transform</span><span class="o">.</span><span class="n">apply_func_graph_transforms</span><span class="p">(</span><span class="n">traced_func_graph</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">324</span> <span class="n">graph_capture_container</span> <span class="o">=</span> <span class="n">traced_func_graph</span><span class="o">.</span><span class="n">function_captures</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1059,</span> in <span class="ni">func_graph_from_py_func</span><span class="nt">(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)</span>
<span class="g g-Whitespace">   </span><span class="mi">1056</span>   <span class="k">return</span> <span class="n">x</span>
<span class="g g-Whitespace">   </span><span class="mi">1058</span> <span class="n">_</span><span class="p">,</span> <span class="n">original_func</span> <span class="o">=</span> <span class="n">tf_decorator</span><span class="o">.</span><span class="n">unwrap</span><span class="p">(</span><span class="n">python_func</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1059</span> <span class="n">func_outputs</span> <span class="o">=</span> <span class="n">python_func</span><span class="p">(</span><span class="o">*</span><span class="n">func_args</span><span class="p">,</span> <span class="o">**</span><span class="n">func_kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1061</span> <span class="c1"># invariant: `func_outputs` contains only Tensors, CompositeTensors,</span>
<span class="g g-Whitespace">   </span><span class="mi">1062</span> <span class="c1"># TensorArrays and `None`s.</span>
<span class="g g-Whitespace">   </span><span class="mi">1063</span> <span class="n">func_outputs</span> <span class="o">=</span> <span class="n">variable_utils</span><span class="o">.</span><span class="n">convert_variables_to_tensors</span><span class="p">(</span><span class="n">func_outputs</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:599,</span> in <span class="ni">Function._generate_scoped_tracing_options.&lt;locals&gt;.wrapped_fn</span><span class="nt">(*args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">595</span> <span class="k">with</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">_variable_creator_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">priority</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
<span class="g g-Whitespace">    </span><span class="mi">596</span>   <span class="c1"># __wrapped__ allows AutoGraph to swap in a converted function. We give</span>
<span class="g g-Whitespace">    </span><span class="mi">597</span>   <span class="c1"># the function a weak reference to itself to avoid a reference cycle.</span>
<span class="g g-Whitespace">    </span><span class="mi">598</span>   <span class="k">with</span> <span class="n">OptionalXlaContext</span><span class="p">(</span><span class="n">compile_with_xla</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">599</span>     <span class="n">out</span> <span class="o">=</span> <span class="n">weak_wrapped_fn</span><span class="p">()</span><span class="o">.</span><span class="n">__wrapped__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">600</span>   <span class="k">return</span> <span class="n">out</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41,</span> in <span class="ni">py_func_from_autograph.&lt;locals&gt;.autograph_handler</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">39</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;Calls a converted version of original_func.&quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">40</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">41</span>   <span class="k">return</span> <span class="n">api</span><span class="o">.</span><span class="n">converted_call</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span>       <span class="n">original_func</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span>       <span class="n">args</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">44</span>       <span class="n">kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">45</span>       <span class="n">options</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">ConversionOptions</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">46</span>           <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">47</span>           <span class="n">optional_features</span><span class="o">=</span><span class="n">autograph_options</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">48</span>           <span class="n">user_requested</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">49</span>       <span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span> <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># pylint:disable=broad-except</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span>   <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="s2">&quot;ag_error_metadata&quot;</span><span class="p">):</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:339,</span> in <span class="ni">converted_call</span><span class="nt">(f, args, kwargs, caller_fn_scope, options)</span>
<span class="g g-Whitespace">    </span><span class="mi">337</span> <span class="k">if</span> <span class="n">is_autograph_artifact</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">338</span>   <span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;Permanently allowed: </span><span class="si">%s</span><span class="s1">: AutoGraph artifact&#39;</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">339</span>   <span class="k">return</span> <span class="n">_call_unconverted</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">options</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">341</span> <span class="c1"># If this is a partial, unwrap it and redo all the checks.</span>
<span class="g g-Whitespace">    </span><span class="mi">342</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">):</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:459,</span> in <span class="ni">_call_unconverted</span><span class="nt">(f, args, kwargs, options, update_cache)</span>
<span class="g g-Whitespace">    </span><span class="mi">456</span>   <span class="k">return</span> <span class="n">f</span><span class="o">.</span><span class="vm">__self__</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">458</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">459</span>   <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">460</span> <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:643,</span> in <span class="ni">do_not_convert.&lt;locals&gt;.wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">641</span> <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">642</span>   <span class="k">with</span> <span class="n">ag_ctx</span><span class="o">.</span><span class="n">ControlStatusCtx</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">ag_ctx</span><span class="o">.</span><span class="n">Status</span><span class="o">.</span><span class="n">DISABLED</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">643</span>     <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:132,</span> in <span class="ni">TensorFlowTrainer._make_function.&lt;locals&gt;.multi_step_on_iterator</span><span class="nt">(iterator)</span>
<span class="g g-Whitespace">    </span><span class="mi">128</span> <span class="nd">@tf</span><span class="o">.</span><span class="n">autograph</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">do_not_convert</span>
<span class="g g-Whitespace">    </span><span class="mi">129</span> <span class="k">def</span> <span class="nf">multi_step_on_iterator</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">130</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">steps_per_execution</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">131</span>         <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">Optional</span><span class="o">.</span><span class="n">from_value</span><span class="p">(</span>
<span class="ne">--&gt; </span><span class="mi">132</span>             <span class="n">one_step_on_data</span><span class="p">(</span><span class="n">iterator</span><span class="o">.</span><span class="n">get_next</span><span class="p">())</span>
<span class="g g-Whitespace">    </span><span class="mi">133</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">135</span>     <span class="c1"># the spec is set lazily during the tracing of `tf.while_loop`</span>
<span class="g g-Whitespace">    </span><span class="mi">136</span>     <span class="n">empty_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">Optional</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150,</span> in <span class="ni">filter_traceback.&lt;locals&gt;.error_handler</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">148</span> <span class="n">filtered_tb</span> <span class="o">=</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">149</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">150</span>   <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">151</span> <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">152</span>   <span class="n">filtered_tb</span> <span class="o">=</span> <span class="n">_process_traceback_frames</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">__traceback__</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833,</span> in <span class="ni">Function.__call__</span><span class="nt">(self, *args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">830</span> <span class="n">compiler</span> <span class="o">=</span> <span class="s2">&quot;xla&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jit_compile</span> <span class="k">else</span> <span class="s2">&quot;nonXla&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">832</span> <span class="k">with</span> <span class="n">OptionalXlaContext</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jit_compile</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">833</span>   <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">835</span> <span class="n">new_tracing_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experimental_get_tracing_count</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">836</span> <span class="n">without_tracing</span> <span class="o">=</span> <span class="p">(</span><span class="n">tracing_count</span> <span class="o">==</span> <span class="n">new_tracing_count</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:906,</span> in <span class="ni">Function._call</span><span class="nt">(self, *args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">902</span>     <span class="k">pass</span>  <span class="c1"># Fall through to cond-based initialization.</span>
<span class="g g-Whitespace">    </span><span class="mi">903</span>   <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">904</span>     <span class="c1"># Lifting succeeded, so variables are initialized and we can run the</span>
<span class="g g-Whitespace">    </span><span class="mi">905</span>     <span class="c1"># no_variable_creation function.</span>
<span class="ne">--&gt; </span><span class="mi">906</span>     <span class="k">return</span> <span class="n">tracing_compilation</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">907</span>         <span class="n">args</span><span class="p">,</span> <span class="n">kwds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_no_variable_creation_config</span>
<span class="g g-Whitespace">    </span><span class="mi">908</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">909</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">910</span>   <span class="n">bound_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concrete_variable_creation_fn</span><span class="o">.</span><span class="n">function_type</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">911</span>       <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span>
<span class="g g-Whitespace">    </span><span class="mi">912</span>   <span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132,</span> in <span class="ni">call_function</span><span class="nt">(args, kwargs, tracing_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">130</span> <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="k">if</span> <span class="n">args</span> <span class="k">else</span> <span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">131</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="p">{}</span>
<span class="ne">--&gt; </span><span class="mi">132</span> <span class="n">function</span> <span class="o">=</span> <span class="n">trace_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">133</span>     <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">tracing_options</span><span class="o">=</span><span class="n">tracing_options</span>
<span class="g g-Whitespace">    </span><span class="mi">134</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">136</span> <span class="c1"># Bind it ourselves to skip unnecessary canonicalization of default call.</span>
<span class="g g-Whitespace">    </span><span class="mi">137</span> <span class="n">bound_args</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">function_type</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178,</span> in <span class="ni">trace_function</span><span class="nt">(args, kwargs, tracing_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">175</span>     <span class="n">args</span> <span class="o">=</span> <span class="n">tracing_options</span><span class="o">.</span><span class="n">input_signature</span>
<span class="g g-Whitespace">    </span><span class="mi">176</span>     <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
<span class="ne">--&gt; </span><span class="mi">178</span>   <span class="n">concrete_function</span> <span class="o">=</span> <span class="n">_maybe_define_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">179</span>       <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">tracing_options</span>
<span class="g g-Whitespace">    </span><span class="mi">180</span>   <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">182</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tracing_options</span><span class="o">.</span><span class="n">bind_graph_to_function</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">183</span>   <span class="n">concrete_function</span><span class="o">.</span><span class="n">_garbage_collector</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>  <span class="c1"># pylint: disable=protected-access</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283,</span> in <span class="ni">_maybe_define_function</span><span class="nt">(args, kwargs, tracing_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">281</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">282</span>   <span class="n">target_func_type</span> <span class="o">=</span> <span class="n">lookup_func_type</span>
<span class="ne">--&gt; </span><span class="mi">283</span> <span class="n">concrete_function</span> <span class="o">=</span> <span class="n">_create_concrete_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">284</span>     <span class="n">target_func_type</span><span class="p">,</span> <span class="n">lookup_func_context</span><span class="p">,</span> <span class="n">func_graph</span><span class="p">,</span> <span class="n">tracing_options</span>
<span class="g g-Whitespace">    </span><span class="mi">285</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">287</span> <span class="k">if</span> <span class="n">tracing_options</span><span class="o">.</span><span class="n">function_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">288</span>   <span class="n">tracing_options</span><span class="o">.</span><span class="n">function_cache</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">289</span>       <span class="n">concrete_function</span><span class="p">,</span> <span class="n">current_func_context</span>
<span class="g g-Whitespace">    </span><span class="mi">290</span>   <span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:340,</span> in <span class="ni">_create_concrete_function</span><span class="nt">(function_type, type_context, func_graph, tracing_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span> <span class="n">output_type</span> <span class="o">=</span> <span class="n">trace_type</span><span class="o">.</span><span class="n">from_value</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">332</span>     <span class="n">traced_func_graph</span><span class="o">.</span><span class="n">structured_outputs</span><span class="p">,</span> <span class="n">type_context</span>
<span class="g g-Whitespace">    </span><span class="mi">333</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">334</span> <span class="n">traced_func_type</span> <span class="o">=</span> <span class="n">function_type_lib</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">335</span>     <span class="n">function_type</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
<span class="g g-Whitespace">    </span><span class="mi">336</span>     <span class="n">traced_func_graph</span><span class="o">.</span><span class="n">function_captures</span><span class="o">.</span><span class="n">capture_types</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">337</span>     <span class="n">return_annotation</span><span class="o">=</span><span class="n">output_type</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">338</span> <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">340</span> <span class="n">concrete_function</span> <span class="o">=</span> <span class="n">concrete_function_lib</span><span class="o">.</span><span class="n">ConcreteFunction</span><span class="o">.</span><span class="n">from_func_graph</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">341</span>     <span class="n">traced_func_graph</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">342</span>     <span class="n">traced_func_type</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">343</span>     <span class="n">tracing_options</span><span class="o">.</span><span class="n">attributes</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">344</span>     <span class="c1"># Tell the ConcreteFunction to clean up its graph once it goes out of</span>
<span class="g g-Whitespace">    </span><span class="mi">345</span>     <span class="c1"># scope. This is not the default behavior since it gets used in some</span>
<span class="g g-Whitespace">    </span><span class="mi">346</span>     <span class="c1"># places (like Keras) where the FuncGraph lives longer than the</span>
<span class="g g-Whitespace">    </span><span class="mi">347</span>     <span class="c1"># ConcreteFunction.</span>
<span class="g g-Whitespace">    </span><span class="mi">348</span>     <span class="n">shared_func_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">349</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">350</span> <span class="n">_set_arg_keywords</span><span class="p">(</span><span class="n">concrete_function</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">351</span> <span class="n">transform</span><span class="o">.</span><span class="n">call_concrete_function_callbacks</span><span class="p">(</span><span class="n">concrete_function</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1078,</span> in <span class="ni">ConcreteFunction.from_func_graph</span><span class="nt">(cls, graph, function_type, attrs, shared_func_graph)</span>
<span class="g g-Whitespace">   </span><span class="mi">1073</span> <span class="nd">@classmethod</span>
<span class="g g-Whitespace">   </span><span class="mi">1074</span> <span class="k">def</span> <span class="nf">from_func_graph</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">function_type</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">shared_func_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1075</span>   <span class="n">atomic_fn</span> <span class="o">=</span> <span class="n">atomic_function</span><span class="o">.</span><span class="n">from_func_graph</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1076</span>       <span class="n">_inference_name</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">name</span><span class="p">),</span> <span class="n">graph</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">function_type</span>
<span class="g g-Whitespace">   </span><span class="mi">1077</span>   <span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1078</span>   <span class="k">return</span> <span class="n">ConcreteFunction</span><span class="p">(</span><span class="n">atomic_fn</span><span class="p">,</span> <span class="n">shared_func_graph</span><span class="o">=</span><span class="n">shared_func_graph</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1065,</span> in <span class="ni">ConcreteFunction.__init__</span><span class="nt">(self, atomic_fn, shared_func_graph)</span>
<span class="g g-Whitespace">   </span><span class="mi">1057</span>   <span class="bp">self</span><span class="o">.</span><span class="n">_garbage_collector</span> <span class="o">=</span> <span class="n">ConcreteFunctionGarbageCollector</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1058</span>       <span class="n">atomic_fn</span><span class="o">.</span><span class="n">graph</span>
<span class="g g-Whitespace">   </span><span class="mi">1059</span>   <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1061</span> <span class="c1"># Pairs of forward and backward functions used for computing gradients.</span>
<span class="g g-Whitespace">   </span><span class="mi">1062</span> <span class="c1">#</span>
<span class="g g-Whitespace">   </span><span class="mi">1063</span> <span class="c1"># These each get a reference to the FuncGraph deleter since they use the</span>
<span class="g g-Whitespace">   </span><span class="mi">1064</span> <span class="c1"># FuncGraph directly.</span>
<span class="ne">-&gt; </span><span class="mi">1065</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delayed_rewrite_functions</span> <span class="o">=</span> <span class="n">_DelayedRewriteGradientFunctions</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1066</span>     <span class="n">atomic_fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_garbage_collector</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1067</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_order_tape_functions</span> <span class="o">=</span> <span class="p">{}</span>
<span class="g g-Whitespace">   </span><span class="mi">1068</span> <span class="bp">self</span><span class="o">.</span><span class="n">_higher_order_tape_functions</span> <span class="o">=</span> <span class="p">{}</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:130,</span> in <span class="ni">_DelayedRewriteGradientFunctions.__init__</span><span class="nt">(self, atomic_fn, func_graph_deleter)</span>
<span class="g g-Whitespace">    </span><span class="mi">128</span> <span class="bp">self</span><span class="o">.</span><span class="n">_func_graph</span> <span class="o">=</span> <span class="n">atomic_fn</span><span class="o">.</span><span class="n">graph</span>
<span class="g g-Whitespace">    </span><span class="mi">129</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inference_function</span> <span class="o">=</span> <span class="n">atomic_fn</span>
<span class="ne">--&gt; </span><span class="mi">130</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attrs</span> <span class="o">=</span> <span class="n">atomic_fn</span><span class="o">.</span><span class="n">attributes</span>
<span class="g g-Whitespace">    </span><span class="mi">131</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_name</span> <span class="o">=</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">132</span> <span class="c1"># Note that the FuncGraph is mutated later, so we need to inspect it now to</span>
<span class="g g-Whitespace">    </span><span class="mi">133</span> <span class="c1"># figure out the user-specified outputs of the inference function.</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:149,</span> in <span class="ni">AtomicFunction.attributes</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">146</span> <span class="nd">@property</span>
<span class="g g-Whitespace">    </span><span class="mi">147</span> <span class="k">def</span> <span class="nf">attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">148</span><span class="w">   </span><span class="sd">&quot;&quot;&quot;Returns FunctionDef attributes in the Runtime.&quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">149</span>   <span class="n">attrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">attr</span>
<span class="g g-Whitespace">    </span><span class="mi">150</span>   <span class="c1"># Remove construction context since it is specific to runtime and this fn.</span>
<span class="g g-Whitespace">    </span><span class="mi">151</span>   <span class="n">attrs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">attributes_lib</span><span class="o">.</span><span class="n">EAGER_RUNTIME_CONSTRUCTION_CONTEXT</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:144,</span> in <span class="ni">AtomicFunction.definition</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">141</span> <span class="nd">@property</span>
<span class="g g-Whitespace">    </span><span class="mi">142</span> <span class="k">def</span> <span class="nf">definition</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">function_pb2</span><span class="o">.</span><span class="n">FunctionDef</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">143</span><span class="w">   </span><span class="sd">&quot;&quot;&quot;Current FunctionDef in the Runtime.&quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">144</span>   <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bound_context</span><span class="o">.</span><span class="n">get_function_def</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1592,</span> in <span class="ni">Context.get_function_def</span><span class="nt">(self, name)</span>
<span class="g g-Whitespace">   </span><span class="mi">1590</span>     <span class="n">proto_data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GetBuffer</span><span class="p">(</span><span class="n">buffer_</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1591</span>   <span class="n">function_def</span> <span class="o">=</span> <span class="n">function_pb2</span><span class="o">.</span><span class="n">FunctionDef</span><span class="p">()</span>
<span class="ne">-&gt; </span><span class="mi">1592</span>   <span class="n">function_def</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">proto_data</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1593</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1594</span>   <span class="n">function_def</span> <span class="o">=</span> <span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">TFE_ContextGetFunctionDefNoSerialization</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1595</span>       <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">name</span>
<span class="g g-Whitespace">   </span><span class="mi">1596</span>   <span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/message.py:202,</span> in <span class="ni">Message.ParseFromString</span><span class="nt">(self, serialized)</span>
<span class="g g-Whitespace">    </span><span class="mi">194</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;Parse serialized protocol buffer data into this message.</span>
<span class="g g-Whitespace">    </span><span class="mi">195</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">196</span><span class="sd"> Like :func:`MergeFromString()`, except we clear the object first.</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">199</span><span class="sd">   message.DecodeError if the input cannot be parsed.</span>
<span class="g g-Whitespace">    </span><span class="mi">200</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">201</span> <span class="bp">self</span><span class="o">.</span><span class="n">Clear</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">202</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">MergeFromString</span><span class="p">(</span><span class="n">serialized</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/internal/python_message.py:1128,</span> in <span class="ni">_AddMergeFromStringMethod.&lt;locals&gt;.MergeFromString</span><span class="nt">(self, serialized)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">serialized</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1128</span>   <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_InternalParse</span><span class="p">(</span><span class="n">serialized</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span> <span class="o">!=</span> <span class="n">length</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>     <span class="c1"># The only reason _InternalParse would return early is if it</span>
<span class="g g-Whitespace">   </span><span class="mi">1130</span>     <span class="c1"># encountered an end-group tag.</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span>     <span class="k">raise</span> <span class="n">message_mod</span><span class="o">.</span><span class="n">DecodeError</span><span class="p">(</span><span class="s1">&#39;Unexpected end-group tag.&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="k">except</span> <span class="p">(</span><span class="ne">IndexError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1133</span>   <span class="c1"># Now ord(buf[p:p+1]) == ord(&#39;&#39;) gets TypeError.</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/internal/python_message.py:1195,</span> in <span class="ni">_AddMergeFromStringMethod.&lt;locals&gt;.InternalParse</span><span class="nt">(self, buffer, pos, end)</span>
<span class="g g-Whitespace">   </span><span class="mi">1193</span>   <span class="n">pos</span> <span class="o">=</span> <span class="n">new_pos</span>
<span class="g g-Whitespace">   </span><span class="mi">1194</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1195</span>   <span class="n">pos</span> <span class="o">=</span> <span class="n">field_decoder</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">new_pos</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">field_dict</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1196</span>   <span class="k">if</span> <span class="n">field_desc</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1197</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_UpdateOneofState</span><span class="p">(</span><span class="n">field_desc</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/internal/decoder.py:705,</span> in <span class="ni">MessageDecoder.&lt;locals&gt;.DecodeRepeatedField</span><span class="nt">(buffer, pos, end, message, field_dict)</span>
<span class="g g-Whitespace">    </span><span class="mi">703</span>   <span class="k">raise</span> <span class="n">_DecodeError</span><span class="p">(</span><span class="s1">&#39;Truncated message.&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">704</span> <span class="c1"># Read sub-message.</span>
<span class="ne">--&gt; </span><span class="mi">705</span> <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">add</span><span class="p">()</span><span class="o">.</span><span class="n">_InternalParse</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">new_pos</span><span class="p">)</span> <span class="o">!=</span> <span class="n">new_pos</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">706</span>   <span class="c1"># The only reason _InternalParse would return early is if it</span>
<span class="g g-Whitespace">    </span><span class="mi">707</span>   <span class="c1"># encountered an end-group tag.</span>
<span class="g g-Whitespace">    </span><span class="mi">708</span>   <span class="k">raise</span> <span class="n">_DecodeError</span><span class="p">(</span><span class="s1">&#39;Unexpected end-group tag.&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">709</span> <span class="c1"># Predict that the next tag is another copy of the same repeated field.</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/internal/python_message.py:1195,</span> in <span class="ni">_AddMergeFromStringMethod.&lt;locals&gt;.InternalParse</span><span class="nt">(self, buffer, pos, end)</span>
<span class="g g-Whitespace">   </span><span class="mi">1193</span>   <span class="n">pos</span> <span class="o">=</span> <span class="n">new_pos</span>
<span class="g g-Whitespace">   </span><span class="mi">1194</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1195</span>   <span class="n">pos</span> <span class="o">=</span> <span class="n">field_decoder</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">new_pos</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">field_dict</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1196</span>   <span class="k">if</span> <span class="n">field_desc</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1197</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_UpdateOneofState</span><span class="p">(</span><span class="n">field_desc</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/internal/decoder.py:726,</span> in <span class="ni">MessageDecoder.&lt;locals&gt;.DecodeField</span><span class="nt">(buffer, pos, end, message, field_dict)</span>
<span class="g g-Whitespace">    </span><span class="mi">724</span>   <span class="k">raise</span> <span class="n">_DecodeError</span><span class="p">(</span><span class="s1">&#39;Truncated message.&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">725</span> <span class="c1"># Read sub-message.</span>
<span class="ne">--&gt; </span><span class="mi">726</span> <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">_InternalParse</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">new_pos</span><span class="p">)</span> <span class="o">!=</span> <span class="n">new_pos</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">727</span>   <span class="c1"># The only reason _InternalParse would return early is if it encountered</span>
<span class="g g-Whitespace">    </span><span class="mi">728</span>   <span class="c1"># an end-group tag.</span>
<span class="g g-Whitespace">    </span><span class="mi">729</span>   <span class="k">raise</span> <span class="n">_DecodeError</span><span class="p">(</span><span class="s1">&#39;Unexpected end-group tag.&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">730</span> <span class="k">return</span> <span class="n">new_pos</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/internal/python_message.py:1195,</span> in <span class="ni">_AddMergeFromStringMethod.&lt;locals&gt;.InternalParse</span><span class="nt">(self, buffer, pos, end)</span>
<span class="g g-Whitespace">   </span><span class="mi">1193</span>   <span class="n">pos</span> <span class="o">=</span> <span class="n">new_pos</span>
<span class="g g-Whitespace">   </span><span class="mi">1194</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1195</span>   <span class="n">pos</span> <span class="o">=</span> <span class="n">field_decoder</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">new_pos</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">field_dict</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1196</span>   <span class="k">if</span> <span class="n">field_desc</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1197</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_UpdateOneofState</span><span class="p">(</span><span class="n">field_desc</span><span class="p">)</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/internal/decoder.py:705,</span> in <span class="ni">MessageDecoder.&lt;locals&gt;.DecodeRepeatedField</span><span class="nt">(buffer, pos, end, message, field_dict)</span>
<span class="g g-Whitespace">    </span><span class="mi">703</span>   <span class="k">raise</span> <span class="n">_DecodeError</span><span class="p">(</span><span class="s1">&#39;Truncated message.&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">704</span> <span class="c1"># Read sub-message.</span>
<span class="ne">--&gt; </span><span class="mi">705</span> <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">add</span><span class="p">()</span><span class="o">.</span><span class="n">_InternalParse</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">new_pos</span><span class="p">)</span> <span class="o">!=</span> <span class="n">new_pos</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">706</span>   <span class="c1"># The only reason _InternalParse would return early is if it</span>
<span class="g g-Whitespace">    </span><span class="mi">707</span>   <span class="c1"># encountered an end-group tag.</span>
<span class="g g-Whitespace">    </span><span class="mi">708</span>   <span class="k">raise</span> <span class="n">_DecodeError</span><span class="p">(</span><span class="s1">&#39;Unexpected end-group tag.&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">709</span> <span class="c1"># Predict that the next tag is another copy of the same repeated field.</span>

<span class="nn">File ~/Library/Caches/pypoetry/virtualenvs/titanic-SA5bcgBn-py3.9/lib/python3.9/site-packages/google/protobuf/internal/python_message.py:1165,</span> in <span class="ni">_AddMergeFromStringMethod.&lt;locals&gt;.InternalParse</span><span class="nt">(self, buffer, pos, end)</span>
<span class="g g-Whitespace">   </span><span class="mi">1163</span> <span class="k">while</span> <span class="n">pos</span> <span class="o">!=</span> <span class="n">end</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1164</span>   <span class="p">(</span><span class="n">tag_bytes</span><span class="p">,</span> <span class="n">new_pos</span><span class="p">)</span> <span class="o">=</span> <span class="n">local_ReadTag</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1165</span>   <span class="n">field_decoder</span><span class="p">,</span> <span class="n">field_desc</span> <span class="o">=</span> <span class="n">decoders_by_tag</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tag_bytes</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1166</span>   <span class="k">if</span> <span class="n">field_decoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1167</span>     <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unknown_fields</span><span class="p">:</span>   <span class="c1"># pylint: disable=protected-access</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># test the performance of the model</span>
<span class="c1"># Once the weights are loaded, we will take a few samples from the test data and check the model&#39;s performance.</span>


<span class="c1"># Load the checkpoints</span>
<span class="n">cycle_gan_model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">checkpoint_filepath</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weights loaded successfully&quot;</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_horses</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">4</span><span class="p">)):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">cycle_gan_model</span><span class="o">.</span><span class="n">gen_G</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">*</span> <span class="mf">127.5</span> <span class="o">+</span> <span class="mf">127.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">127.5</span> <span class="o">+</span> <span class="mf">127.5</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Input image&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Input image&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Translated image&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

    <span class="n">prediction</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">array_to_img</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
    <span class="n">prediction</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;predicted_img_</span><span class="si">{i}</span><span class="s2">.png&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./gan"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="simple visible nav section-nav flex-column">
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>